{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524105dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:02.006120Z",
     "iopub.status.busy": "2025-03-26T06:51:02.005760Z",
     "iopub.status.idle": "2025-03-26T06:51:02.660755Z",
     "shell.execute_reply": "2025-03-26T06:51:02.659807Z"
    },
    "papermill": {
     "duration": 0.666139,
     "end_time": "2025-03-26T06:51:02.662649",
     "exception": false,
     "start_time": "2025-03-26T06:51:01.996510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc25d89",
   "metadata": {
    "papermill": {
     "duration": 0.007343,
     "end_time": "2025-03-26T06:51:02.678017",
     "exception": false,
     "start_time": "2025-03-26T06:51:02.670674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035982c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:02.693735Z",
     "iopub.status.busy": "2025-03-26T06:51:02.693267Z",
     "iopub.status.idle": "2025-03-26T06:51:08.394160Z",
     "shell.execute_reply": "2025-03-26T06:51:08.393204Z"
    },
    "papermill": {
     "duration": 5.710522,
     "end_time": "2025-03-26T06:51:08.395824",
     "exception": false,
     "start_time": "2025-03-26T06:51:02.685302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc138f",
   "metadata": {
    "papermill": {
     "duration": 0.007017,
     "end_time": "2025-03-26T06:51:08.410512",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.403495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d04ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.426430Z",
     "iopub.status.busy": "2025-03-26T06:51:08.425982Z",
     "iopub.status.idle": "2025-03-26T06:51:08.434352Z",
     "shell.execute_reply": "2025-03-26T06:51:08.433406Z"
    },
    "papermill": {
     "duration": 0.017985,
     "end_time": "2025-03-26T06:51:08.435754",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.417769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,encoder_sizes,*args,**kwargs):\n",
    "        super().__init__()\n",
    "        self.encoced_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(encoder_sizes, encoder_sizes[1:])])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoced_blocks(x)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self,num_classes,connector_size):\n",
    "        super().__init__();\n",
    "        self.predictor=nn.Sequential(nn.Linear(connector_size * 4 * 4, 256),nn.ReLU(),nn.Dropout(0.5),nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "        \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_c,enc_sizes, num_classes=10,activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "\n",
    "        self.encoder = Encoder(self.enc_sizes, activation=activation)\n",
    "        self.predictor=Predictor(num_classes,self.enc_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.predictor(x);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ca3e4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.451588Z",
     "iopub.status.busy": "2025-03-26T06:51:08.451290Z",
     "iopub.status.idle": "2025-03-26T06:51:08.459121Z",
     "shell.execute_reply": "2025-03-26T06:51:08.458237Z"
    },
    "papermill": {
     "duration": 0.017084,
     "end_time": "2025-03-26T06:51:08.460529",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.443445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,encoder_sizes,*args,**kwargs):\n",
    "        super().__init__()\n",
    "        self.encoced_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(encoder_sizes, encoder_sizes[1:])])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoced_blocks(x)\n",
    "\n",
    "class Predictor2(nn.Module):\n",
    "    def __init__(self,num_classes,connector_size):\n",
    "        super().__init__();\n",
    "        self.predictor=nn.Sequential(nn.Linear(connector_size * 4 * 4, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "        \n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, in_c,enc_sizes, num_classes=10,activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "\n",
    "        self.encoder = Encoder(self.enc_sizes, activation=activation)\n",
    "        self.predictor=Predictor2(num_classes,self.enc_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.predictor(x);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662948e1",
   "metadata": {
    "papermill": {
     "duration": 0.006972,
     "end_time": "2025-03-26T06:51:08.475000",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.468028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Test Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82e5789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.490530Z",
     "iopub.status.busy": "2025-03-26T06:51:08.490230Z",
     "iopub.status.idle": "2025-03-26T06:51:08.684567Z",
     "shell.execute_reply": "2025-03-26T06:51:08.683813Z"
    },
    "papermill": {
     "duration": 0.203965,
     "end_time": "2025-03-26T06:51:08.686120",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.482155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train_model_extra_metric(model, train_loader, val_loader, num_epochs=10, lr=0.001,lambda_reg=0.01, device='cpu', early_stopping_patience=5):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with validation and calculates precision, recall, and F1-score.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        val_loader (DataLoader): The validation data loader.\n",
    "        num_epochs (int, optional): The number of epochs to train for. Defaults to 10.\n",
    "        lr (float, optional): The learning rate. Defaults to 0.001.\n",
    "        device (str, optional): The device to use for training ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        early_stopping_patience (int, optional): Number of epochs to wait before early stopping. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_model, dict_metrics) where dict_metrics contains training history\n",
    "    \"\"\"\n",
    "    # Initialize model and optimization\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_f1 = 0\n",
    "    best_val=0\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            # loss += lambda_reg * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_targets = []\n",
    "            all_predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    predictions = outputs.argmax(dim=1)\n",
    "                    \n",
    "                    correct += (predictions == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "                    \n",
    "                    all_targets.extend(targets.cpu().numpy())\n",
    "                    all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_accuracy = correct / total\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_targets, \n",
    "                all_predictions, \n",
    "                average='weighted', \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Update history\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['val_precision'].append(precision)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_f1'].append(f1)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Model checkpoint and early stopping\n",
    "            if val_accuracy>best_val:\n",
    "                best_model=copy.deepcopy(model)\n",
    "                best_val=val_accuracy\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping check\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6dda77e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.701691Z",
     "iopub.status.busy": "2025-03-26T06:51:08.701422Z",
     "iopub.status.idle": "2025-03-26T06:51:08.707716Z",
     "shell.execute_reply": "2025-03-26T06:51:08.706912Z"
    },
    "papermill": {
     "duration": 0.015428,
     "end_time": "2025-03-26T06:51:08.709034",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.693606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val=0;\n",
    "    best_model=model;\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            # loss += 0.01 * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        # Validation (optional in minimal version)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        val_accuracy = correct / total\n",
    "        if val_accuracy>best_val:\n",
    "            best_model=copy.deepcopy(model)\n",
    "            best_val=val_accuracy\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    return best_model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6f6800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.724346Z",
     "iopub.status.busy": "2025-03-26T06:51:08.724097Z",
     "iopub.status.idle": "2025-03-26T06:51:08.728447Z",
     "shell.execute_reply": "2025-03-26T06:51:08.727659Z"
    },
    "papermill": {
     "duration": 0.013327,
     "end_time": "2025-03-26T06:51:08.729764",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.716437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model,test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    test_accuracy = correct / total\n",
    "    print('test accuracy',test_accuracy)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e95ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.745090Z",
     "iopub.status.busy": "2025-03-26T06:51:08.744786Z",
     "iopub.status.idle": "2025-03-26T06:51:08.748488Z",
     "shell.execute_reply": "2025-03-26T06:51:08.747496Z"
    },
    "papermill": {
     "duration": 0.013205,
     "end_time": "2025-03-26T06:51:08.750244",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.737039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_keys(original_dict, keys_to_remove):\n",
    "    # Create a new dictionary by excluding the specified keys\n",
    "    return {key: value for key, value in original_dict.items() if key not in keys_to_remove}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147b2bb",
   "metadata": {
    "papermill": {
     "duration": 0.010227,
     "end_time": "2025-03-26T06:51:08.770198",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.759971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae4a00a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.789641Z",
     "iopub.status.busy": "2025-03-26T06:51:08.789336Z",
     "iopub.status.idle": "2025-03-26T06:51:08.795131Z",
     "shell.execute_reply": "2025-03-26T06:51:08.794357Z"
    },
    "papermill": {
     "duration": 0.017027,
     "end_time": "2025-03-26T06:51:08.796303",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.779276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def state_dict_to_vector(state_dict, remove_keys=[]):\n",
    "\n",
    "    shared_state_dict = copy.deepcopy(state_dict)\n",
    "\n",
    "    for key in remove_keys:\n",
    "\n",
    "        if key in shared_state_dict:\n",
    "\n",
    "            del shared_state_dict[key]\n",
    "\n",
    "    sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n",
    "\n",
    "    return torch.nn.utils.parameters_to_vector([value.reshape(-1) for key, value in sorted_shared_state_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vector_to_state_dict(vector, state_dict, remove_keys=[]):\n",
    "\n",
    "    # create a reference dict to define the order of the vector\n",
    "\n",
    "    reference_dict = copy.deepcopy(state_dict)\n",
    "\n",
    "    for key in remove_keys:\n",
    "\n",
    "        if key in reference_dict:\n",
    "\n",
    "            del reference_dict[key]\n",
    "\n",
    "    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n",
    "\n",
    "\n",
    "\n",
    "    # create a shared state dict using the refence dict\n",
    "\n",
    "    torch.nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "    # add back the encoder and decoder embedding weights.\n",
    "\n",
    "    if \"transformer.shared.weight\" in sorted_reference_dict:\n",
    "\n",
    "        for key in remove_keys:\n",
    "\n",
    "            sorted_reference_dict[key] = sorted_reference_dict[\"transformer.shared.weight\"]\n",
    "\n",
    "    return sorted_reference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795c6b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.812011Z",
     "iopub.status.busy": "2025-03-26T06:51:08.811702Z",
     "iopub.status.idle": "2025-03-26T06:51:08.817248Z",
     "shell.execute_reply": "2025-03-26T06:51:08.816594Z"
    },
    "papermill": {
     "duration": 0.014682,
     "end_time": "2025-03-26T06:51:08.818425",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.803743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_sign_conflicts(tensor: torch.Tensor) -> int:\n",
    "    # Get the sign of each element in the tensor\n",
    "    # -1 for negative, 1 for positive, and 0 for zero\n",
    "    signs = torch.sign(tensor).to(device)\n",
    "\n",
    "    # Create a mask for non-zero entries\n",
    "    non_zero_mask = (signs != 0)\n",
    "\n",
    "    # Check for sign conflicts, ignoring zeros\n",
    "    negative_mask = (signs == -1) & non_zero_mask\n",
    "    positive_mask = (signs == 1) & non_zero_mask\n",
    "\n",
    "    # A conflict occurs if a column contains both -1 and 1\n",
    "    conflict_mask = negative_mask.any(dim=0) & positive_mask.any(dim=0)\n",
    "\n",
    "    # Count the number of columns with sign conflicts\n",
    "    num_conflicts = conflict_mask.sum().item()\n",
    "\n",
    "    return num_conflicts\n",
    "\n",
    "def calc_sign_conflicts(model_list,dummy_model,exclude_keys,model_class,number_of_tasks, is_transformer=False):\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new=remove_keys(dummy_model.state_dict(),exclude_keys)\n",
    "    flat_ptm= state_dict_to_vector(dm_new)\n",
    "\n",
    "    flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x].state_dict(),exclude_keys) )for x in range(number_of_tasks)]);\n",
    "  \n",
    "    tv_flat_checks = flat_ft - flat_ptm;\n",
    "    # torch.save(tv_flat_checks,model_name+\"_\"+task_name+\"_\"+str(number_of_tasks)+\"_\"+version+\".pt\");\n",
    "    # plot_mds_torch(tv_flat_checks);\n",
    "    # plot_tsne_torch(tv_flat_checks)\n",
    "    # flipped=flip(tv_flat_checks)\n",
    "    # plot_mds_torch(flipped)\n",
    "    # plot_tsne_torch(flipped)\n",
    "    return count_sign_conflicts(tv_flat_checks)/tv_flat_checks.shape[1], tv_flat_checks.shape[1];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b7106",
   "metadata": {
    "papermill": {
     "duration": 0.00707,
     "end_time": "2025-03-26T06:51:08.834337",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.827267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d7086f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.850019Z",
     "iopub.status.busy": "2025-03-26T06:51:08.849691Z",
     "iopub.status.idle": "2025-03-26T06:51:08.855683Z",
     "shell.execute_reply": "2025-03-26T06:51:08.855002Z"
    },
    "papermill": {
     "duration": 0.01541,
     "end_time": "2025-03-26T06:51:08.856951",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.841541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, data_loader, num_classes, device='cpu', class_names=None, plot=False):\n",
    "    \"\"\"\n",
    "    Compute and optionally plot the confusion matrix for a model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        data_loader: DataLoader for the dataset to evaluate.\n",
    "        num_classes: Number of classes in the dataset.\n",
    "        device: Device to use ('cpu' or 'cuda').\n",
    "        class_names: List of class names (optional, used for plotting).\n",
    "        plot: Boolean, whether to plot the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "        confusion_matrix: Normalized confusion matrix as a NumPy array.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "            for t, p in zip(targets, predictions):\n",
    "                confusion_matrix[t, p] += 1\n",
    "    \n",
    "    # Normalize rows to sum to 1\n",
    "    row_sums = confusion_matrix.sum(axis=1, keepdims=True)\n",
    "    confusion_matrix = confusion_matrix / (row_sums + 1e-7)  # Avoid division by zero\n",
    "\n",
    "    # Optionally plot the confusion matrix\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(\"Normalized Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.show()\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef682e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.872907Z",
     "iopub.status.busy": "2025-03-26T06:51:08.872591Z",
     "iopub.status.idle": "2025-03-26T06:51:08.887035Z",
     "shell.execute_reply": "2025-03-26T06:51:08.886353Z"
    },
    "papermill": {
     "duration": 0.024157,
     "end_time": "2025-03-26T06:51:08.888380",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.864223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "__author__ = 'Otilia Stretcu'\n",
    "\n",
    "\n",
    "def _find_closest_cluster_min(dist_matrix, clusters, node_to_cluster):\n",
    "    n = dist_matrix.shape[0]\n",
    "    closest_cluster = []\n",
    "    closest_cluster_dists = []\n",
    "    # For each cluster, find the edge of min weight going out of it.\n",
    "    for cluster in clusters:\n",
    "        cluster_nodes = np.asarray(cluster)\n",
    "        # Create a mask that marks the nodes in the current cluster.\n",
    "        cluster_mask = np.zeros((n,), dtype=np.bool)\n",
    "        cluster_mask[cluster_nodes] = True\n",
    "        non_cluster_mask = np.logical_not(cluster_mask)\n",
    "        non_cluster_nodes = np.where(non_cluster_mask)[0]\n",
    "        # Only consider the edges from nodes belonging to the current\n",
    "        # cluster and nodes from a different cluster.\n",
    "        dists = dist_matrix[cluster_mask][:, non_cluster_mask]\n",
    "        # Find the min of these distances and the node from a different\n",
    "        # cluster this connects to.\n",
    "        if len(dists.shape) < 2:\n",
    "            if len(cluster_nodes) == 1:\n",
    "                dists = dists[None]\n",
    "            else:\n",
    "                dists = dists[:, None]\n",
    "        min_dists = np.amin(dists, axis=1)\n",
    "        cluster_node_id = np.argmin(min_dists)\n",
    "        non_cluster_node_id = np.argmin(dists[cluster_node_id])\n",
    "        smallest_dist = dists[cluster_node_id][non_cluster_node_id]\n",
    "        non_cluster_node_id = non_cluster_nodes[non_cluster_node_id]\n",
    "        # Find the cluster of this closest node.\n",
    "        closest_cluster.append(node_to_cluster[non_cluster_node_id])\n",
    "        closest_cluster_dists.append(smallest_dist)\n",
    "\n",
    "    return closest_cluster, closest_cluster_dists\n",
    "\n",
    "\n",
    "def find_closest_cluster(dist_matrix, clusters):\n",
    "    \"\"\"Finds the closest cluster to each cluster.\n",
    "\n",
    "    Arguments:\n",
    "        dist_matrix: A numpy array representing a distance matrix between all pairs of nodes.\n",
    "        clusters: A list of lists, where each inner list contains the node ids that are in that\n",
    "            cluster.\n",
    "    Returns:\n",
    "        closest_cluster\n",
    "        closest_cluster_dists\n",
    "    \"\"\"\n",
    "    # Compute the number of nodes.\n",
    "    n = dist_matrix.shape[0]\n",
    "    # Map each node id to its cluster.\n",
    "    node_to_cluster = np.zeros((n,), dtype=np.int)\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for node in cluster:\n",
    "            node_to_cluster[node] = cluster_id\n",
    "\n",
    "    # For each cluster, find the nearest cluster to it, where nearest is\n",
    "    # defined depending on the linkage method.\n",
    "    closest_cluster, closest_cluster_dists = _find_closest_cluster_min(\n",
    "        dist_matrix, clusters, node_to_cluster)\n",
    "\n",
    "    return closest_cluster, closest_cluster_dists\n",
    "\n",
    "\n",
    "def merge_clusters(clusters, closest_cluster, num_clusters, last_cluster_id):\n",
    "    # Merge each cluster with the one closest to it. This will lead to a\n",
    "    # list of cluster sets.\n",
    "    color = ['not_visited' for _ in range(num_clusters)]\n",
    "    cluster_to_set = np.zeros((num_clusters,), dtype=np.int)\n",
    "    cluster_sets = []\n",
    "    cluster_set_to_id = []\n",
    "    for i in range(num_clusters):\n",
    "        if color[i] == 'not_visited':\n",
    "            current_set = {i}\n",
    "            color[i] = 'in_progress'\n",
    "            j = i\n",
    "            while closest_cluster[j] is not None and color[closest_cluster[j]] == 'not_visited':\n",
    "                current_set.add(closest_cluster[j])\n",
    "                color[closest_cluster[j]] = 'in_progress'\n",
    "                j = closest_cluster[j]\n",
    "            # If the cluster where we stopped doesn't have a closest cluster or it was visited\n",
    "            # during the current traversal starting at i, then we form a new set.\n",
    "            if closest_cluster[j] is None or color[closest_cluster[j]] == 'in_progress':\n",
    "                # This a new set of clusters.\n",
    "                set_index = len(cluster_sets)\n",
    "                cluster_sets.append(current_set)\n",
    "                last_cluster_id += 1\n",
    "                cluster_set_to_id.append(last_cluster_id)\n",
    "            else:\n",
    "                # This has to be added to an existing set of clusters.\n",
    "                set_index = cluster_to_set[closest_cluster[j]]\n",
    "                cluster_sets[set_index].update(current_set)\n",
    "\n",
    "            for j in list(current_set):\n",
    "                color[j] = 'visited'\n",
    "                cluster_to_set[j] = set_index\n",
    "    # Expand the cluster sets to actual clusters.)\n",
    "    clusters = [\n",
    "        [elem for cluster_index in list(c)\n",
    "         for elem in clusters[cluster_index]]\n",
    "        for c in cluster_sets]\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def affinity_clustering(dist_matrix, eps=1e-7):\n",
    "    \"\"\"Performs affinity clustering based on the provided distance matrix.\n",
    "\n",
    "    Our implementation is based on a publication of Bateni et al., \"Affinity clustering:\n",
    "    Hierarchical clustering at scale.\" NeurIPS, 2017. Specifically, it is based on the authors'\n",
    "    description of the algorithm, not on their released map-reduce-based code.\n",
    "\n",
    "    Args:\n",
    "        dist_matrix: A numpy array of shape (num_classes, num_classes) representing the distance\n",
    "            matrix between labels.\n",
    "        eps: A small float value representing the standard deviation of Gaussian noise added to\n",
    "            all distances to break ties.\n",
    "    Returns:\n",
    "        A list of lists containing the label clusters per level. Each outer list corresponds to a\n",
    "        level in the hierarchy. For each level, each inner list contains the label ids that are\n",
    "        clustered together at this level. Levels are indexed from bottom to top.\n",
    "        E.g., for the following hierarchy containing k=5 labels:\n",
    "                           c\n",
    "                        /     \\\n",
    "                       a      b\n",
    "                     / | \\   /\\\n",
    "                    1  2 3  4 5\n",
    "        the returned list of lists will be:\n",
    "        [[1, 2, 3, 4, 5], [[1, 2, 3], [4, 5]], [[1], [2], [3], [4], [5]].\n",
    "    \"\"\"\n",
    "    # Add a small value eps to all edges to break ties.\n",
    "    num_elem = dist_matrix.shape[0]\n",
    "    dist_matrix = dist_matrix + np.random.rand(num_elem, num_elem) * eps\n",
    "\n",
    "    # Num samples.\n",
    "    n = dist_matrix.shape[0]\n",
    "    # Each sample is it's own cluster in the beginning.\n",
    "    clusters = [[i] for i in range(n)]\n",
    "    # Keep track of clusters formed as we go.\n",
    "    last_cluster_id = len(clusters) - 1\n",
    "\n",
    "    # Merge clusters iteratively.\n",
    "    num_clusters = len(clusters)\n",
    "    clusters_per_level = [clusters]\n",
    "    level = 0\n",
    "    while num_clusters > 1:\n",
    "        # For each cluster, find the cluster that is nearest to it.\n",
    "        closest_cluster, closest_cluster_dists = find_closest_cluster(dist_matrix, clusters)\n",
    "\n",
    "        # Merge each cluster with the one closest to it.\n",
    "        new_clusters = merge_clusters(clusters, closest_cluster, num_clusters, last_cluster_id)\n",
    "\n",
    "        # If everything was merged in the first iteration, we remove the most expensive edge.\n",
    "        if level == 0:\n",
    "            while len(new_clusters) == 1:\n",
    "                logging.info('All nodes were merged in the first iteration. Removing the most '\n",
    "                             'expensive edge and trying again.')\n",
    "                idx = np.argmax(closest_cluster_dists)\n",
    "                closest_cluster[idx] = None\n",
    "                closest_cluster_dists[idx] = -np.inf\n",
    "                new_clusters = merge_clusters(clusters, closest_cluster, num_clusters,\n",
    "                                              last_cluster_id)\n",
    "\n",
    "        num_clusters = len(new_clusters)\n",
    "        clusters = new_clusters\n",
    "        clusters_per_level.append(clusters)\n",
    "        level += 1\n",
    "\n",
    "    return clusters_per_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f224f1a",
   "metadata": {
    "papermill": {
     "duration": 0.007228,
     "end_time": "2025-03-26T06:51:08.903318",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.896090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e0a625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.918899Z",
     "iopub.status.busy": "2025-03-26T06:51:08.918546Z",
     "iopub.status.idle": "2025-03-26T06:51:08.923989Z",
     "shell.execute_reply": "2025-03-26T06:51:08.923190Z"
    },
    "papermill": {
     "duration": 0.014746,
     "end_time": "2025-03-26T06:51:08.925231",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.910485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Load CIFAR-10 Dataset\n",
    "def load_cifar10(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize with CIFAR-10 stats\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split train dataset into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset.classes),train_dataset.classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3c7bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.940890Z",
     "iopub.status.busy": "2025-03-26T06:51:08.940595Z",
     "iopub.status.idle": "2025-03-26T06:51:08.946393Z",
     "shell.execute_reply": "2025-03-26T06:51:08.945559Z"
    },
    "papermill": {
     "duration": 0.015283,
     "end_time": "2025-03-26T06:51:08.947911",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.932628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Load CIFAR-10 Dataset\n",
    "def load_cifar100(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize with CIFAR-10 stats\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split train dataset into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset.classes),train_dataset.classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b133febd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.962983Z",
     "iopub.status.busy": "2025-03-26T06:51:08.962715Z",
     "iopub.status.idle": "2025-03-26T06:51:08.968056Z",
     "shell.execute_reply": "2025-03-26T06:51:08.967402Z"
    },
    "papermill": {
     "duration": 0.014066,
     "end_time": "2025-03-26T06:51:08.969133",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.955067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1. Load MNIST Dataset\n",
    "def load_mnist(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),  # Resize images to 32x32 to match CIFAR-10 size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST stats (mean, std)\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split train dataset into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset.classes), train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83ec3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:08.984050Z",
     "iopub.status.busy": "2025-03-26T06:51:08.983793Z",
     "iopub.status.idle": "2025-03-26T06:51:09.003101Z",
     "shell.execute_reply": "2025-03-26T06:51:09.002522Z"
    },
    "papermill": {
     "duration": 0.02816,
     "end_time": "2025-03-26T06:51:09.004362",
     "exception": false,
     "start_time": "2025-03-26T06:51:08.976202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_folder_dataset(data_path, valid=False, batch_size=64, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Load and prepare the dataset with train/validation/test split.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Size of batches for DataLoader\n",
    "        test_size (float): Fraction of data for testing\n",
    "        val_size (float): Fraction of remaining data for validation after test split\n",
    "        \n",
    "    Returns:\n",
    "        train_loader: DataLoader for training set\n",
    "        val_loader: DataLoader for validation set\n",
    "        test_loader: DataLoader for test set\n",
    "        total_classes (int): Number of classes in the dataset\n",
    "        classnames (list): List of class names\n",
    "    \"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if valid == False:\n",
    "        # Load the full dataset\n",
    "        full_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "        \n",
    "        # Get class names and total number of classes\n",
    "        classnames = full_dataset.classes\n",
    "        total_classes = len(classnames)\n",
    "        \n",
    "        # Get indices for splitting the dataset\n",
    "        indices = list(range(len(full_dataset)))\n",
    "        \n",
    "        # Split into training + (validation + test)\n",
    "        train_indices, test_val_indices = train_test_split(\n",
    "            indices,\n",
    "            test_size=test_size,  # Test size, e.g., 10%\n",
    "            stratify=[full_dataset.targets[i] for i in indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Split the remaining data (validation + test)\n",
    "        val_indices, test_indices = train_test_split(\n",
    "            test_val_indices,\n",
    "            test_size=val_size / (test_size + val_size),  # Validation size within remaining data\n",
    "            stratify=[full_dataset.targets[i] for i in test_val_indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create datasets for train, validation, and test\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "        test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "    \n",
    "    else:\n",
    "        # Separate train, validation, and test folders\n",
    "        train_dataset = datasets.ImageFolder(root=os.path.join(data_path, 'train'), transform=transform)\n",
    "        val_dataset = datasets.ImageFolder(root=os.path.join(data_path, 'val'), transform=transform)\n",
    "        test_dataset = datasets.ImageFolder(root=os.path.join(data_path, 'test'), transform=transform)\n",
    "        classnames = train_dataset.classes\n",
    "        total_classes = len(classnames)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, total_classes, classnames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d135b7",
   "metadata": {
    "papermill": {
     "duration": 0.006873,
     "end_time": "2025-03-26T06:51:09.018254",
     "exception": false,
     "start_time": "2025-03-26T06:51:09.011381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa98e49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:09.032898Z",
     "iopub.status.busy": "2025-03-26T06:51:09.032658Z",
     "iopub.status.idle": "2025-03-26T06:51:09.037474Z",
     "shell.execute_reply": "2025-03-26T06:51:09.036900Z"
    },
    "papermill": {
     "duration": 0.013561,
     "end_time": "2025-03-26T06:51:09.038762",
     "exception": false,
     "start_time": "2025-03-26T06:51:09.025201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_hierarchy_scipy(dist_matrix,symm=True, method='average', t=1.0, criterion='distance',depth=2):\n",
    "    \"\"\"\n",
    "    Compute hierarchy using scipy's hierarchical clustering.\n",
    "\n",
    "    Args:\n",
    "        dist_matrix: A numpy array of shape (num_classes, num_classes) representing the distance matrix.\n",
    "        method: Linkage method to use ('single', 'complete', 'average', etc.).\n",
    "        t: Threshold for forming clusters.\n",
    "        criterion: Criterion to use to form clusters ('distance', 'maxclust', etc.).\n",
    "    \n",
    "    Returns:\n",
    "        clusters_per_level: A list of lists containing the label clusters per level.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute linkage matrix from the distance matrix\n",
    "    # Convert distance matrix to condensed form (needed for scipy linkage)\n",
    "    if dist_matrix is not None:\n",
    "        if symm:\n",
    "            dist_matrix = dist_matrix + dist_matrix.T\n",
    "    condensed_dist_matrix = np.triu(dist_matrix, k=1)[np.triu_indices(dist_matrix.shape[0], k=1)]\n",
    "    linkage_matrix = linkage(condensed_dist_matrix, method=method)\n",
    "    # print(linkage_matrix)\n",
    "    # Step 2: Determine clusters at different thresholds\n",
    "    clusters_per_level = []\n",
    "    num_classes = dist_matrix.shape[0]\n",
    "    \n",
    "    # Iterate over levels\n",
    "    for threshold in np.linspace(0, t, num_classes):\n",
    "        cluster_labels = fcluster(linkage_matrix, t=threshold, criterion=criterion)\n",
    "        # Group labels into clusters\n",
    "        clusters = [[] for _ in range(max(cluster_labels))]\n",
    "        for label_idx, cluster_idx in enumerate(cluster_labels):\n",
    "            clusters[cluster_idx - 1].append(label_idx)\n",
    "        clusters_per_level.append(clusters)\n",
    "\n",
    "    return clusters_per_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a252ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:09.053578Z",
     "iopub.status.busy": "2025-03-26T06:51:09.053367Z",
     "iopub.status.idle": "2025-03-26T06:51:09.058140Z",
     "shell.execute_reply": "2025-03-26T06:51:09.057499Z"
    },
    "papermill": {
     "duration": 0.013413,
     "end_time": "2025-03-26T06:51:09.059270",
     "exception": false,
     "start_time": "2025-03-26T06:51:09.045857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UpdatedDataset(Dataset):\n",
    "    def __init__(self, original_dataset, new_labels):\n",
    "        \"\"\"\n",
    "        Create a new dataset with updated labels.\n",
    "        \n",
    "        Args:\n",
    "            original_dataset: The original dataset (e.g., train_loader.dataset).\n",
    "            new_labels: The updated labels for the dataset.\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.new_labels = new_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, _ = self.original_dataset[index]  # Get the input data, ignore the original label\n",
    "        new_label = self.new_labels[index]  # Get the updated label\n",
    "        return data, new_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "\n",
    "def update_data_labels(original_dataset, new_labels, num_classes):\n",
    "    \"\"\"\n",
    "    Update the labels of the dataset to reflect the new clusters at the current level.\n",
    "    \n",
    "    Args:\n",
    "        original_dataset: The original dataset (e.g., train_loader.dataset).\n",
    "        new_labels: A list or array of updated labels corresponding to the new clusters.\n",
    "        num_classes: The number of clusters (classes) at this level of the hierarchy.\n",
    "    \n",
    "    Returns:\n",
    "        A new dataset with updated labels.\n",
    "    \"\"\"\n",
    "    if len(new_labels) != len(original_dataset):\n",
    "        raise ValueError(\"The length of new_labels must match the size of the original dataset.\")\n",
    "    \n",
    "    return UpdatedDataset(original_dataset, new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13d6635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:09.074221Z",
     "iopub.status.busy": "2025-03-26T06:51:09.074015Z",
     "iopub.status.idle": "2025-03-26T06:51:09.126353Z",
     "shell.execute_reply": "2025-03-26T06:51:09.125714Z"
    },
    "papermill": {
     "duration": 0.061466,
     "end_time": "2025-03-26T06:51:09.127672",
     "exception": false,
     "start_time": "2025-03-26T06:51:09.066206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c15a6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T06:51:09.146096Z",
     "iopub.status.busy": "2025-03-26T06:51:09.145706Z",
     "iopub.status.idle": "2025-03-26T08:03:59.915013Z",
     "shell.execute_reply": "2025-03-26T08:03:59.914067Z"
    },
    "papermill": {
     "duration": 4370.781371,
     "end_time": "2025-03-26T08:03:59.916506",
     "exception": false,
     "start_time": "2025-03-26T06:51:09.135135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "dis\n",
      "Epoch 1/5\n",
      "Train Loss: 1.2460\n",
      "Val Accuracy: 0.4477\n",
      "Precision: 0.5169, Recall: 0.4477, F1: 0.4085\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.1918\n",
      "Val Accuracy: 0.4651\n",
      "Precision: 0.4882, Recall: 0.4651, F1: 0.4400\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1627\n",
      "Val Accuracy: 0.4869\n",
      "Precision: 0.5035, Recall: 0.4869, F1: 0.4650\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.1315\n",
      "Val Accuracy: 0.5087\n",
      "Precision: 0.5597, Recall: 0.5087, F1: 0.4824\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.0979\n",
      "Val Accuracy: 0.5000\n",
      "Precision: 0.5086, Recall: 0.5000, F1: 0.4745\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 3.0743\n",
      "Val Accuracy: 0.2471\n",
      "Precision: 0.2834, Recall: 0.2471, F1: 0.2347\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.5934\n",
      "Val Accuracy: 0.2922\n",
      "Precision: 0.3195, Recall: 0.2922, F1: 0.2725\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 2.3390\n",
      "Val Accuracy: 0.2922\n",
      "Precision: 0.3278, Recall: 0.2922, F1: 0.2829\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 2.1284\n",
      "Val Accuracy: 0.3241\n",
      "Precision: 0.3483, Recall: 0.3241, F1: 0.3201\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.9135\n",
      "Val Accuracy: 0.3023\n",
      "Precision: 0.3433, Recall: 0.3023, F1: 0.3030\n",
      "--------------------------------------------------\n",
      "test accuracy 0.311046511627907\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6360\n",
      "Val Accuracy: 0.8163\n",
      "Precision: 0.8203, Recall: 0.8163, F1: 0.8170\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4147\n",
      "Val Accuracy: 0.8110\n",
      "Precision: 0.8242, Recall: 0.8110, F1: 0.8059\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3556\n",
      "Val Accuracy: 0.8635\n",
      "Precision: 0.8664, Recall: 0.8635, F1: 0.8619\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3169\n",
      "Val Accuracy: 0.8478\n",
      "Precision: 0.8514, Recall: 0.8478, F1: 0.8441\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2773\n",
      "Val Accuracy: 0.8556\n",
      "Precision: 0.8586, Recall: 0.8556, F1: 0.8499\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.3351\n",
      "Val Accuracy: 0.4226\n",
      "Precision: 0.4806, Recall: 0.4226, F1: 0.3935\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.6936\n",
      "Val Accuracy: 0.4777\n",
      "Precision: 0.5255, Recall: 0.4777, F1: 0.4645\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.4034\n",
      "Val Accuracy: 0.5118\n",
      "Precision: 0.5291, Recall: 0.5118, F1: 0.4849\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.1765\n",
      "Val Accuracy: 0.5512\n",
      "Precision: 0.5775, Recall: 0.5512, F1: 0.5472\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.9713\n",
      "Val Accuracy: 0.5564\n",
      "Precision: 0.5951, Recall: 0.5564, F1: 0.5426\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.2322\n",
      "Val Accuracy: 0.4934\n",
      "Precision: 0.5543, Recall: 0.4934, F1: 0.4729\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.3965\n",
      "Val Accuracy: 0.4961\n",
      "Precision: 0.5416, Recall: 0.4961, F1: 0.4863\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.0373\n",
      "Val Accuracy: 0.5564\n",
      "Precision: 0.6014, Recall: 0.5564, F1: 0.5558\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.7846\n",
      "Val Accuracy: 0.5722\n",
      "Precision: 0.6367, Recall: 0.5722, F1: 0.5681\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5935\n",
      "Val Accuracy: 0.5696\n",
      "Precision: 0.6284, Recall: 0.5696, F1: 0.5661\n",
      "--------------------------------------------------\n",
      "test accuracy 0.5418848167539267\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 169001437/169001437 [00:11<00:00, 14486899.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6104\n",
      "Val Accuracy: 0.8889\n",
      "Precision: 0.8366, Recall: 0.8889, F1: 0.8463\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4737\n",
      "Val Accuracy: 0.8921\n",
      "Precision: 0.8745, Recall: 0.8921, F1: 0.8526\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4053\n",
      "Val Accuracy: 0.9000\n",
      "Precision: 0.8771, Recall: 0.9000, F1: 0.8712\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3614\n",
      "Val Accuracy: 0.9001\n",
      "Precision: 0.8809, Recall: 0.9001, F1: 0.8731\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3307\n",
      "Val Accuracy: 0.9049\n",
      "Precision: 0.8871, Recall: 0.9049, F1: 0.8861\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 1.8167\n",
      "Val Accuracy: 0.5829\n",
      "Precision: 0.5803, Recall: 0.5829, F1: 0.5280\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.4145\n",
      "Val Accuracy: 0.6013\n",
      "Precision: 0.5967, Recall: 0.6013, F1: 0.5555\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2684\n",
      "Val Accuracy: 0.6071\n",
      "Precision: 0.6044, Recall: 0.6071, F1: 0.5761\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.1578\n",
      "Val Accuracy: 0.6132\n",
      "Precision: 0.6051, Recall: 0.6132, F1: 0.5676\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.0640\n",
      "Val Accuracy: 0.6095\n",
      "Precision: 0.5989, Recall: 0.6095, F1: 0.5796\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.4446\n",
      "Val Accuracy: 0.4244\n",
      "Precision: 0.4447, Recall: 0.4244, F1: 0.4067\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.8202\n",
      "Val Accuracy: 0.4491\n",
      "Precision: 0.4659, Recall: 0.4491, F1: 0.4345\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.5944\n",
      "Val Accuracy: 0.4554\n",
      "Precision: 0.4648, Recall: 0.4554, F1: 0.4434\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.4472\n",
      "Val Accuracy: 0.4448\n",
      "Precision: 0.4613, Recall: 0.4448, F1: 0.4401\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.3266\n",
      "Val Accuracy: 0.4459\n",
      "Precision: 0.4631, Recall: 0.4459, F1: 0.4450\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.4437\n",
      "Val Accuracy: 0.4295\n",
      "Precision: 0.4540, Recall: 0.4295, F1: 0.4197\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.6892\n",
      "Val Accuracy: 0.4366\n",
      "Precision: 0.4588, Recall: 0.4366, F1: 0.4283\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.4424\n",
      "Val Accuracy: 0.4348\n",
      "Precision: 0.4547, Recall: 0.4348, F1: 0.4325\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2729\n",
      "Val Accuracy: 0.4263\n",
      "Precision: 0.4361, Recall: 0.4263, F1: 0.4186\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1505\n",
      "Val Accuracy: 0.4237\n",
      "Precision: 0.4323, Recall: 0.4237, F1: 0.4158\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.3383\n",
      "Val Accuracy: 0.4226\n",
      "Precision: 0.4584, Recall: 0.4226, F1: 0.4144\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.5139\n",
      "Val Accuracy: 0.4353\n",
      "Precision: 0.4538, Recall: 0.4353, F1: 0.4297\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2571\n",
      "Val Accuracy: 0.4244\n",
      "Precision: 0.4385, Recall: 0.4244, F1: 0.4187\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.0919\n",
      "Val Accuracy: 0.4265\n",
      "Precision: 0.4382, Recall: 0.4265, F1: 0.4217\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.9644\n",
      "Val Accuracy: 0.4211\n",
      "Precision: 0.4313, Recall: 0.4211, F1: 0.4142\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.3020\n",
      "Val Accuracy: 0.4198\n",
      "Precision: 0.4536, Recall: 0.4198, F1: 0.4164\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.3993\n",
      "Val Accuracy: 0.4268\n",
      "Precision: 0.4549, Recall: 0.4268, F1: 0.4253\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1320\n",
      "Val Accuracy: 0.4193\n",
      "Precision: 0.4326, Recall: 0.4193, F1: 0.4172\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.9660\n",
      "Val Accuracy: 0.4091\n",
      "Precision: 0.4337, Recall: 0.4091, F1: 0.4092\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.8505\n",
      "Val Accuracy: 0.4111\n",
      "Precision: 0.4274, Recall: 0.4111, F1: 0.4114\n",
      "--------------------------------------------------\n",
      "test accuracy 0.4363\n",
      "Epoch 1/5\n",
      "Train Loss: 1.3076\n",
      "Val Accuracy: 0.4608\n",
      "Precision: 0.4811, Recall: 0.4608, F1: 0.4198\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.2475\n",
      "Val Accuracy: 0.4390\n",
      "Precision: 0.4974, Recall: 0.4390, F1: 0.3400\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2089\n",
      "Val Accuracy: 0.4942\n",
      "Precision: 0.4917, Recall: 0.4942, F1: 0.4594\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.1789\n",
      "Val Accuracy: 0.4855\n",
      "Precision: 0.4947, Recall: 0.4855, F1: 0.4340\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1473\n",
      "Val Accuracy: 0.5247\n",
      "Precision: 0.5661, Recall: 0.5247, F1: 0.4806\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 3.0485\n",
      "Val Accuracy: 0.2413\n",
      "Precision: 0.2627, Recall: 0.2413, F1: 0.2198\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.5690\n",
      "Val Accuracy: 0.2747\n",
      "Precision: 0.3025, Recall: 0.2747, F1: 0.2510\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 2.3176\n",
      "Val Accuracy: 0.3140\n",
      "Precision: 0.3256, Recall: 0.3140, F1: 0.3010\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 2.1106\n",
      "Val Accuracy: 0.3067\n",
      "Precision: 0.3415, Recall: 0.3067, F1: 0.3021\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.9183\n",
      "Val Accuracy: 0.3198\n",
      "Precision: 0.3460, Recall: 0.3198, F1: 0.3105\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.7950\n",
      "Val Accuracy: 0.2994\n",
      "Precision: 0.3199, Recall: 0.2994, F1: 0.2793\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.1708\n",
      "Val Accuracy: 0.3343\n",
      "Precision: 0.3522, Recall: 0.3343, F1: 0.3217\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.8773\n",
      "Val Accuracy: 0.3387\n",
      "Precision: 0.3600, Recall: 0.3387, F1: 0.3337\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.6606\n",
      "Val Accuracy: 0.3343\n",
      "Precision: 0.3937, Recall: 0.3343, F1: 0.3320\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.4752\n",
      "Val Accuracy: 0.3459\n",
      "Precision: 0.3784, Recall: 0.3459, F1: 0.3359\n",
      "--------------------------------------------------\n",
      "test accuracy 0.30523255813953487\n",
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "dis_layer\n",
      "skipped ---------------------------------------\n",
      "dis\n",
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "dis_layer\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "Epoch 1/5\n",
      "Train Loss: 1.2525\n",
      "Val Accuracy: 0.4622\n",
      "Precision: 0.4928, Recall: 0.4622, F1: 0.4153\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.1979\n",
      "Val Accuracy: 0.4738\n",
      "Precision: 0.5081, Recall: 0.4738, F1: 0.4200\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1673\n",
      "Val Accuracy: 0.4811\n",
      "Precision: 0.5835, Recall: 0.4811, F1: 0.4265\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.1366\n",
      "Val Accuracy: 0.4927\n",
      "Precision: 0.4886, Recall: 0.4927, F1: 0.4749\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1048\n",
      "Val Accuracy: 0.5145\n",
      "Precision: 0.5308, Recall: 0.5145, F1: 0.4816\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.7735\n",
      "Val Accuracy: 0.2442\n",
      "Precision: 0.2631, Recall: 0.2442, F1: 0.2171\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.4049\n",
      "Val Accuracy: 0.2791\n",
      "Precision: 0.2876, Recall: 0.2791, F1: 0.2565\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 2.2312\n",
      "Val Accuracy: 0.2791\n",
      "Precision: 0.3038, Recall: 0.2791, F1: 0.2647\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 2.0921\n",
      "Val Accuracy: 0.2994\n",
      "Precision: 0.3392, Recall: 0.2994, F1: 0.2971\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.9663\n",
      "Val Accuracy: 0.3081\n",
      "Precision: 0.3232, Recall: 0.3081, F1: 0.2988\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.8135\n",
      "Val Accuracy: 0.3183\n",
      "Precision: 0.3329, Recall: 0.3183, F1: 0.3013\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.2305\n",
      "Val Accuracy: 0.3270\n",
      "Precision: 0.3500, Recall: 0.3270, F1: 0.3190\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.9674\n",
      "Val Accuracy: 0.3299\n",
      "Precision: 0.3634, Recall: 0.3299, F1: 0.3287\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.7541\n",
      "Val Accuracy: 0.3270\n",
      "Precision: 0.3296, Recall: 0.3270, F1: 0.3189\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.5817\n",
      "Val Accuracy: 0.3401\n",
      "Precision: 0.3691, Recall: 0.3401, F1: 0.3317\n",
      "--------------------------------------------------\n",
      "test accuracy 0.3386627906976744\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "skipped ---------------------------------------\n",
      "dis\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/5\n",
      "Train Loss: 0.9993\n",
      "Val Accuracy: 0.7894\n",
      "Precision: 0.7202, Recall: 0.7894, F1: 0.7108\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.7587\n",
      "Val Accuracy: 0.8136\n",
      "Precision: 0.7868, Recall: 0.8136, F1: 0.7694\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6486\n",
      "Val Accuracy: 0.8230\n",
      "Precision: 0.7966, Recall: 0.8230, F1: 0.7799\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5843\n",
      "Val Accuracy: 0.8184\n",
      "Precision: 0.7948, Recall: 0.8184, F1: 0.7696\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5335\n",
      "Val Accuracy: 0.8318\n",
      "Precision: 0.8063, Recall: 0.8318, F1: 0.8060\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 1.9436\n",
      "Val Accuracy: 0.5264\n",
      "Precision: 0.5196, Recall: 0.5264, F1: 0.4999\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.5318\n",
      "Val Accuracy: 0.5571\n",
      "Precision: 0.5785, Recall: 0.5571, F1: 0.5177\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.3695\n",
      "Val Accuracy: 0.5661\n",
      "Precision: 0.5624, Recall: 0.5661, F1: 0.5492\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2544\n",
      "Val Accuracy: 0.5720\n",
      "Precision: 0.5736, Recall: 0.5720, F1: 0.5547\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1641\n",
      "Val Accuracy: 0.5690\n",
      "Precision: 0.5653, Recall: 0.5690, F1: 0.5439\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.4374\n",
      "Val Accuracy: 0.4295\n",
      "Precision: 0.4593, Recall: 0.4295, F1: 0.4108\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.8063\n",
      "Val Accuracy: 0.4460\n",
      "Precision: 0.4573, Recall: 0.4460, F1: 0.4391\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.5811\n",
      "Val Accuracy: 0.4492\n",
      "Precision: 0.4695, Recall: 0.4492, F1: 0.4428\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.4260\n",
      "Val Accuracy: 0.4447\n",
      "Precision: 0.4573, Recall: 0.4447, F1: 0.4419\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.3094\n",
      "Val Accuracy: 0.4461\n",
      "Precision: 0.4594, Recall: 0.4461, F1: 0.4387\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.4315\n",
      "Val Accuracy: 0.4371\n",
      "Precision: 0.4616, Recall: 0.4371, F1: 0.4325\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.6684\n",
      "Val Accuracy: 0.4362\n",
      "Precision: 0.4526, Recall: 0.4362, F1: 0.4286\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.4227\n",
      "Val Accuracy: 0.4297\n",
      "Precision: 0.4455, Recall: 0.4297, F1: 0.4230\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2572\n",
      "Val Accuracy: 0.4276\n",
      "Precision: 0.4409, Recall: 0.4276, F1: 0.4228\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1344\n",
      "Val Accuracy: 0.4214\n",
      "Precision: 0.4372, Recall: 0.4214, F1: 0.4186\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.3413\n",
      "Val Accuracy: 0.4369\n",
      "Precision: 0.4512, Recall: 0.4369, F1: 0.4268\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.5084\n",
      "Val Accuracy: 0.4311\n",
      "Precision: 0.4591, Recall: 0.4311, F1: 0.4255\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2526\n",
      "Val Accuracy: 0.4323\n",
      "Precision: 0.4404, Recall: 0.4323, F1: 0.4251\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.0964\n",
      "Val Accuracy: 0.4161\n",
      "Precision: 0.4396, Recall: 0.4161, F1: 0.4150\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.9688\n",
      "Val Accuracy: 0.4173\n",
      "Precision: 0.4235, Recall: 0.4173, F1: 0.4105\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.2973\n",
      "Val Accuracy: 0.4279\n",
      "Precision: 0.4626, Recall: 0.4279, F1: 0.4270\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.3892\n",
      "Val Accuracy: 0.4266\n",
      "Precision: 0.4473, Recall: 0.4266, F1: 0.4184\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1241\n",
      "Val Accuracy: 0.4251\n",
      "Precision: 0.4458, Recall: 0.4251, F1: 0.4240\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.9631\n",
      "Val Accuracy: 0.4109\n",
      "Precision: 0.4319, Recall: 0.4109, F1: 0.4108\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.8479\n",
      "Val Accuracy: 0.4089\n",
      "Precision: 0.4263, Recall: 0.4089, F1: 0.4072\n",
      "--------------------------------------------------\n",
      "test accuracy 0.4352\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4150\n",
      "Val Accuracy: 0.8976\n",
      "Precision: 0.8979, Recall: 0.8976, F1: 0.8978\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.2515\n",
      "Val Accuracy: 0.9108\n",
      "Precision: 0.9127, Recall: 0.9108, F1: 0.9114\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2147\n",
      "Val Accuracy: 0.9186\n",
      "Precision: 0.9178, Recall: 0.9186, F1: 0.9171\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1821\n",
      "Val Accuracy: 0.9265\n",
      "Precision: 0.9293, Recall: 0.9265, F1: 0.9237\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1552\n",
      "Val Accuracy: 0.8950\n",
      "Precision: 0.9089, Recall: 0.8950, F1: 0.8978\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.9810\n",
      "Val Accuracy: 0.7559\n",
      "Precision: 0.7498, Recall: 0.7559, F1: 0.7392\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.7076\n",
      "Val Accuracy: 0.7612\n",
      "Precision: 0.7595, Recall: 0.7612, F1: 0.7428\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6037\n",
      "Val Accuracy: 0.7480\n",
      "Precision: 0.7660, Recall: 0.7480, F1: 0.7362\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5197\n",
      "Val Accuracy: 0.7454\n",
      "Precision: 0.7460, Recall: 0.7454, F1: 0.7378\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4525\n",
      "Val Accuracy: 0.7743\n",
      "Precision: 0.7724, Recall: 0.7743, F1: 0.7695\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.1433\n",
      "Val Accuracy: 0.4934\n",
      "Precision: 0.5432, Recall: 0.4934, F1: 0.4662\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.4622\n",
      "Val Accuracy: 0.5407\n",
      "Precision: 0.5744, Recall: 0.5407, F1: 0.5311\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1451\n",
      "Val Accuracy: 0.5564\n",
      "Precision: 0.5621, Recall: 0.5564, F1: 0.5376\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.9095\n",
      "Val Accuracy: 0.5722\n",
      "Precision: 0.6412, Recall: 0.5722, F1: 0.5669\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.7241\n",
      "Val Accuracy: 0.5538\n",
      "Precision: 0.6181, Recall: 0.5538, F1: 0.5500\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.1885\n",
      "Val Accuracy: 0.5039\n",
      "Precision: 0.5659, Recall: 0.5039, F1: 0.4865\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.2550\n",
      "Val Accuracy: 0.5512\n",
      "Precision: 0.5951, Recall: 0.5512, F1: 0.5451\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.8717\n",
      "Val Accuracy: 0.5643\n",
      "Precision: 0.5927, Recall: 0.5643, F1: 0.5622\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5938\n",
      "Val Accuracy: 0.5617\n",
      "Precision: 0.6132, Recall: 0.5617, F1: 0.5526\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4445\n",
      "Val Accuracy: 0.5774\n",
      "Precision: 0.6299, Recall: 0.5774, F1: 0.5812\n",
      "--------------------------------------------------\n",
      "test accuracy 0.5575916230366492\n",
      "skipped ---------------------------------------\n",
      "dis_layer\n",
      "Epoch 1/5\n",
      "Train Loss: 1.4355\n",
      "Val Accuracy: 0.4172\n",
      "Precision: 0.3637, Recall: 0.4172, F1: 0.3740\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.3433\n",
      "Val Accuracy: 0.4273\n",
      "Precision: 0.3882, Recall: 0.4273, F1: 0.3955\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2972\n",
      "Val Accuracy: 0.4622\n",
      "Precision: 0.4981, Recall: 0.4622, F1: 0.4286\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2524\n",
      "Val Accuracy: 0.4767\n",
      "Precision: 0.4865, Recall: 0.4767, F1: 0.4327\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.2090\n",
      "Val Accuracy: 0.4956\n",
      "Precision: 0.4942, Recall: 0.4956, F1: 0.4810\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 3.0163\n",
      "Val Accuracy: 0.2267\n",
      "Precision: 0.2413, Recall: 0.2267, F1: 0.2075\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 2.5420\n",
      "Val Accuracy: 0.2762\n",
      "Precision: 0.2778, Recall: 0.2762, F1: 0.2562\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 2.2865\n",
      "Val Accuracy: 0.3009\n",
      "Precision: 0.3373, Recall: 0.3009, F1: 0.2915\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 2.0675\n",
      "Val Accuracy: 0.3183\n",
      "Precision: 0.3312, Recall: 0.3183, F1: 0.3050\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.8800\n",
      "Val Accuracy: 0.3314\n",
      "Precision: 0.3385, Recall: 0.3314, F1: 0.3201\n",
      "--------------------------------------------------\n",
      "test accuracy 0.30377906976744184\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5246\n",
      "Val Accuracy: 0.8819\n",
      "Precision: 0.8699, Recall: 0.8819, F1: 0.8658\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3327\n",
      "Val Accuracy: 0.8871\n",
      "Precision: 0.8807, Recall: 0.8871, F1: 0.8797\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2748\n",
      "Val Accuracy: 0.8793\n",
      "Precision: 0.8605, Recall: 0.8793, F1: 0.8673\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2480\n",
      "Val Accuracy: 0.8845\n",
      "Precision: 0.8833, Recall: 0.8845, F1: 0.8818\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2200\n",
      "Val Accuracy: 0.9003\n",
      "Precision: 0.9002, Recall: 0.9003, F1: 0.8892\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 1.2443\n",
      "Val Accuracy: 0.6982\n",
      "Precision: 0.7404, Recall: 0.6982, F1: 0.6690\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.8796\n",
      "Val Accuracy: 0.7428\n",
      "Precision: 0.7453, Recall: 0.7428, F1: 0.7311\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.7714\n",
      "Val Accuracy: 0.7559\n",
      "Precision: 0.7526, Recall: 0.7559, F1: 0.7440\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6727\n",
      "Val Accuracy: 0.7559\n",
      "Precision: 0.7662, Recall: 0.7559, F1: 0.7531\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5789\n",
      "Val Accuracy: 0.7402\n",
      "Precision: 0.7346, Recall: 0.7402, F1: 0.7313\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.2213\n",
      "Val Accuracy: 0.4856\n",
      "Precision: 0.5679, Recall: 0.4856, F1: 0.4783\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.5060\n",
      "Val Accuracy: 0.5538\n",
      "Precision: 0.5946, Recall: 0.5538, F1: 0.5465\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.1821\n",
      "Val Accuracy: 0.5538\n",
      "Precision: 0.6249, Recall: 0.5538, F1: 0.5473\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.9343\n",
      "Val Accuracy: 0.5722\n",
      "Precision: 0.6501, Recall: 0.5722, F1: 0.5770\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.7411\n",
      "Val Accuracy: 0.5827\n",
      "Precision: 0.6093, Recall: 0.5827, F1: 0.5713\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.2188\n",
      "Val Accuracy: 0.5249\n",
      "Precision: 0.6125, Recall: 0.5249, F1: 0.5125\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.2709\n",
      "Val Accuracy: 0.5433\n",
      "Precision: 0.6281, Recall: 0.5433, F1: 0.5437\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.8998\n",
      "Val Accuracy: 0.5801\n",
      "Precision: 0.6500, Recall: 0.5801, F1: 0.5781\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6434\n",
      "Val Accuracy: 0.5722\n",
      "Precision: 0.6268, Recall: 0.5722, F1: 0.5702\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4730\n",
      "Val Accuracy: 0.5459\n",
      "Precision: 0.6050, Recall: 0.5459, F1: 0.5430\n",
      "--------------------------------------------------\n",
      "test accuracy 0.5575916230366492\n",
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "ci\n",
      "skipped ---------------------------------------\n",
      "sim\n",
      "Epoch 1/5\n",
      "Train Loss: 0.0969\n",
      "Val Accuracy: 0.9843\n",
      "Precision: 0.9845, Recall: 0.9843, F1: 0.9796\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.0553\n",
      "Val Accuracy: 0.9869\n",
      "Precision: 0.9871, Recall: 0.9869, F1: 0.9839\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.0477\n",
      "Val Accuracy: 0.9895\n",
      "Precision: 0.9896, Recall: 0.9895, F1: 0.9878\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.0391\n",
      "Val Accuracy: 0.9843\n",
      "Precision: 0.9818, Recall: 0.9843, F1: 0.9817\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0292\n",
      "Val Accuracy: 0.9843\n",
      "Precision: 0.9818, Recall: 0.9843, F1: 0.9817\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.6120\n",
      "Val Accuracy: 0.4199\n",
      "Precision: 0.4499, Recall: 0.4199, F1: 0.3843\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.8290\n",
      "Val Accuracy: 0.5013\n",
      "Precision: 0.5563, Recall: 0.5013, F1: 0.4885\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.4976\n",
      "Val Accuracy: 0.5407\n",
      "Precision: 0.6203, Recall: 0.5407, F1: 0.5438\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2888\n",
      "Val Accuracy: 0.5643\n",
      "Precision: 0.6407, Recall: 0.5643, F1: 0.5590\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 1.0751\n",
      "Val Accuracy: 0.5564\n",
      "Precision: 0.6156, Recall: 0.5564, F1: 0.5626\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 2.1992\n",
      "Val Accuracy: 0.4829\n",
      "Precision: 0.5361, Recall: 0.4829, F1: 0.4592\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 1.3744\n",
      "Val Accuracy: 0.5512\n",
      "Precision: 0.6077, Recall: 0.5512, F1: 0.5504\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 1.0533\n",
      "Val Accuracy: 0.5801\n",
      "Precision: 0.6515, Recall: 0.5801, F1: 0.5787\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.7890\n",
      "Val Accuracy: 0.5748\n",
      "Precision: 0.6236, Recall: 0.5748, F1: 0.5729\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.6165\n",
      "Val Accuracy: 0.5801\n",
      "Precision: 0.6175, Recall: 0.5801, F1: 0.5716\n",
      "--------------------------------------------------\n",
      "test accuracy 0.5549738219895288\n"
     ]
    }
   ],
   "source": [
    "conflict_folder=\"/kaggle/input/taskvector-distances\"\n",
    "conflicts_files=os.listdir(conflict_folder);\n",
    "hyperParam={'ci':7,'dis':250,'dis_layer':70,'sim':4,'diseuc':40,'diseuc_layer':10, 'disci':40,'disci_layer':10}\n",
    "results={}\n",
    "\n",
    "dataset_names={'cifar100':\"CIFAR100\",'cards':\"/kaggle/input/cards-image-datasetclassification/train\",'mammals':\"/kaggle/input/mammals-image-classification-dataset-45-animals/mammals\"};\n",
    "for c_file in conflicts_files:\n",
    "    \n",
    "    temp_df=pd.read_csv(os.path.join(conflict_folder,c_file),header=None)\n",
    "    np_conflict=temp_df.to_numpy()\n",
    "  \n",
    "    counters=0;\n",
    " \n",
    "    \n",
    "    dist_mat= np.abs(np_conflict)\n",
    "    # allclusters=example_clustering2(np_conflict,k,flag)\n",
    "    key_f=c_file.split('_')[-1].split('.')[0];\n",
    "    key_s=c_file.split('_')[1];\n",
    "    if key_s=='layer' and key_f=='dis':\n",
    "        key_f=key_f+'_layer'\n",
    "    if key_f in ['ci', 'dis', 'dis_layer', 'sim']:\n",
    "        print(\"skipped ---------------------------------------\")\n",
    "        print(key_f)\n",
    "        continue;\n",
    "    t_val=hyperParam[key_f]\n",
    "    \n",
    "    cluster_levels=compute_hierarchy_scipy(dist_mat,symm=True,t=t_val,criterion='distance')\n",
    "    unique_clusters=[];\n",
    "    for x in cluster_levels:\n",
    "        if x not in unique_clusters:\n",
    "            unique_clusters.append(x);\n",
    "    # print(unique_clusters)\n",
    "    unique_clusters=unique_clusters\n",
    "    # print(unique_clusters)\n",
    "    unique_clusters.reverse()\n",
    "    unique_clusters=unique_clusters[1:]\n",
    "    # print(key_f);\n",
    "    # print(len(unique_clusters))\n",
    "    # print('-------')\n",
    "    import copy\n",
    "    enc_sizes=[32,64,64]\n",
    "    num_classes_lim=100;\n",
    "    counter_1=0;\n",
    "    # model=best_model\n",
    "    dataset_name=dataset_names[c_file.split('_')[0]]\n",
    "    if dataset_name=='CIFAR100':\n",
    "        train_loader, val_loader,test_loader, num_classes, classnames= load_cifar100()\n",
    "    else:\n",
    "        train_loader, val_loader, test_loader,num_classes, classnames= load_folder_dataset(data_path=dataset_name)\n",
    "    for level in range(len(unique_clusters)):\n",
    "        clusters=unique_clusters[level]\n",
    "        original_to_new_label = {}\n",
    "        new_label_to_cluster = {}\n",
    "        # print('training for', clusters)\n",
    "        for new_label, label_group in enumerate(clusters):\n",
    "           \n",
    "            for original_label in label_group:\n",
    "                original_to_new_label[original_label] = new_label\n",
    "            new_label_to_cluster[new_label] = label_group\n",
    "        num_classes_current = len(clusters)\n",
    "        # print(num_classes_current)\n",
    "        train_labels = np.asarray([original_to_new_label[l[1]] for l in train_loader.dataset])\n",
    "        updated_train_dataset=update_data_labels(train_loader.dataset,train_labels, num_classes_current)\n",
    "        val_labels = np.asarray([original_to_new_label[l[1]] for l in val_loader.dataset])\n",
    "        updated_val_dataset=update_data_labels(val_loader.dataset,val_labels, num_classes_current)\n",
    "        temp_train_dataloader=DataLoader(updated_train_dataset,batch_size=64,shuffle=True)\n",
    "        temp_val_dataloader=DataLoader(updated_val_dataset,batch_size=64,shuffle=True)\n",
    "        \n",
    "        if level>0:\n",
    "            # print('no copy')\n",
    "            model_prev=copy.deepcopy(model)\n",
    "            model=CNN2(3,enc_sizes,num_classes=num_classes_current);\n",
    "            model.encoder=model_prev.encoder;\n",
    "        else:\n",
    "            model=CNN2(3,enc_sizes,num_classes=num_classes_current);\n",
    "        \n",
    "        best_model=train_model_extra_metric(model,temp_train_dataloader,temp_val_dataloader,num_epochs=5,device=device)\n",
    "        counter_1+=1;\n",
    "    test_labels = np.asarray([original_to_new_label[l[1]] for l in test_loader.dataset])\n",
    "    updated_test_dataset=update_data_labels(test_loader.dataset,test_labels, num_classes_current)\n",
    "    # test_labels = np.asarray([original_to_new_label[l[1]] for l in test_loader.dataset])\n",
    "    temp_test_dataloader=DataLoader(updated_test_dataset,batch_size=64,shuffle=True)\n",
    "    test_res=test_model(best_model,temp_test_dataloader)\n",
    "        # if num_classes_current == num_classes_lim:\n",
    "        #     break;\n",
    "    results[c_file]=test_res;\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcce61c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:03:59.955553Z",
     "iopub.status.busy": "2025-03-26T08:03:59.955273Z",
     "iopub.status.idle": "2025-03-26T08:03:59.959644Z",
     "shell.execute_reply": "2025-03-26T08:03:59.958815Z"
    },
    "papermill": {
     "duration": 0.024869,
     "end_time": "2025-03-26T08:03:59.960794",
     "exception": false,
     "start_time": "2025-03-26T08:03:59.935925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mammals_layer_wise_disci.csv': 0.311046511627907, 'cards_layer_wise_disci.csv': 0.5418848167539267, 'cifar100_layer_wise_diseuc.csv': 0.4363, 'mammals_full_diseuc.csv': 0.30523255813953487, 'mammals_full_disci.csv': 0.3386627906976744, 'cifar100_layer_wise_disci.csv': 0.4352, 'cards_full_disci.csv': 0.5575916230366492, 'mammals_layer_wise_diseuc.csv': 0.30377906976744184, 'cards_full_diseuc.csv': 0.5575916230366492, 'cards_layer_wise_diseuc.csv': 0.5549738219895288}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe71436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.000925Z",
     "iopub.status.busy": "2025-03-26T08:04:00.000556Z",
     "iopub.status.idle": "2025-03-26T08:04:00.004112Z",
     "shell.execute_reply": "2025-03-26T08:04:00.003237Z"
    },
    "papermill": {
     "duration": 0.026016,
     "end_time": "2025-03-26T08:04:00.005523",
     "exception": false,
     "start_time": "2025-03-26T08:03:59.979507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # dist_mat=1-conf_mat ## uncomment when using confusion matrix\n",
    "# # dist_mat= np.array(conflict2d)\n",
    "# dist_mat= np_conflict\n",
    "# # print(dist_mat)\n",
    "# cluster_levels=compute_hierarchy_scipy(dist_mat,symm=True,t=7,criterion='distance')\n",
    "# # cluster_levels=compute_hierarchy_scipy(dist_mat,symm=True,method='complete',t=6,criterion='inconsistent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4538ccb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.045718Z",
     "iopub.status.busy": "2025-03-26T08:04:00.045441Z",
     "iopub.status.idle": "2025-03-26T08:04:00.048422Z",
     "shell.execute_reply": "2025-03-26T08:04:00.047731Z"
    },
    "papermill": {
     "duration": 0.02474,
     "end_time": "2025-03-26T08:04:00.049718",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.024978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique_clusters=[];\n",
    "# for x in cluster_levels:\n",
    "#     # if x not in unique_clusters:\n",
    "#         unique_clusters.append(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ecbb5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.095974Z",
     "iopub.status.busy": "2025-03-26T08:04:00.095588Z",
     "iopub.status.idle": "2025-03-26T08:04:00.099013Z",
     "shell.execute_reply": "2025-03-26T08:04:00.098141Z"
    },
    "papermill": {
     "duration": 0.030054,
     "end_time": "2025-03-26T08:04:00.100488",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.070434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print(unique_clusters)\n",
    "# unique_clusters=unique_clusters\n",
    "# # print(unique_clusters)\n",
    "# unique_clusters.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1538185f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.144612Z",
     "iopub.status.busy": "2025-03-26T08:04:00.144293Z",
     "iopub.status.idle": "2025-03-26T08:04:00.147789Z",
     "shell.execute_reply": "2025-03-26T08:04:00.146921Z"
    },
    "papermill": {
     "duration": 0.026206,
     "end_time": "2025-03-26T08:04:00.149272",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.123066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique_clusters=unique_clusters[1:]\n",
    "# # new_unique_clusters=[];\n",
    "\n",
    "# # new_unique_clusters.append(unique_clusters[-1])\n",
    "# # new_unique_clusters.extend(unique_clusters)\n",
    "# # unique_clusters=new_unique_clusters\n",
    "# # print(len(unique_clusters))\n",
    "# # for x in unique_clusters:\n",
    "# #     print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d63c56b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.188761Z",
     "iopub.status.busy": "2025-03-26T08:04:00.188449Z",
     "iopub.status.idle": "2025-03-26T08:04:00.191788Z",
     "shell.execute_reply": "2025-03-26T08:04:00.190916Z"
    },
    "papermill": {
     "duration": 0.024745,
     "end_time": "2025-03-26T08:04:00.193235",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.168490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in unique_clusters:\n",
    "#     print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e14abc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.233107Z",
     "iopub.status.busy": "2025-03-26T08:04:00.232753Z",
     "iopub.status.idle": "2025-03-26T08:04:00.236358Z",
     "shell.execute_reply": "2025-03-26T08:04:00.235312Z"
    },
    "papermill": {
     "duration": 0.025117,
     "end_time": "2025-03-26T08:04:00.237827",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.212710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for sublist in unique_clusters[5]:\n",
    "#     for element in sublist:\n",
    "#         result.append([element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88b3a9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.277306Z",
     "iopub.status.busy": "2025-03-26T08:04:00.277011Z",
     "iopub.status.idle": "2025-03-26T08:04:00.280267Z",
     "shell.execute_reply": "2025-03-26T08:04:00.279559Z"
    },
    "papermill": {
     "duration": 0.024232,
     "end_time": "2025-03-26T08:04:00.281500",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.257268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique_clusters.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a76e0356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.319407Z",
     "iopub.status.busy": "2025-03-26T08:04:00.319173Z",
     "iopub.status.idle": "2025-03-26T08:04:00.321968Z",
     "shell.execute_reply": "2025-03-26T08:04:00.321374Z"
    },
    "papermill": {
     "duration": 0.023068,
     "end_time": "2025-03-26T08:04:00.323225",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.300157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in unique_clusters:\n",
    "#     for y in x:\n",
    "#         print(\"cluster:---------------------------------------\" )\n",
    "#         for z in y:\n",
    "#             print(class_names[z]);\n",
    "        \n",
    "#     print(\"-------------------------------------level------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c57d6",
   "metadata": {
    "papermill": {
     "duration": 0.018853,
     "end_time": "2025-03-26T08:04:00.361395",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.342542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3d42139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.399931Z",
     "iopub.status.busy": "2025-03-26T08:04:00.399592Z",
     "iopub.status.idle": "2025-03-26T08:04:00.402850Z",
     "shell.execute_reply": "2025-03-26T08:04:00.402227Z"
    },
    "papermill": {
     "duration": 0.024429,
     "end_time": "2025-03-26T08:04:00.404150",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.379721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# enc_sizes=[32,64,64]\n",
    "# num_classes_lim=100;\n",
    "# counter_1=0;\n",
    "# # model=best_model\n",
    "# for level in range(len(unique_clusters)):\n",
    "#     clusters=unique_clusters[level]\n",
    "#     original_to_new_label = {}\n",
    "#     new_label_to_cluster = {}\n",
    "#     # print('training for', clusters)\n",
    "#     for new_label, label_group in enumerate(clusters):\n",
    "       \n",
    "#         for original_label in label_group:\n",
    "#             original_to_new_label[original_label] = new_label\n",
    "#         new_label_to_cluster[new_label] = label_group\n",
    "#     num_classes_current = len(clusters)\n",
    "#     # print(num_classes_current)\n",
    "#     train_labels = np.asarray([original_to_new_label[l[1]] for l in train_loader.dataset])\n",
    "#     updated_train_dataset=update_data_labels(train_loader.dataset,train_labels, num_classes_current)\n",
    "#     val_labels = np.asarray([original_to_new_label[l[1]] for l in val_loader.dataset])\n",
    "#     updated_val_dataset=update_data_labels(val_loader.dataset,val_labels, num_classes_current)\n",
    "#     temp_train_dataloader=DataLoader(updated_train_dataset,batch_size=64,shuffle=True)\n",
    "#     temp_val_dataloader=DataLoader(updated_val_dataset,batch_size=64,shuffle=True)\n",
    "    \n",
    "#     if level>0:\n",
    "#         # print('no copy')\n",
    "#         model_prev=copy.deepcopy(model)\n",
    "#         model=CNN2(3,enc_sizes,num_classes=num_classes_current);\n",
    "#         model.encoder=model_prev.encoder;\n",
    "#     else:\n",
    "#         model=CNN2(3,enc_sizes,num_classes=num_classes_current);\n",
    "    \n",
    "#     best_model=train_model_extra_metric(model,temp_train_dataloader,temp_val_dataloader,num_epochs=3,device=device)\n",
    "#     counter_1+=1;\n",
    "#     # if num_classes_current == num_classes_lim:\n",
    "#     #     break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "943e6e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.442254Z",
     "iopub.status.busy": "2025-03-26T08:04:00.442017Z",
     "iopub.status.idle": "2025-03-26T08:04:00.444746Z",
     "shell.execute_reply": "2025-03-26T08:04:00.444113Z"
    },
    "papermill": {
     "duration": 0.023127,
     "end_time": "2025-03-26T08:04:00.446076",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.422949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_labels = np.asarray([original_to_new_label[l[1]] for l in test_loader.dataset])\n",
    "# updated_test_dataset=update_data_labels(test_loader.dataset,test_labels, num_classes_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f7244aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.485193Z",
     "iopub.status.busy": "2025-03-26T08:04:00.484883Z",
     "iopub.status.idle": "2025-03-26T08:04:00.488163Z",
     "shell.execute_reply": "2025-03-26T08:04:00.487303Z"
    },
    "papermill": {
     "duration": 0.02443,
     "end_time": "2025-03-26T08:04:00.489529",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.465099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test_labels = np.asarray([original_to_new_label[l[1]] for l in test_loader.dataset])\n",
    "# temp_test_dataloader=DataLoader(updated_test_dataset,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71221b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.528868Z",
     "iopub.status.busy": "2025-03-26T08:04:00.528517Z",
     "iopub.status.idle": "2025-03-26T08:04:00.531873Z",
     "shell.execute_reply": "2025-03-26T08:04:00.531040Z"
    },
    "papermill": {
     "duration": 0.02444,
     "end_time": "2025-03-26T08:04:00.533242",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.508802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_model(best_model,temp_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d29f4c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.572783Z",
     "iopub.status.busy": "2025-03-26T08:04:00.572465Z",
     "iopub.status.idle": "2025-03-26T08:04:00.576137Z",
     "shell.execute_reply": "2025-03-26T08:04:00.575280Z"
    },
    "papermill": {
     "duration": 0.024898,
     "end_time": "2025-03-26T08:04:00.577404",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.552506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# enc_sizes=[32,64,128]\n",
    "# num_classes_lim=100;\n",
    "# counter_1=0;\n",
    "\n",
    "# for level in range(len(unique_clusters)):\n",
    "#     clusters=unique_clusters[level]\n",
    "#     original_to_new_label = {}\n",
    "#     new_label_to_cluster = {}\n",
    "#     # print('training for', clusters)\n",
    "#     for new_label, label_group in enumerate(clusters):\n",
    "       \n",
    "#         for original_label in label_group:\n",
    "#             original_to_new_label[original_label] = new_label\n",
    "#         new_label_to_cluster[new_label] = label_group\n",
    "#     num_classes_current = len(clusters)\n",
    "#     # print(num_classes_current)\n",
    "#     train_labels = np.asarray([original_to_new_label[l[1]] for l in train_loader.dataset])\n",
    "#     updated_train_dataset=update_data_labels(train_loader.dataset,train_labels, num_classes_current)\n",
    "#     val_labels = np.asarray([original_to_new_label[l[1]] for l in val_loader.dataset])\n",
    "#     updated_val_dataset=update_data_labels(val_loader.dataset,val_labels, num_classes_current)\n",
    "#     temp_train_dataloader=DataLoader(updated_train_dataset,batch_size=64,shuffle=True)\n",
    "#     temp_val_dataloader=DataLoader(updated_val_dataset,batch_size=64,shuffle=True)\n",
    "    \n",
    "#     if level>0:\n",
    "#         model_prev=copy.deepcopy(model)\n",
    "#         model=CNN(3,enc_sizes,num_classes=num_classes_current);\n",
    "#         model.encoder=model_prev.encoder;\n",
    "#     else:\n",
    "#         model=CNN(3,enc_sizes,num_classes=num_classes_current);\n",
    "    \n",
    "#     best_model=train_model(model,temp_train_dataloader,temp_val_dataloader,num_epochs=5,device=device)\n",
    "#     counter_1+=1;\n",
    "#     if num_classes_current == num_classes_lim:\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6af5907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.617269Z",
     "iopub.status.busy": "2025-03-26T08:04:00.616973Z",
     "iopub.status.idle": "2025-03-26T08:04:00.622546Z",
     "shell.execute_reply": "2025-03-26T08:04:00.621856Z"
    },
    "papermill": {
     "duration": 0.026845,
     "end_time": "2025-03-26T08:04:00.623816",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.596971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN2(\n",
       "  (encoder): Encoder(\n",
       "    (encoced_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictor): Predictor2(\n",
       "    (predictor): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=53, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab8975ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.664303Z",
     "iopub.status.busy": "2025-03-26T08:04:00.664008Z",
     "iopub.status.idle": "2025-03-26T08:04:00.667380Z",
     "shell.execute_reply": "2025-03-26T08:04:00.666679Z"
    },
    "papermill": {
     "duration": 0.024899,
     "end_time": "2025-03-26T08:04:00.668540",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.643641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# enc_sizes=[32,64,128]\n",
    "# num_classes_lim=10;\n",
    "# counter_1=0;\n",
    "# for level in range(len(unique_clusters)):\n",
    "#     clusters=unique_clusters[level]\n",
    "#     original_to_new_label = {}\n",
    "#     new_label_to_cluster = {}\n",
    "#     print('training for', clusters)\n",
    "#     for new_label, label_group in enumerate(clusters):\n",
    "       \n",
    "#         for original_label in label_group:\n",
    "#             original_to_new_label[original_label] = new_label\n",
    "#         new_label_to_cluster[new_label] = label_group\n",
    "#     num_classes_current = len(clusters)\n",
    "#     # print(num_classes_current)\n",
    "#     train_labels = np.asarray([original_to_new_label[l[1]] for l in train_loader.dataset])\n",
    "#     updated_train_dataset=update_data_labels(train_loader.dataset,train_labels, num_classes_current)\n",
    "#     val_labels = np.asarray([original_to_new_label[l[1]] for l in val_loader.dataset])\n",
    "#     updated_val_dataset=update_data_labels(val_loader.dataset,val_labels, num_classes_current)\n",
    "#     temp_train_dataloader=DataLoader(updated_train_dataset,batch_size=64,shuffle=True)\n",
    "#     temp_val_dataloader=DataLoader(updated_val_dataset,batch_size=64,shuffle=True)\n",
    "    \n",
    "#     if level>0:\n",
    "#         model_prev=copy.deepcopy(model)\n",
    "#         model=CNN(3,enc_sizes,num_classes=num_classes_current);\n",
    "#         # model.encoder=model_prev.encoder;\n",
    "#     else:\n",
    "#         model=CNN(3,enc_sizes,num_classes=num_classes_current);\n",
    "\n",
    "#     train_model(model,temp_train_dataloader,temp_val_dataloader,num_epochs=10,device=device)\n",
    "#     counter_1+=1;\n",
    "#     if num_classes_current == num_classes_lim:\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83795a",
   "metadata": {
    "papermill": {
     "duration": 0.018686,
     "end_time": "2025-03-26T08:04:00.706138",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.687452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b227c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.744479Z",
     "iopub.status.busy": "2025-03-26T08:04:00.744193Z",
     "iopub.status.idle": "2025-03-26T08:04:00.747475Z",
     "shell.execute_reply": "2025-03-26T08:04:00.746739Z"
    },
    "papermill": {
     "duration": 0.02395,
     "end_time": "2025-03-26T08:04:00.748782",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.724832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enc_sizes=[32,64,64]\n",
    "# baseline_model=CNN2(3,enc_sizes,num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33e0a07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.787426Z",
     "iopub.status.busy": "2025-03-26T08:04:00.787162Z",
     "iopub.status.idle": "2025-03-26T08:04:00.790319Z",
     "shell.execute_reply": "2025-03-26T08:04:00.789683Z"
    },
    "papermill": {
     "duration": 0.023908,
     "end_time": "2025-03-26T08:04:00.791515",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.767607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_baseline_model=train_model_extra_metric(baseline_model,train_loader,val_loader,num_epochs=50,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dba45e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T08:04:00.830666Z",
     "iopub.status.busy": "2025-03-26T08:04:00.830390Z",
     "iopub.status.idle": "2025-03-26T08:04:00.833586Z",
     "shell.execute_reply": "2025-03-26T08:04:00.832832Z"
    },
    "papermill": {
     "duration": 0.024092,
     "end_time": "2025-03-26T08:04:00.834813",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.810721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_model(baseline_model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53aa61",
   "metadata": {
    "papermill": {
     "duration": 0.018722,
     "end_time": "2025-03-26T08:04:00.872494",
     "exception": false,
     "start_time": "2025-03-26T08:04:00.853772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2579480,
     "sourceId": 4532039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3999173,
     "sourceId": 6961629,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6528196,
     "sourceId": 10550934,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6937475,
     "sourceId": 11165719,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4383.612949,
   "end_time": "2025-03-26T08:04:03.344210",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-26T06:50:59.731261",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
