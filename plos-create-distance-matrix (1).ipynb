{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d82651f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:08.725730Z",
     "iopub.status.busy": "2025-04-03T13:42:08.725353Z",
     "iopub.status.idle": "2025-04-03T13:42:18.966016Z",
     "shell.execute_reply": "2025-04-03T13:42:18.964991Z"
    },
    "papermill": {
     "duration": 10.250734,
     "end_time": "2025-04-03T13:42:18.967930",
     "exception": false,
     "start_time": "2025-04-03T13:42:08.717196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45840c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:18.981862Z",
     "iopub.status.busy": "2025-04-03T13:42:18.981372Z",
     "iopub.status.idle": "2025-04-03T13:42:18.990641Z",
     "shell.execute_reply": "2025-04-03T13:42:18.989741Z"
    },
    "papermill": {
     "duration": 0.017912,
     "end_time": "2025-04-03T13:42:18.992193",
     "exception": false,
     "start_time": "2025-04-03T13:42:18.974281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,encoder_sizes,*args,**kwargs):\n",
    "        super().__init__()\n",
    "        self.encoced_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(encoder_sizes, encoder_sizes[1:])])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoced_blocks(x)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self,num_classes,connector_size):\n",
    "        super().__init__();\n",
    "        self.predictor=nn.Sequential(nn.Linear(connector_size * 4 * 4, 256),nn.ReLU(),nn.Dropout(0.5),nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "        \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_c,enc_sizes, num_classes=10,activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "\n",
    "        self.encoder = Encoder(self.enc_sizes, activation=activation)\n",
    "        self.predictor=Predictor(num_classes,self.enc_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.predictor(x);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e690e926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.005063Z",
     "iopub.status.busy": "2025-04-03T13:42:19.004782Z",
     "iopub.status.idle": "2025-04-03T13:42:19.013297Z",
     "shell.execute_reply": "2025-04-03T13:42:19.012231Z"
    },
    "papermill": {
     "duration": 0.016623,
     "end_time": "2025-04-03T13:42:19.014819",
     "exception": false,
     "start_time": "2025-04-03T13:42:18.998196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,encoder_sizes,*args,**kwargs):\n",
    "        super().__init__()\n",
    "        self.encoced_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(encoder_sizes, encoder_sizes[1:])])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoced_blocks(x)\n",
    "\n",
    "class Predictor2(nn.Module):\n",
    "    def __init__(self,num_classes,connector_size):\n",
    "        super().__init__();\n",
    "        self.predictor=nn.Sequential(nn.Linear(connector_size * 4 * 4, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "        \n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, in_c,enc_sizes, num_classes=10,activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "\n",
    "        self.encoder = Encoder(self.enc_sizes, activation=activation)\n",
    "        self.predictor=Predictor2(num_classes,self.enc_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.predictor(x);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869cf44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.027533Z",
     "iopub.status.busy": "2025-04-03T13:42:19.027273Z",
     "iopub.status.idle": "2025-04-03T13:42:19.249783Z",
     "shell.execute_reply": "2025-04-03T13:42:19.248938Z"
    },
    "papermill": {
     "duration": 0.230876,
     "end_time": "2025-04-03T13:42:19.251642",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.020766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train_model_extra_metric(model, train_loader, val_loader, num_epochs=10, lr=0.001,lambda_reg=0.01, device='cpu', early_stopping_patience=5):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with validation and calculates precision, recall, and F1-score.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        val_loader (DataLoader): The validation data loader.\n",
    "        num_epochs (int, optional): The number of epochs to train for. Defaults to 10.\n",
    "        lr (float, optional): The learning rate. Defaults to 0.001.\n",
    "        device (str, optional): The device to use for training ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        early_stopping_patience (int, optional): Number of epochs to wait before early stopping. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_model, dict_metrics) where dict_metrics contains training history\n",
    "    \"\"\"\n",
    "    # Initialize model and optimization\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_f1 = 0\n",
    "    best_val=0\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            # loss += lambda_reg * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_targets = []\n",
    "            all_predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    predictions = outputs.argmax(dim=1)\n",
    "                    \n",
    "                    correct += (predictions == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "                    \n",
    "                    all_targets.extend(targets.cpu().numpy())\n",
    "                    all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_accuracy = correct / total\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_targets, \n",
    "                all_predictions, \n",
    "                average='weighted', \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Update history\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['val_precision'].append(precision)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_f1'].append(f1)\n",
    "            \n",
    "            # # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Model checkpoint and early stopping\n",
    "            if val_accuracy>best_val:\n",
    "                best_model=copy.deepcopy(model)\n",
    "                best_val=val_accuracy\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping check\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                # print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691921d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.266411Z",
     "iopub.status.busy": "2025-04-03T13:42:19.266123Z",
     "iopub.status.idle": "2025-04-03T13:42:19.282797Z",
     "shell.execute_reply": "2025-04-03T13:42:19.281988Z"
    },
    "papermill": {
     "duration": 0.025032,
     "end_time": "2025-04-03T13:42:19.284142",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.259110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train_model_memory_optimized(model, train_loader, val_loader, num_epochs=10, lr=0.001, \n",
    "                                lambda_reg=0.01, device='cpu', early_stopping_patience=5,\n",
    "                                accumulation_steps=1, use_mixed_precision=True):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with validation and memory optimization techniques.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        val_loader (DataLoader): The validation data loader.\n",
    "        num_epochs (int, optional): The number of epochs to train for. Defaults to 10.\n",
    "        lr (float, optional): The learning rate. Defaults to 0.001.\n",
    "        lambda_reg (float, optional): L2 regularization strength. Defaults to 0.01.\n",
    "        device (str, optional): The device to use for training ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        early_stopping_patience (int, optional): Number of epochs to wait before early stopping. Defaults to 5.\n",
    "        accumulation_steps (int, optional): Number of steps to accumulate gradients. Defaults to 1.\n",
    "        use_mixed_precision (bool, optional): Whether to use mixed precision training. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_model_state_dict, dict_metrics) where dict_metrics contains training history\n",
    "    \"\"\"\n",
    "    # Initialize model and optimization\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = GradScaler() if use_mixed_precision and device.startswith('cuda') else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val = 0\n",
    "    best_model_state_dict = None\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if use_mixed_precision and device.startswith('cuda'):\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    # Scale loss for gradient accumulation\n",
    "                    loss = loss / accumulation_steps\n",
    "                \n",
    "                # Scales loss and calls backward()\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Step with gradient accumulation\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    # More efficient than zero_grad()\n",
    "                    for param in model.parameters():\n",
    "                        param.grad = None\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                # Step with gradient accumulation\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    # More efficient than zero_grad()\n",
    "                    for param in model.parameters():\n",
    "                        param.grad = None\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Explicitly delete intermediate tensors\n",
    "            del inputs, outputs, targets, loss\n",
    "            \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # More memory-efficient tracking of predictions\n",
    "            all_targets = torch.tensor([], dtype=torch.long, device='cpu')\n",
    "            all_predictions = torch.tensor([], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    \n",
    "                    # For validation, always use full precision\n",
    "                    outputs = model(inputs)\n",
    "                    predictions = outputs.argmax(dim=1)\n",
    "                    \n",
    "                    correct += (predictions == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "                    \n",
    "                    # Move to CPU before concatenating\n",
    "                    all_targets = torch.cat([all_targets, targets.cpu()])\n",
    "                    all_predictions = torch.cat([all_predictions, predictions.cpu()])\n",
    "                    \n",
    "                    # Explicitly delete tensors\n",
    "                    del inputs, outputs, targets, predictions\n",
    "            \n",
    "            # Calculate metrics (convert to numpy only once)\n",
    "            val_accuracy = correct / total\n",
    "            all_targets_np = all_targets.numpy()\n",
    "            all_predictions_np = all_predictions.numpy()\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_targets_np, \n",
    "                all_predictions_np, \n",
    "                average='weighted', \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Update history\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['val_precision'].append(precision)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_f1'].append(f1)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Clear unnecessary variables\n",
    "            del all_targets, all_predictions, all_targets_np, all_predictions_np\n",
    "            \n",
    "            # Model checkpoint (save state_dict instead of whole model)\n",
    "            if val_accuracy > best_val:\n",
    "                best_model_state_dict = model.state_dict().copy()\n",
    "                best_val = val_accuracy\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping check\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_model_state_dict, history\n",
    "\n",
    "# Usage example:\n",
    "def use_optimized_training(model, train_loader, val_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Train with memory optimizations\n",
    "    best_model_state_dict, history = train_model_memory_optimized(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=0.001,\n",
    "        device=device,\n",
    "        early_stopping_patience=5,\n",
    "        accumulation_steps=4,  # Accumulate gradients over 4 batches\n",
    "        use_mixed_precision=True  # Use mixed precision if on CUDA\n",
    "    )\n",
    "    \n",
    "    # Load the best model state dict\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "    # Clean up memory before inference or further use\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac1a06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.296792Z",
     "iopub.status.busy": "2025-04-03T13:42:19.296459Z",
     "iopub.status.idle": "2025-04-03T13:42:19.303905Z",
     "shell.execute_reply": "2025-04-03T13:42:19.303119Z"
    },
    "papermill": {
     "duration": 0.015215,
     "end_time": "2025-04-03T13:42:19.305255",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.290040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val=0;\n",
    "    best_model=model;\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            # loss += 0.01 * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        # Validation (optional in minimal version)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        val_accuracy = correct / total\n",
    "        if val_accuracy>best_val:\n",
    "            best_model=copy.deepcopy(model)\n",
    "            best_val=val_accuracy\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    return best_model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8731f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.317827Z",
     "iopub.status.busy": "2025-04-03T13:42:19.317498Z",
     "iopub.status.idle": "2025-04-03T13:42:19.322589Z",
     "shell.execute_reply": "2025-04-03T13:42:19.321642Z"
    },
    "papermill": {
     "duration": 0.012921,
     "end_time": "2025-04-03T13:42:19.324128",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.311207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model,test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    test_accuracy = correct / total\n",
    "    print('test accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb18702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.336736Z",
     "iopub.status.busy": "2025-04-03T13:42:19.336381Z",
     "iopub.status.idle": "2025-04-03T13:42:19.347913Z",
     "shell.execute_reply": "2025-04-03T13:42:19.346950Z"
    },
    "papermill": {
     "duration": 0.019479,
     "end_time": "2025-04-03T13:42:19.349400",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.329921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_keys(original_dict, keys_to_remove):\n",
    "    # Create a new dictionary by excluding the specified keys\n",
    "    return {key: value for key, value in original_dict.items() if key not in keys_to_remove}\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def state_dict_to_vector(state_dict, remove_keys=[]):\n",
    "\n",
    "    shared_state_dict = copy.deepcopy(state_dict)\n",
    "\n",
    "    for key in remove_keys:\n",
    "\n",
    "        if key in shared_state_dict:\n",
    "\n",
    "            del shared_state_dict[key]\n",
    "\n",
    "    sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n",
    "\n",
    "    return torch.nn.utils.parameters_to_vector([value.reshape(-1) for key, value in sorted_shared_state_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vector_to_state_dict(vector, state_dict, remove_keys=[]):\n",
    "\n",
    "    # create a reference dict to define the order of the vector\n",
    "\n",
    "    reference_dict = copy.deepcopy(state_dict)\n",
    "\n",
    "    for key in remove_keys:\n",
    "\n",
    "        if key in reference_dict:\n",
    "\n",
    "            del reference_dict[key]\n",
    "\n",
    "    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n",
    "\n",
    "\n",
    "\n",
    "    # create a shared state dict using the refence dict\n",
    "\n",
    "    torch.nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "    # add back the encoder and decoder embedding weights.\n",
    "\n",
    "    if \"transformer.shared.weight\" in sorted_reference_dict:\n",
    "\n",
    "        for key in remove_keys:\n",
    "\n",
    "            sorted_reference_dict[key] = sorted_reference_dict[\"transformer.shared.weight\"]\n",
    "\n",
    "    return sorted_reference_dict\n",
    "\n",
    "import torch\n",
    "\n",
    "def count_sign_conflicts(tensor: torch.Tensor) -> int:\n",
    "    # Get the sign of each element in the tensor\n",
    "    # -1 for negative, 1 for positive, and 0 for zero\n",
    "    signs = torch.sign(tensor).to(device)\n",
    "\n",
    "    # Create a mask for non-zero entries\n",
    "    non_zero_mask = (signs != 0)\n",
    "\n",
    "    # Check for sign conflicts, ignoring zeros\n",
    "    negative_mask = (signs == -1) & non_zero_mask\n",
    "    positive_mask = (signs == 1) & non_zero_mask\n",
    "\n",
    "    # A conflict occurs if a column contains both -1 and 1\n",
    "    conflict_mask = negative_mask.any(dim=0) & positive_mask.any(dim=0)\n",
    "\n",
    "    # Count the number of columns with sign conflicts\n",
    "    num_conflicts = conflict_mask.sum().item()\n",
    "\n",
    "    return num_conflicts\n",
    "\n",
    "def calc_sign_conflicts(model_list,dummy_model,exclude_keys,model_class,number_of_tasks,is_layer=False, is_transformer=False):\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new=remove_keys(dummy_model.state_dict(),exclude_keys)\n",
    "    flat_ptm= state_dict_to_vector(dm_new)\n",
    "\n",
    "    if is_layer==True:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(model_list[x] )for x in range(number_of_tasks)]);\n",
    "    else:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x],exclude_keys) )for x in range(number_of_tasks)]);\n",
    "    flat_ptm=flat_ptm.to('cpu')\n",
    "    flat_ft=flat_ft.to('cpu')\n",
    "    tv_flat_checks = flat_ft - flat_ptm;\n",
    "\n",
    "    # torch.save(tv_flat_checks,model_name+\"_\"+task_name+\"_\"+str(number_of_tasks)+\"_\"+version+\".pt\");\n",
    "    # plot_mds_torch(tv_flat_checks);\n",
    "    # plot_tsne_torch(tv_flat_checks)\n",
    "    # flipped=flip(tv_flat_checks)\n",
    "    # plot_mds_torch(flipped)\n",
    "    # plot_tsne_torch(flipped)\n",
    "\n",
    "    res,count1= count_sign_conflicts(tv_flat_checks)/tv_flat_checks.shape[1], tv_flat_checks.shape[1];\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return res, count1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb32676c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.362092Z",
     "iopub.status.busy": "2025-04-03T13:42:19.361795Z",
     "iopub.status.idle": "2025-04-03T13:42:19.368356Z",
     "shell.execute_reply": "2025-04-03T13:42:19.367351Z"
    },
    "papermill": {
     "duration": 0.014302,
     "end_time": "2025-04-03T13:42:19.369671",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.355369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def distance_of_task_vectors(task_vectors):\n",
    " \n",
    "    res= ((task_vectors[0] - task_vectors[1]))**2\n",
    "\n",
    "    res=torch.sum(res)\n",
    "  \n",
    "    return res\n",
    "def calc_distance(model_list,dummy_model,exclude_keys,model_class,number_of_tasks,is_layer=False, is_transformer=False):\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new=remove_keys(dummy_model.state_dict(),exclude_keys)\n",
    "    flat_ptm= state_dict_to_vector(dm_new)\n",
    "\n",
    "    if is_layer==True:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(model_list[x] )for x in range(number_of_tasks)]);\n",
    "    else:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x],exclude_keys) )for x in range(number_of_tasks)]);\n",
    "    flat_ptm=flat_ptm.to('cpu')\n",
    "    flat_ft=flat_ft.to('cpu')\n",
    "    tv_flat_checks = flat_ft - flat_ptm;\n",
    "\n",
    "    # torch.save(tv_flat_checks,model_name+\"_\"+task_name+\"_\"+str(number_of_tasks)+\"_\"+version+\".pt\");\n",
    "    # plot_mds_torch(tv_flat_checks);\n",
    "    # plot_tsne_torch(tv_flat_checks)\n",
    "    # flipped=flip(tv_flat_checks)\n",
    "    # plot_mds_torch(flipped)\n",
    "    # plot_tsne_torch(flipped)\n",
    "    \n",
    "    res= distance_of_task_vectors(tv_flat_checks)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return res,3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d899de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.382240Z",
     "iopub.status.busy": "2025-04-03T13:42:19.381960Z",
     "iopub.status.idle": "2025-04-03T13:42:19.388530Z",
     "shell.execute_reply": "2025-04-03T13:42:19.387646Z"
    },
    "papermill": {
     "duration": 0.014497,
     "end_time": "2025-04-03T13:42:19.390014",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.375517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_of_task_vectors(task_vectors):\n",
    "    # Normalize the vectors\n",
    "    task_vectors_0 = task_vectors[0] / torch.norm(task_vectors[0], dim=-1, keepdim=True)\n",
    "    task_vectors_1 = task_vectors[1] / torch.norm(task_vectors[1], dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = F.cosine_similarity(task_vectors_0, task_vectors_1, dim=-1)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "def calc_similarity(model_list, dummy_model, exclude_keys, model_class, number_of_tasks,is_layer=False, is_transformer=False):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new = remove_keys(dummy_model.state_dict(), exclude_keys)\n",
    "    flat_ptm = state_dict_to_vector(dm_new)\n",
    "    if is_layer==True:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(model_list[x] )for x in range(number_of_tasks)]);\n",
    "    else:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x],exclude_keys) )for x in range(number_of_tasks)]);\n",
    "    flat_ft=flat_ft.to('cpu')\n",
    "    tv_flat_checks = flat_ft - flat_ptm\n",
    "    \n",
    "    # Use cosine similarity instead of Euclidean distance\n",
    "    res = cosine_similarity_of_task_vectors(tv_flat_checks)\n",
    "\n",
    "    return res,3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85d6b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.402418Z",
     "iopub.status.busy": "2025-04-03T13:42:19.402153Z",
     "iopub.status.idle": "2025-04-03T13:42:19.408345Z",
     "shell.execute_reply": "2025-04-03T13:42:19.407416Z"
    },
    "papermill": {
     "duration": 0.014025,
     "end_time": "2025-04-03T13:42:19.409882",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.395857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def distance_of_task_vectors_euc(task_vectors):\n",
    " \n",
    "    res= ((task_vectors[0] - task_vectors[1]))**2\n",
    "\n",
    "    res=torch.sqrt(torch.sum(res))\n",
    "  \n",
    "    return res\n",
    "def calc_distance_euc(model_list,dummy_model,exclude_keys,model_class,number_of_tasks,is_layer=False, is_transformer=False):\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new=remove_keys(dummy_model.state_dict(),exclude_keys)\n",
    "    flat_ptm= state_dict_to_vector(dm_new)\n",
    "\n",
    "    if is_layer==True:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(model_list[x] )for x in range(number_of_tasks)]);\n",
    "    else:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x],exclude_keys) )for x in range(number_of_tasks)]);\n",
    "    flat_ptm=flat_ptm.to('cpu')\n",
    "    flat_ft=flat_ft.to('cpu')\n",
    "    tv_flat_checks = flat_ft - flat_ptm;\n",
    "\n",
    "    # torch.save(tv_flat_checks,model_name+\"_\"+task_name+\"_\"+str(number_of_tasks)+\"_\"+version+\".pt\");\n",
    "    # plot_mds_torch(tv_flat_checks);\n",
    "    # plot_tsne_torch(tv_flat_checks)\n",
    "    # flipped=flip(tv_flat_checks)\n",
    "    # plot_mds_torch(flipped)\n",
    "    # plot_tsne_torch(flipped)\n",
    "    \n",
    "    res= distance_of_task_vectors_euc(tv_flat_checks)\n",
    "    # print(res)\n",
    "\n",
    "    return res,3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf2b802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.422747Z",
     "iopub.status.busy": "2025-04-03T13:42:19.422419Z",
     "iopub.status.idle": "2025-04-03T13:42:19.428301Z",
     "shell.execute_reply": "2025-04-03T13:42:19.427358Z"
    },
    "papermill": {
     "duration": 0.013896,
     "end_time": "2025-04-03T13:42:19.429800",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.415904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_simdis(model_list,dummy_model,exclude_keys,model_class,number_of_tasks,is_layer=False, is_transformer=False):\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # dummy_model=model_class(3,enc_sizes,num_classes=2).to(device)  \n",
    "    dm_new=remove_keys(dummy_model.state_dict(),exclude_keys)\n",
    "    flat_ptm= state_dict_to_vector(dm_new)\n",
    "    if is_layer==True:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(model_list[x] )for x in range(number_of_tasks)]);\n",
    "    else:\n",
    "        flat_ft = torch.vstack([state_dict_to_vector(remove_keys(model_list[x].state_dict(),exclude_keys) )for x in range(number_of_tasks)]);\n",
    "    flat_ft=flat_ft.to('cpu')\n",
    "\n",
    "    tv_flat_checks = flat_ft - flat_ptm;\n",
    "    # torch.save(tv_flat_checks,model_name+\"_\"+task_name+\"_\"+str(number_of_tasks)+\"_\"+version+\".pt\");\n",
    "    # plot_mds_torch(tv_flat_checks);\n",
    "    # plot_tsne_torch(tv_flat_checks)\n",
    "    # flipped=flip(tv_flat_checks)\n",
    "    # plot_mds_torch(flipped)\n",
    "    # plot_tsne_torch(flipped)\n",
    "    return (count_sign_conflicts(tv_flat_checks)/tv_flat_checks.shape[1])+distance_of_task_vectors_euc(tv_flat_checks), tv_flat_checks.shape[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbac711",
   "metadata": {
    "papermill": {
     "duration": 0.005508,
     "end_time": "2025-04-03T13:42:19.441013",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.435505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8199bf77",
   "metadata": {
    "papermill": {
     "duration": 0.005663,
     "end_time": "2025-04-03T13:42:19.452694",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.447031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b2f02",
   "metadata": {
    "papermill": {
     "duration": 0.005647,
     "end_time": "2025-04-03T13:42:19.464128",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.458481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cifar 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13217a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.477382Z",
     "iopub.status.busy": "2025-04-03T13:42:19.477036Z",
     "iopub.status.idle": "2025-04-03T13:42:19.490813Z",
     "shell.execute_reply": "2025-04-03T13:42:19.489787Z"
    },
    "papermill": {
     "duration": 0.022302,
     "end_time": "2025-04-03T13:42:19.492384",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.470082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Load CIFAR-10 Dataset\n",
    "def load_cifar100(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize with CIFAR-10 stats\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split train dataset into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset.classes),train_dataset.classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class BinaryCIFAR100(Dataset):\n",
    "    def __init__(self, dataset, target_label, undersample_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Original CIFAR100 dataset\n",
    "            target_label: The positive class label\n",
    "            undersample_ratio: Ratio of negative to positive samples (e.g., 1.0 means balanced)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.target_label = target_label\n",
    "        \n",
    "        # Separate positive and negative indices\n",
    "        self.positive_indices = []\n",
    "        self.negative_indices = []\n",
    "        \n",
    "        for idx in range(len(dataset)):\n",
    "            _, label = dataset[idx]\n",
    "            if label == target_label:\n",
    "                self.positive_indices.append(idx)\n",
    "            else:\n",
    "                self.negative_indices.append(idx)\n",
    "        \n",
    "        # Undersample negative class\n",
    "        num_positive = len(self.positive_indices)\n",
    "        num_negative_keep = int(num_positive * undersample_ratio)\n",
    "        \n",
    "        # Randomly select negative samples\n",
    "        if len(self.negative_indices) > num_negative_keep:\n",
    "            self.negative_indices = np.random.choice(\n",
    "                self.negative_indices, \n",
    "                size=num_negative_keep, \n",
    "                replace=False\n",
    "            ).tolist()\n",
    "        \n",
    "        # Combine indices\n",
    "        self.used_indices = self.positive_indices + self.negative_indices\n",
    "        \n",
    "        print(f\"Task {target_label}: Positive samples: {len(self.positive_indices)}, \"\n",
    "              f\"Negative samples: {len(self.negative_indices)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.used_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.used_indices[idx]\n",
    "        img, label = self.dataset[original_idx]\n",
    "        binary_label = 1 if label == self.target_label else 0\n",
    "        return img, binary_label\n",
    "\n",
    "def load_cifar100_binary(batch_size=64, undersample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Load CIFAR100 as binary classification tasks with undersampling.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for DataLoader\n",
    "        undersample_ratio: Ratio of negative to positive samples (1.0 means balanced)\n",
    "    \n",
    "    Returns:\n",
    "        list of dictionaries containing train and val loaders for each binary task\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split train dataset into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    task_loaders = []\n",
    "    \n",
    "    for label in range(100):\n",
    "        # Create balanced datasets for this task\n",
    "        train_task_dataset = BinaryCIFAR100(\n",
    "            train_subset, \n",
    "            label, \n",
    "            undersample_ratio=undersample_ratio\n",
    "        )\n",
    "        val_task_dataset = BinaryCIFAR100(\n",
    "            val_subset, \n",
    "            label,\n",
    "            undersample_ratio=undersample_ratio\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader for this task\n",
    "        train_loader = DataLoader(\n",
    "            train_task_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_task_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        task_loaders.append({\"train\": train_loader, \"val\": val_loader})\n",
    "    \n",
    "    return task_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627289e",
   "metadata": {
    "papermill": {
     "duration": 0.005688,
     "end_time": "2025-04-03T13:42:19.504133",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.498445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mammals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d54bd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.517058Z",
     "iopub.status.busy": "2025-04-03T13:42:19.516744Z",
     "iopub.status.idle": "2025-04-03T13:42:19.521175Z",
     "shell.execute_reply": "2025-04-03T13:42:19.520300Z"
    },
    "papermill": {
     "duration": 0.01272,
     "end_time": "2025-04-03T13:42:19.522817",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.510097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2291cd98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.535667Z",
     "iopub.status.busy": "2025-04-03T13:42:19.535394Z",
     "iopub.status.idle": "2025-04-03T13:42:19.561004Z",
     "shell.execute_reply": "2025-04-03T13:42:19.560194Z"
    },
    "papermill": {
     "duration": 0.03363,
     "end_time": "2025-04-03T13:42:19.562492",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.528862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BinaryAnimals(Dataset):\n",
    "    def __init__(self, dataset, target_label, undersample_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Original Animals dataset\n",
    "            target_label: The positive class label\n",
    "            undersample_ratio: Ratio of negative to positive samples (e.g., 1.0 means balanced)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.target_label = target_label\n",
    "        \n",
    "        # Get all labels at once for faster processing\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            labels = torch.tensor([dataset.dataset.targets[i] for i in dataset.indices])\n",
    "        else:\n",
    "            labels = torch.tensor(dataset.targets)\n",
    "        \n",
    "        # Use tensor operations instead of loops for speed\n",
    "        self.positive_indices = torch.where(labels == target_label)[0].tolist()\n",
    "        self.negative_indices = torch.where(labels != target_label)[0].tolist()\n",
    "        \n",
    "        # Undersample negative class\n",
    "        num_positive = len(self.positive_indices)\n",
    "        num_negative_keep = int(num_positive * undersample_ratio)\n",
    "        \n",
    "        # Use numpy's faster random selection\n",
    "        if len(self.negative_indices) > num_negative_keep:\n",
    "            rng = np.random.default_rng()\n",
    "            self.negative_indices = rng.choice(\n",
    "                self.negative_indices, \n",
    "                size=num_negative_keep, \n",
    "                replace=False\n",
    "            ).tolist()\n",
    "        \n",
    "        # Pre-compute indices and binary labels - now using Long (int64) type\n",
    "        self.used_indices = self.positive_indices + self.negative_indices\n",
    "        self.binary_labels = torch.zeros(len(self.used_indices), dtype=torch.long)  # Changed to long\n",
    "        self.binary_labels[:len(self.positive_indices)] = 1\n",
    "        \n",
    "        print(f\"Task {target_label}: Positive samples: {len(self.positive_indices)}, \"\n",
    "              f\"Negative samples: {len(self.negative_indices)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.used_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.used_indices[idx]\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            img, _ = self.dataset[original_idx]\n",
    "        else:\n",
    "            img, _ = self.dataset[original_idx]\n",
    "        return img, self.binary_labels[idx]\n",
    "\n",
    "def load_animals_binary(data_path, batch_size=64, undersample_ratio=1.0, image_size=32, valid=False):\n",
    "    \"\"\"\n",
    "    Load Animals dataset as binary classification tasks with undersampling.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the animals dataset\n",
    "        batch_size: Batch size for DataLoader\n",
    "        undersample_ratio: Ratio of negative to positive samples (1.0 means balanced)\n",
    "        image_size: Size to resize images to\n",
    "    \n",
    "    Returns:\n",
    "        list of dictionaries containing train and val loaders for each binary task\n",
    "    \"\"\"\n",
    "    # Optimize transform pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        AddGaussianNoise(mean=0.0, std=0.1),\n",
    "    ])\n",
    "    \n",
    "    # Load full dataset\n",
    "    if valid==False:\n",
    "        full_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "        \n",
    "        # Get number of classes and class names\n",
    "        num_classes = len(full_dataset.classes)\n",
    "        class_names = full_dataset.classes\n",
    "        \n",
    "        # Create train/val split indices\n",
    "        indices = list(range(len(full_dataset)))\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            indices,\n",
    "            train_size=0.8,\n",
    "            stratify=[full_dataset.targets[i] for i in indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create train and validation datasets\n",
    "        \n",
    "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    else:\n",
    "        train_subset = datasets.ImageFolder(root=data_path+'/train', transform=transform)\n",
    "        val_subset = datasets.ImageFolder(root=data_path+'/valid', transform=transform)\n",
    "\n",
    "        # Get number of classes and class names\n",
    "        num_classes = len(train_subset.classes)\n",
    "        class_names = train_subset.classes\n",
    "    \n",
    "    # Loader settings for better performance\n",
    "    loader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 4,\n",
    "        'pin_memory': True,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 2\n",
    "    }\n",
    "    \n",
    "    task_loaders = []\n",
    "    \n",
    "    for label in range(num_classes):\n",
    "        # Create balanced datasets for this task\n",
    "        train_task_dataset = BinaryAnimals(\n",
    "            train_subset, \n",
    "            label, \n",
    "            undersample_ratio=undersample_ratio\n",
    "        )\n",
    "        val_task_dataset = BinaryAnimals(\n",
    "            val_subset, \n",
    "            label,\n",
    "            undersample_ratio=undersample_ratio\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader for this task\n",
    "        train_loader = DataLoader(\n",
    "            train_task_dataset, \n",
    "            shuffle=True,\n",
    "            **loader_kwargs\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_task_dataset, \n",
    "            shuffle=False,\n",
    "            **loader_kwargs\n",
    "        )\n",
    "        task_loaders.append({\"train\": train_loader, \"val\": val_loader})\n",
    "        # task_loaders.append({\n",
    "        #     \"train\": train_loader, \n",
    "        #     \"val\": val_loader,\n",
    "        #     \"class_name\": class_names[label]\n",
    "        # })\n",
    "    \n",
    "    return task_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a08615f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:42:19.575425Z",
     "iopub.status.busy": "2025-04-03T13:42:19.575164Z",
     "iopub.status.idle": "2025-04-03T14:02:16.672531Z",
     "shell.execute_reply": "2025-04-03T14:02:16.671418Z"
    },
    "papermill": {
     "duration": 1197.105239,
     "end_time": "2025-04-03T14:02:16.674034",
     "exception": false,
     "start_time": "2025-04-03T13:42:19.568795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:14<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Task 0: Positive samples: 383, Negative samples: 383\n",
      "Task 0: Positive samples: 117, Negative samples: 117\n",
      "Task 1: Positive samples: 390, Negative samples: 390\n",
      "Task 1: Positive samples: 110, Negative samples: 110\n",
      "Task 2: Positive samples: 391, Negative samples: 391\n",
      "Task 2: Positive samples: 109, Negative samples: 109\n",
      "Task 3: Positive samples: 407, Negative samples: 407\n",
      "Task 3: Positive samples: 93, Negative samples: 93\n",
      "Task 4: Positive samples: 404, Negative samples: 404\n",
      "Task 4: Positive samples: 96, Negative samples: 96\n",
      "Task 5: Positive samples: 402, Negative samples: 402\n",
      "Task 5: Positive samples: 98, Negative samples: 98\n",
      "Task 6: Positive samples: 390, Negative samples: 390\n",
      "Task 6: Positive samples: 110, Negative samples: 110\n",
      "Task 7: Positive samples: 409, Negative samples: 409\n",
      "Task 7: Positive samples: 91, Negative samples: 91\n",
      "Task 8: Positive samples: 420, Negative samples: 420\n",
      "Task 8: Positive samples: 80, Negative samples: 80\n",
      "Task 9: Positive samples: 397, Negative samples: 397\n",
      "Task 9: Positive samples: 103, Negative samples: 103\n",
      "Task 10: Positive samples: 397, Negative samples: 397\n",
      "Task 10: Positive samples: 103, Negative samples: 103\n",
      "Task 11: Positive samples: 402, Negative samples: 402\n",
      "Task 11: Positive samples: 98, Negative samples: 98\n",
      "Task 12: Positive samples: 407, Negative samples: 407\n",
      "Task 12: Positive samples: 93, Negative samples: 93\n",
      "Task 13: Positive samples: 401, Negative samples: 401\n",
      "Task 13: Positive samples: 99, Negative samples: 99\n",
      "Task 14: Positive samples: 382, Negative samples: 382\n",
      "Task 14: Positive samples: 118, Negative samples: 118\n",
      "Task 15: Positive samples: 400, Negative samples: 400\n",
      "Task 15: Positive samples: 100, Negative samples: 100\n",
      "Task 16: Positive samples: 406, Negative samples: 406\n",
      "Task 16: Positive samples: 94, Negative samples: 94\n",
      "Task 17: Positive samples: 407, Negative samples: 407\n",
      "Task 17: Positive samples: 93, Negative samples: 93\n",
      "Task 18: Positive samples: 403, Negative samples: 403\n",
      "Task 18: Positive samples: 97, Negative samples: 97\n",
      "Task 19: Positive samples: 405, Negative samples: 405\n",
      "Task 19: Positive samples: 95, Negative samples: 95\n",
      "Task 20: Positive samples: 392, Negative samples: 392\n",
      "Task 20: Positive samples: 108, Negative samples: 108\n",
      "Task 21: Positive samples: 399, Negative samples: 399\n",
      "Task 21: Positive samples: 101, Negative samples: 101\n",
      "Task 22: Positive samples: 387, Negative samples: 387\n",
      "Task 22: Positive samples: 113, Negative samples: 113\n",
      "Task 23: Positive samples: 403, Negative samples: 403\n",
      "Task 23: Positive samples: 97, Negative samples: 97\n",
      "Task 24: Positive samples: 386, Negative samples: 386\n",
      "Task 24: Positive samples: 114, Negative samples: 114\n",
      "Task 25: Positive samples: 396, Negative samples: 396\n",
      "Task 25: Positive samples: 104, Negative samples: 104\n",
      "Task 26: Positive samples: 395, Negative samples: 395\n",
      "Task 26: Positive samples: 105, Negative samples: 105\n",
      "Task 27: Positive samples: 404, Negative samples: 404\n",
      "Task 27: Positive samples: 96, Negative samples: 96\n",
      "Task 28: Positive samples: 397, Negative samples: 397\n",
      "Task 28: Positive samples: 103, Negative samples: 103\n",
      "Task 29: Positive samples: 406, Negative samples: 406\n",
      "Task 29: Positive samples: 94, Negative samples: 94\n",
      "Task 30: Positive samples: 411, Negative samples: 411\n",
      "Task 30: Positive samples: 89, Negative samples: 89\n",
      "Task 31: Positive samples: 412, Negative samples: 412\n",
      "Task 31: Positive samples: 88, Negative samples: 88\n",
      "Task 32: Positive samples: 408, Negative samples: 408\n",
      "Task 32: Positive samples: 92, Negative samples: 92\n",
      "Task 33: Positive samples: 394, Negative samples: 394\n",
      "Task 33: Positive samples: 106, Negative samples: 106\n",
      "Task 34: Positive samples: 399, Negative samples: 399\n",
      "Task 34: Positive samples: 101, Negative samples: 101\n",
      "Task 35: Positive samples: 405, Negative samples: 405\n",
      "Task 35: Positive samples: 95, Negative samples: 95\n",
      "Task 36: Positive samples: 404, Negative samples: 404\n",
      "Task 36: Positive samples: 96, Negative samples: 96\n",
      "Task 37: Positive samples: 409, Negative samples: 409\n",
      "Task 37: Positive samples: 91, Negative samples: 91\n",
      "Task 38: Positive samples: 398, Negative samples: 398\n",
      "Task 38: Positive samples: 102, Negative samples: 102\n",
      "Task 39: Positive samples: 401, Negative samples: 401\n",
      "Task 39: Positive samples: 99, Negative samples: 99\n",
      "Task 40: Positive samples: 400, Negative samples: 400\n",
      "Task 40: Positive samples: 100, Negative samples: 100\n",
      "Task 41: Positive samples: 402, Negative samples: 402\n",
      "Task 41: Positive samples: 98, Negative samples: 98\n",
      "Task 42: Positive samples: 416, Negative samples: 416\n",
      "Task 42: Positive samples: 84, Negative samples: 84\n",
      "Task 43: Positive samples: 399, Negative samples: 399\n",
      "Task 43: Positive samples: 101, Negative samples: 101\n",
      "Task 44: Positive samples: 406, Negative samples: 406\n",
      "Task 44: Positive samples: 94, Negative samples: 94\n",
      "Task 45: Positive samples: 409, Negative samples: 409\n",
      "Task 45: Positive samples: 91, Negative samples: 91\n",
      "Task 46: Positive samples: 394, Negative samples: 394\n",
      "Task 46: Positive samples: 106, Negative samples: 106\n",
      "Task 47: Positive samples: 404, Negative samples: 404\n",
      "Task 47: Positive samples: 96, Negative samples: 96\n",
      "Task 48: Positive samples: 418, Negative samples: 418\n",
      "Task 48: Positive samples: 82, Negative samples: 82\n",
      "Task 49: Positive samples: 414, Negative samples: 414\n",
      "Task 49: Positive samples: 86, Negative samples: 86\n",
      "Task 50: Positive samples: 391, Negative samples: 391\n",
      "Task 50: Positive samples: 109, Negative samples: 109\n",
      "Task 51: Positive samples: 401, Negative samples: 401\n",
      "Task 51: Positive samples: 99, Negative samples: 99\n",
      "Task 52: Positive samples: 408, Negative samples: 408\n",
      "Task 52: Positive samples: 92, Negative samples: 92\n",
      "Task 53: Positive samples: 377, Negative samples: 377\n",
      "Task 53: Positive samples: 123, Negative samples: 123\n",
      "Task 54: Positive samples: 390, Negative samples: 390\n",
      "Task 54: Positive samples: 110, Negative samples: 110\n",
      "Task 55: Positive samples: 409, Negative samples: 409\n",
      "Task 55: Positive samples: 91, Negative samples: 91\n",
      "Task 56: Positive samples: 398, Negative samples: 398\n",
      "Task 56: Positive samples: 102, Negative samples: 102\n",
      "Task 57: Positive samples: 380, Negative samples: 380\n",
      "Task 57: Positive samples: 120, Negative samples: 120\n",
      "Task 58: Positive samples: 400, Negative samples: 400\n",
      "Task 58: Positive samples: 100, Negative samples: 100\n",
      "Task 59: Positive samples: 407, Negative samples: 407\n",
      "Task 59: Positive samples: 93, Negative samples: 93\n",
      "Task 60: Positive samples: 390, Negative samples: 390\n",
      "Task 60: Positive samples: 110, Negative samples: 110\n",
      "Task 61: Positive samples: 397, Negative samples: 397\n",
      "Task 61: Positive samples: 103, Negative samples: 103\n",
      "Task 62: Positive samples: 411, Negative samples: 411\n",
      "Task 62: Positive samples: 89, Negative samples: 89\n",
      "Task 63: Positive samples: 395, Negative samples: 395\n",
      "Task 63: Positive samples: 105, Negative samples: 105\n",
      "Task 64: Positive samples: 416, Negative samples: 416\n",
      "Task 64: Positive samples: 84, Negative samples: 84\n",
      "Task 65: Positive samples: 401, Negative samples: 401\n",
      "Task 65: Positive samples: 99, Negative samples: 99\n",
      "Task 66: Positive samples: 400, Negative samples: 400\n",
      "Task 66: Positive samples: 100, Negative samples: 100\n",
      "Task 67: Positive samples: 396, Negative samples: 396\n",
      "Task 67: Positive samples: 104, Negative samples: 104\n",
      "Task 68: Positive samples: 406, Negative samples: 406\n",
      "Task 68: Positive samples: 94, Negative samples: 94\n",
      "Task 69: Positive samples: 398, Negative samples: 398\n",
      "Task 69: Positive samples: 102, Negative samples: 102\n",
      "Task 70: Positive samples: 381, Negative samples: 381\n",
      "Task 70: Positive samples: 119, Negative samples: 119\n",
      "Task 71: Positive samples: 392, Negative samples: 392\n",
      "Task 71: Positive samples: 108, Negative samples: 108\n",
      "Task 72: Positive samples: 415, Negative samples: 415\n",
      "Task 72: Positive samples: 85, Negative samples: 85\n",
      "Task 73: Positive samples: 398, Negative samples: 398\n",
      "Task 73: Positive samples: 102, Negative samples: 102\n",
      "Task 74: Positive samples: 399, Negative samples: 399\n",
      "Task 74: Positive samples: 101, Negative samples: 101\n",
      "Task 75: Positive samples: 408, Negative samples: 408\n",
      "Task 75: Positive samples: 92, Negative samples: 92\n",
      "Task 76: Positive samples: 379, Negative samples: 379\n",
      "Task 76: Positive samples: 121, Negative samples: 121\n",
      "Task 77: Positive samples: 408, Negative samples: 408\n",
      "Task 77: Positive samples: 92, Negative samples: 92\n",
      "Task 78: Positive samples: 397, Negative samples: 397\n",
      "Task 78: Positive samples: 103, Negative samples: 103\n",
      "Task 79: Positive samples: 412, Negative samples: 412\n",
      "Task 79: Positive samples: 88, Negative samples: 88\n",
      "Task 80: Positive samples: 396, Negative samples: 396\n",
      "Task 80: Positive samples: 104, Negative samples: 104\n",
      "Task 81: Positive samples: 404, Negative samples: 404\n",
      "Task 81: Positive samples: 96, Negative samples: 96\n",
      "Task 82: Positive samples: 409, Negative samples: 409\n",
      "Task 82: Positive samples: 91, Negative samples: 91\n",
      "Task 83: Positive samples: 394, Negative samples: 394\n",
      "Task 83: Positive samples: 106, Negative samples: 106\n",
      "Task 84: Positive samples: 408, Negative samples: 408\n",
      "Task 84: Positive samples: 92, Negative samples: 92\n",
      "Task 85: Positive samples: 409, Negative samples: 409\n",
      "Task 85: Positive samples: 91, Negative samples: 91\n",
      "Task 86: Positive samples: 400, Negative samples: 400\n",
      "Task 86: Positive samples: 100, Negative samples: 100\n",
      "Task 87: Positive samples: 395, Negative samples: 395\n",
      "Task 87: Positive samples: 105, Negative samples: 105\n",
      "Task 88: Positive samples: 410, Negative samples: 410\n",
      "Task 88: Positive samples: 90, Negative samples: 90\n",
      "Task 89: Positive samples: 393, Negative samples: 393\n",
      "Task 89: Positive samples: 107, Negative samples: 107\n",
      "Task 90: Positive samples: 409, Negative samples: 409\n",
      "Task 90: Positive samples: 91, Negative samples: 91\n",
      "Task 91: Positive samples: 405, Negative samples: 405\n",
      "Task 91: Positive samples: 95, Negative samples: 95\n",
      "Task 92: Positive samples: 393, Negative samples: 393\n",
      "Task 92: Positive samples: 107, Negative samples: 107\n",
      "Task 93: Positive samples: 387, Negative samples: 387\n",
      "Task 93: Positive samples: 113, Negative samples: 113\n",
      "Task 94: Positive samples: 395, Negative samples: 395\n",
      "Task 94: Positive samples: 105, Negative samples: 105\n",
      "Task 95: Positive samples: 397, Negative samples: 397\n",
      "Task 95: Positive samples: 103, Negative samples: 103\n",
      "Task 96: Positive samples: 385, Negative samples: 385\n",
      "Task 96: Positive samples: 115, Negative samples: 115\n",
      "Task 97: Positive samples: 406, Negative samples: 406\n",
      "Task 97: Positive samples: 94, Negative samples: 94\n",
      "Task 98: Positive samples: 385, Negative samples: 385\n",
      "Task 98: Positive samples: 115, Negative samples: 115\n",
      "Task 99: Positive samples: 389, Negative samples: 389\n",
      "Task 99: Positive samples: 111, Negative samples: 111\n"
     ]
    }
   ],
   "source": [
    "cifar100_tasks=load_cifar100_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "388460ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:16.722566Z",
     "iopub.status.busy": "2025-04-03T14:02:16.722247Z",
     "iopub.status.idle": "2025-04-03T14:03:03.625196Z",
     "shell.execute_reply": "2025-04-03T14:03:03.624094Z"
    },
    "papermill": {
     "duration": 46.928811,
     "end_time": "2025-04-03T14:03:03.626781",
     "exception": false,
     "start_time": "2025-04-03T14:02:16.697970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Positive samples: 96, Negative samples: 96\n",
      "Task 0: Positive samples: 24, Negative samples: 24\n",
      "Task 1: Positive samples: 103, Negative samples: 103\n",
      "Task 1: Positive samples: 26, Negative samples: 26\n",
      "Task 2: Positive samples: 137, Negative samples: 137\n",
      "Task 2: Positive samples: 34, Negative samples: 34\n",
      "Task 3: Positive samples: 145, Negative samples: 145\n",
      "Task 3: Positive samples: 36, Negative samples: 36\n",
      "Task 4: Positive samples: 110, Negative samples: 110\n",
      "Task 4: Positive samples: 28, Negative samples: 28\n",
      "Task 5: Positive samples: 127, Negative samples: 127\n",
      "Task 5: Positive samples: 32, Negative samples: 32\n",
      "Task 6: Positive samples: 122, Negative samples: 122\n",
      "Task 6: Positive samples: 30, Negative samples: 30\n",
      "Task 7: Positive samples: 108, Negative samples: 108\n",
      "Task 7: Positive samples: 27, Negative samples: 27\n",
      "Task 8: Positive samples: 120, Negative samples: 120\n",
      "Task 8: Positive samples: 30, Negative samples: 30\n",
      "Task 9: Positive samples: 110, Negative samples: 110\n",
      "Task 9: Positive samples: 28, Negative samples: 28\n",
      "Task 10: Positive samples: 109, Negative samples: 109\n",
      "Task 10: Positive samples: 27, Negative samples: 27\n",
      "Task 11: Positive samples: 126, Negative samples: 126\n",
      "Task 11: Positive samples: 32, Negative samples: 32\n",
      "Task 12: Positive samples: 126, Negative samples: 126\n",
      "Task 12: Positive samples: 31, Negative samples: 31\n",
      "Task 13: Positive samples: 91, Negative samples: 91\n",
      "Task 13: Positive samples: 23, Negative samples: 23\n",
      "Task 14: Positive samples: 123, Negative samples: 123\n",
      "Task 14: Positive samples: 31, Negative samples: 31\n",
      "Task 15: Positive samples: 112, Negative samples: 112\n",
      "Task 15: Positive samples: 28, Negative samples: 28\n",
      "Task 16: Positive samples: 137, Negative samples: 137\n",
      "Task 16: Positive samples: 34, Negative samples: 34\n",
      "Task 17: Positive samples: 128, Negative samples: 128\n",
      "Task 17: Positive samples: 32, Negative samples: 32\n",
      "Task 18: Positive samples: 134, Negative samples: 134\n",
      "Task 18: Positive samples: 34, Negative samples: 34\n",
      "Task 19: Positive samples: 138, Negative samples: 138\n",
      "Task 19: Positive samples: 34, Negative samples: 34\n",
      "Task 20: Positive samples: 92, Negative samples: 92\n",
      "Task 20: Positive samples: 23, Negative samples: 23\n",
      "Task 21: Positive samples: 103, Negative samples: 103\n",
      "Task 21: Positive samples: 25, Negative samples: 25\n",
      "Task 22: Positive samples: 108, Negative samples: 108\n",
      "Task 22: Positive samples: 27, Negative samples: 27\n",
      "Task 23: Positive samples: 100, Negative samples: 100\n",
      "Task 23: Positive samples: 25, Negative samples: 25\n",
      "Task 24: Positive samples: 121, Negative samples: 121\n",
      "Task 24: Positive samples: 30, Negative samples: 30\n",
      "Task 25: Positive samples: 99, Negative samples: 99\n",
      "Task 25: Positive samples: 25, Negative samples: 25\n",
      "Task 26: Positive samples: 103, Negative samples: 103\n",
      "Task 26: Positive samples: 26, Negative samples: 26\n",
      "Task 27: Positive samples: 106, Negative samples: 106\n",
      "Task 27: Positive samples: 27, Negative samples: 27\n",
      "Task 28: Positive samples: 123, Negative samples: 123\n",
      "Task 28: Positive samples: 31, Negative samples: 31\n",
      "Task 29: Positive samples: 129, Negative samples: 129\n",
      "Task 29: Positive samples: 32, Negative samples: 32\n",
      "Task 30: Positive samples: 130, Negative samples: 130\n",
      "Task 30: Positive samples: 33, Negative samples: 33\n",
      "Task 31: Positive samples: 111, Negative samples: 111\n",
      "Task 31: Positive samples: 28, Negative samples: 28\n",
      "Task 32: Positive samples: 130, Negative samples: 130\n",
      "Task 32: Positive samples: 32, Negative samples: 32\n",
      "Task 33: Positive samples: 87, Negative samples: 87\n",
      "Task 33: Positive samples: 21, Negative samples: 21\n",
      "Task 34: Positive samples: 99, Negative samples: 99\n",
      "Task 34: Positive samples: 25, Negative samples: 25\n",
      "Task 35: Positive samples: 114, Negative samples: 114\n",
      "Task 35: Positive samples: 29, Negative samples: 29\n",
      "Task 36: Positive samples: 132, Negative samples: 132\n",
      "Task 36: Positive samples: 33, Negative samples: 33\n",
      "Task 37: Positive samples: 122, Negative samples: 122\n",
      "Task 37: Positive samples: 30, Negative samples: 30\n",
      "Task 38: Positive samples: 111, Negative samples: 111\n",
      "Task 38: Positive samples: 28, Negative samples: 28\n",
      "Task 39: Positive samples: 102, Negative samples: 102\n",
      "Task 39: Positive samples: 25, Negative samples: 25\n",
      "Task 40: Positive samples: 126, Negative samples: 126\n",
      "Task 40: Positive samples: 32, Negative samples: 32\n",
      "Task 41: Positive samples: 113, Negative samples: 113\n",
      "Task 41: Positive samples: 28, Negative samples: 28\n",
      "Task 42: Positive samples: 121, Negative samples: 121\n",
      "Task 42: Positive samples: 30, Negative samples: 30\n",
      "Task 43: Positive samples: 103, Negative samples: 103\n",
      "Task 43: Positive samples: 26, Negative samples: 26\n",
      "Task 44: Positive samples: 126, Negative samples: 126\n",
      "Task 44: Positive samples: 32, Negative samples: 32\n",
      "Task 45: Positive samples: 101, Negative samples: 101\n",
      "Task 45: Positive samples: 25, Negative samples: 25\n",
      "Task 46: Positive samples: 122, Negative samples: 122\n",
      "Task 46: Positive samples: 31, Negative samples: 31\n",
      "Task 47: Positive samples: 91, Negative samples: 91\n",
      "Task 47: Positive samples: 22, Negative samples: 22\n",
      "Task 48: Positive samples: 114, Negative samples: 114\n",
      "Task 48: Positive samples: 28, Negative samples: 28\n",
      "Task 49: Positive samples: 104, Negative samples: 104\n",
      "Task 49: Positive samples: 26, Negative samples: 26\n",
      "Task 50: Positive samples: 106, Negative samples: 106\n",
      "Task 50: Positive samples: 27, Negative samples: 27\n",
      "Task 51: Positive samples: 124, Negative samples: 124\n",
      "Task 51: Positive samples: 31, Negative samples: 31\n",
      "Task 52: Positive samples: 124, Negative samples: 124\n",
      "Task 52: Positive samples: 31, Negative samples: 31\n"
     ]
    }
   ],
   "source": [
    "mammal_tasks=load_animals_binary('/kaggle/input/cards-image-datasetclassification/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aeb928a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:03:03.676545Z",
     "iopub.status.busy": "2025-04-03T14:03:03.676194Z",
     "iopub.status.idle": "2025-04-03T14:03:07.932105Z",
     "shell.execute_reply": "2025-04-03T14:03:07.930905Z"
    },
    "papermill": {
     "duration": 4.282659,
     "end_time": "2025-04-03T14:03:07.933750",
     "exception": false,
     "start_time": "2025-04-03T14:03:03.651091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Positive samples: 96, Negative samples: 96\n",
      "Task 0: Positive samples: 24, Negative samples: 24\n",
      "Task 1: Positive samples: 103, Negative samples: 103\n",
      "Task 1: Positive samples: 26, Negative samples: 26\n",
      "Task 2: Positive samples: 137, Negative samples: 137\n",
      "Task 2: Positive samples: 34, Negative samples: 34\n",
      "Task 3: Positive samples: 145, Negative samples: 145\n",
      "Task 3: Positive samples: 36, Negative samples: 36\n",
      "Task 4: Positive samples: 110, Negative samples: 110\n",
      "Task 4: Positive samples: 28, Negative samples: 28\n",
      "Task 5: Positive samples: 127, Negative samples: 127\n",
      "Task 5: Positive samples: 32, Negative samples: 32\n",
      "Task 6: Positive samples: 122, Negative samples: 122\n",
      "Task 6: Positive samples: 30, Negative samples: 30\n",
      "Task 7: Positive samples: 108, Negative samples: 108\n",
      "Task 7: Positive samples: 27, Negative samples: 27\n",
      "Task 8: Positive samples: 120, Negative samples: 120\n",
      "Task 8: Positive samples: 30, Negative samples: 30\n",
      "Task 9: Positive samples: 110, Negative samples: 110\n",
      "Task 9: Positive samples: 28, Negative samples: 28\n",
      "Task 10: Positive samples: 109, Negative samples: 109\n",
      "Task 10: Positive samples: 27, Negative samples: 27\n",
      "Task 11: Positive samples: 126, Negative samples: 126\n",
      "Task 11: Positive samples: 32, Negative samples: 32\n",
      "Task 12: Positive samples: 126, Negative samples: 126\n",
      "Task 12: Positive samples: 31, Negative samples: 31\n",
      "Task 13: Positive samples: 91, Negative samples: 91\n",
      "Task 13: Positive samples: 23, Negative samples: 23\n",
      "Task 14: Positive samples: 123, Negative samples: 123\n",
      "Task 14: Positive samples: 31, Negative samples: 31\n",
      "Task 15: Positive samples: 112, Negative samples: 112\n",
      "Task 15: Positive samples: 28, Negative samples: 28\n",
      "Task 16: Positive samples: 137, Negative samples: 137\n",
      "Task 16: Positive samples: 34, Negative samples: 34\n",
      "Task 17: Positive samples: 128, Negative samples: 128\n",
      "Task 17: Positive samples: 32, Negative samples: 32\n",
      "Task 18: Positive samples: 134, Negative samples: 134\n",
      "Task 18: Positive samples: 34, Negative samples: 34\n",
      "Task 19: Positive samples: 138, Negative samples: 138\n",
      "Task 19: Positive samples: 34, Negative samples: 34\n",
      "Task 20: Positive samples: 92, Negative samples: 92\n",
      "Task 20: Positive samples: 23, Negative samples: 23\n",
      "Task 21: Positive samples: 103, Negative samples: 103\n",
      "Task 21: Positive samples: 25, Negative samples: 25\n",
      "Task 22: Positive samples: 108, Negative samples: 108\n",
      "Task 22: Positive samples: 27, Negative samples: 27\n",
      "Task 23: Positive samples: 100, Negative samples: 100\n",
      "Task 23: Positive samples: 25, Negative samples: 25\n",
      "Task 24: Positive samples: 121, Negative samples: 121\n",
      "Task 24: Positive samples: 30, Negative samples: 30\n",
      "Task 25: Positive samples: 99, Negative samples: 99\n",
      "Task 25: Positive samples: 25, Negative samples: 25\n",
      "Task 26: Positive samples: 103, Negative samples: 103\n",
      "Task 26: Positive samples: 26, Negative samples: 26\n",
      "Task 27: Positive samples: 106, Negative samples: 106\n",
      "Task 27: Positive samples: 27, Negative samples: 27\n",
      "Task 28: Positive samples: 123, Negative samples: 123\n",
      "Task 28: Positive samples: 31, Negative samples: 31\n",
      "Task 29: Positive samples: 129, Negative samples: 129\n",
      "Task 29: Positive samples: 32, Negative samples: 32\n",
      "Task 30: Positive samples: 130, Negative samples: 130\n",
      "Task 30: Positive samples: 33, Negative samples: 33\n",
      "Task 31: Positive samples: 111, Negative samples: 111\n",
      "Task 31: Positive samples: 28, Negative samples: 28\n",
      "Task 32: Positive samples: 130, Negative samples: 130\n",
      "Task 32: Positive samples: 32, Negative samples: 32\n",
      "Task 33: Positive samples: 87, Negative samples: 87\n",
      "Task 33: Positive samples: 21, Negative samples: 21\n",
      "Task 34: Positive samples: 99, Negative samples: 99\n",
      "Task 34: Positive samples: 25, Negative samples: 25\n",
      "Task 35: Positive samples: 114, Negative samples: 114\n",
      "Task 35: Positive samples: 29, Negative samples: 29\n",
      "Task 36: Positive samples: 132, Negative samples: 132\n",
      "Task 36: Positive samples: 33, Negative samples: 33\n",
      "Task 37: Positive samples: 122, Negative samples: 122\n",
      "Task 37: Positive samples: 30, Negative samples: 30\n",
      "Task 38: Positive samples: 111, Negative samples: 111\n",
      "Task 38: Positive samples: 28, Negative samples: 28\n",
      "Task 39: Positive samples: 102, Negative samples: 102\n",
      "Task 39: Positive samples: 25, Negative samples: 25\n",
      "Task 40: Positive samples: 126, Negative samples: 126\n",
      "Task 40: Positive samples: 32, Negative samples: 32\n",
      "Task 41: Positive samples: 113, Negative samples: 113\n",
      "Task 41: Positive samples: 28, Negative samples: 28\n",
      "Task 42: Positive samples: 121, Negative samples: 121\n",
      "Task 42: Positive samples: 30, Negative samples: 30\n",
      "Task 43: Positive samples: 103, Negative samples: 103\n",
      "Task 43: Positive samples: 26, Negative samples: 26\n",
      "Task 44: Positive samples: 126, Negative samples: 126\n",
      "Task 44: Positive samples: 32, Negative samples: 32\n",
      "Task 45: Positive samples: 101, Negative samples: 101\n",
      "Task 45: Positive samples: 25, Negative samples: 25\n",
      "Task 46: Positive samples: 122, Negative samples: 122\n",
      "Task 46: Positive samples: 31, Negative samples: 31\n",
      "Task 47: Positive samples: 91, Negative samples: 91\n",
      "Task 47: Positive samples: 22, Negative samples: 22\n",
      "Task 48: Positive samples: 114, Negative samples: 114\n",
      "Task 48: Positive samples: 28, Negative samples: 28\n",
      "Task 49: Positive samples: 104, Negative samples: 104\n",
      "Task 49: Positive samples: 26, Negative samples: 26\n",
      "Task 50: Positive samples: 106, Negative samples: 106\n",
      "Task 50: Positive samples: 27, Negative samples: 27\n",
      "Task 51: Positive samples: 124, Negative samples: 124\n",
      "Task 51: Positive samples: 31, Negative samples: 31\n",
      "Task 52: Positive samples: 124, Negative samples: 124\n",
      "Task 52: Positive samples: 31, Negative samples: 31\n"
     ]
    }
   ],
   "source": [
    "cards_tasks=load_animals_binary('/kaggle/input/cards-image-datasetclassification/train',valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ac2be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:03:08.032158Z",
     "iopub.status.busy": "2025-04-03T14:03:08.031804Z",
     "iopub.status.idle": "2025-04-03T14:03:08.048353Z",
     "shell.execute_reply": "2025-04-03T14:03:08.047565Z"
    },
    "papermill": {
     "duration": 0.04349,
     "end_time": "2025-04-03T14:03:08.049843",
     "exception": false,
     "start_time": "2025-04-03T14:03:08.006353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu';\n",
    "enc_sizes=[32,64,64]\n",
    "conflict_model=CNN(3,enc_sizes,num_classes=2)\n",
    "models=[]\n",
    "flags=['layer_wise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beacff8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:03:08.099503Z",
     "iopub.status.busy": "2025-04-03T14:03:08.099183Z",
     "iopub.status.idle": "2025-04-03T14:03:08.103768Z",
     "shell.execute_reply": "2025-04-03T14:03:08.102866Z"
    },
    "papermill": {
     "duration": 0.031091,
     "end_time": "2025-04-03T14:03:08.105283",
     "exception": false,
     "start_time": "2025-04-03T14:03:08.074192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "task_dict={\n",
    "    # 'cards': cards_tasks,\n",
    "    # 'mammals': mammal_tasks\n",
    "    'cifar100': cifar100_tasks,\n",
    "    # 'imagenet':tiny_imagenet_tasks\n",
    "    # 'flowers': flower_tasks\n",
    "}\n",
    "num_classes_dict={\n",
    "    'cards':53,\n",
    "    'mammals': 45,\n",
    "    'cifar100': 100,\n",
    "    'imagenet':200,\n",
    "    'flowers':102\n",
    "\n",
    "}\n",
    "metrics={\n",
    "    \"ci\": calc_sign_conflicts,\n",
    "    # 'dis': calc_distance, \n",
    "    # 'sim': calc_similarity,\n",
    "    # 'disci': calc_simdis,\n",
    "    'diseuc': calc_distance_euc\n",
    "}\n",
    "types=['full','layer_wise']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeabc861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:03:08.155728Z",
     "iopub.status.busy": "2025-04-03T14:03:08.155359Z",
     "iopub.status.idle": "2025-04-03T15:10:37.401334Z",
     "shell.execute_reply": "2025-04-03T15:10:37.400420Z"
    },
    "papermill": {
     "duration": 4049.273576,
     "end_time": "2025-04-03T15:10:37.403323",
     "exception": false,
     "start_time": "2025-04-03T14:03:08.129747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5814\n",
      "Val Accuracy: 0.7863\n",
      "Precision: 0.8287, Recall: 0.7863, F1: 0.7792\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4180\n",
      "Val Accuracy: 0.8205\n",
      "Precision: 0.8229, Recall: 0.8205, F1: 0.8202\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3450\n",
      "Val Accuracy: 0.8376\n",
      "Precision: 0.8687, Recall: 0.8376, F1: 0.8341\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3142\n",
      "Val Accuracy: 0.8504\n",
      "Precision: 0.8536, Recall: 0.8504, F1: 0.8501\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2817\n",
      "Val Accuracy: 0.8547\n",
      "Precision: 0.8615, Recall: 0.8547, F1: 0.8540\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5883\n",
      "Val Accuracy: 0.7500\n",
      "Precision: 0.7510, Recall: 0.7500, F1: 0.7497\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5189\n",
      "Val Accuracy: 0.7591\n",
      "Precision: 0.7628, Recall: 0.7591, F1: 0.7582\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4894\n",
      "Val Accuracy: 0.7773\n",
      "Precision: 0.7778, Recall: 0.7773, F1: 0.7772\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4067\n",
      "Val Accuracy: 0.8045\n",
      "Precision: 0.8058, Recall: 0.8045, F1: 0.8043\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4101\n",
      "Val Accuracy: 0.7864\n",
      "Precision: 0.8047, Recall: 0.7864, F1: 0.7831\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6764\n",
      "Val Accuracy: 0.7064\n",
      "Precision: 0.7233, Recall: 0.7064, F1: 0.7008\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5977\n",
      "Val Accuracy: 0.7615\n",
      "Precision: 0.7647, Recall: 0.7615, F1: 0.7607\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5676\n",
      "Val Accuracy: 0.7385\n",
      "Precision: 0.7660, Recall: 0.7385, F1: 0.7316\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5874\n",
      "Val Accuracy: 0.7569\n",
      "Precision: 0.7689, Recall: 0.7569, F1: 0.7541\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5277\n",
      "Val Accuracy: 0.7339\n",
      "Precision: 0.7359, Recall: 0.7339, F1: 0.7334\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6655\n",
      "Val Accuracy: 0.6505\n",
      "Precision: 0.6557, Recall: 0.6505, F1: 0.6476\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5953\n",
      "Val Accuracy: 0.6720\n",
      "Precision: 0.6722, Recall: 0.6720, F1: 0.6720\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5548\n",
      "Val Accuracy: 0.6667\n",
      "Precision: 0.6686, Recall: 0.6667, F1: 0.6657\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5129\n",
      "Val Accuracy: 0.7151\n",
      "Precision: 0.7157, Recall: 0.7151, F1: 0.7148\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4859\n",
      "Val Accuracy: 0.6989\n",
      "Precision: 0.7131, Recall: 0.6989, F1: 0.6938\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6628\n",
      "Val Accuracy: 0.7135\n",
      "Precision: 0.7154, Recall: 0.7135, F1: 0.7129\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5604\n",
      "Val Accuracy: 0.7240\n",
      "Precision: 0.7802, Recall: 0.7240, F1: 0.7094\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5896\n",
      "Val Accuracy: 0.7865\n",
      "Precision: 0.8073, Recall: 0.7865, F1: 0.7828\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5392\n",
      "Val Accuracy: 0.7448\n",
      "Precision: 0.7932, Recall: 0.7448, F1: 0.7338\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5108\n",
      "Val Accuracy: 0.7865\n",
      "Precision: 0.8152, Recall: 0.7865, F1: 0.7815\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6650\n",
      "Val Accuracy: 0.6786\n",
      "Precision: 0.7122, Recall: 0.6786, F1: 0.6653\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6222\n",
      "Val Accuracy: 0.7398\n",
      "Precision: 0.7398, Recall: 0.7398, F1: 0.7398\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5557\n",
      "Val Accuracy: 0.7755\n",
      "Precision: 0.7765, Recall: 0.7755, F1: 0.7753\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5030\n",
      "Val Accuracy: 0.7806\n",
      "Precision: 0.7916, Recall: 0.7806, F1: 0.7785\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4729\n",
      "Val Accuracy: 0.7857\n",
      "Precision: 0.8073, Recall: 0.7857, F1: 0.7819\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6272\n",
      "Val Accuracy: 0.7091\n",
      "Precision: 0.7410, Recall: 0.7091, F1: 0.6991\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5793\n",
      "Val Accuracy: 0.7318\n",
      "Precision: 0.7579, Recall: 0.7318, F1: 0.7249\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5044\n",
      "Val Accuracy: 0.7545\n",
      "Precision: 0.7615, Recall: 0.7545, F1: 0.7529\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4940\n",
      "Val Accuracy: 0.8045\n",
      "Precision: 0.8058, Recall: 0.8045, F1: 0.8043\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4777\n",
      "Val Accuracy: 0.7909\n",
      "Precision: 0.7909, Recall: 0.7909, F1: 0.7909\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6536\n",
      "Val Accuracy: 0.7308\n",
      "Precision: 0.7530, Recall: 0.7308, F1: 0.7247\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5115\n",
      "Val Accuracy: 0.7692\n",
      "Precision: 0.7758, Recall: 0.7692, F1: 0.7679\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4410\n",
      "Val Accuracy: 0.7802\n",
      "Precision: 0.7804, Recall: 0.7802, F1: 0.7802\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4498\n",
      "Val Accuracy: 0.7418\n",
      "Precision: 0.7691, Recall: 0.7418, F1: 0.7350\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4300\n",
      "Val Accuracy: 0.7802\n",
      "Precision: 0.7808, Recall: 0.7802, F1: 0.7801\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6802\n",
      "Val Accuracy: 0.6750\n",
      "Precision: 0.6778, Recall: 0.6750, F1: 0.6737\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6366\n",
      "Val Accuracy: 0.7937\n",
      "Precision: 0.7949, Recall: 0.7937, F1: 0.7935\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5395\n",
      "Val Accuracy: 0.7250\n",
      "Precision: 0.7821, Recall: 0.7250, F1: 0.7103\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5088\n",
      "Val Accuracy: 0.8000\n",
      "Precision: 0.8200, Recall: 0.8000, F1: 0.7968\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4609\n",
      "Val Accuracy: 0.7937\n",
      "Precision: 0.8315, Recall: 0.7937, F1: 0.7877\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6673\n",
      "Val Accuracy: 0.6456\n",
      "Precision: 0.7099, Recall: 0.6456, F1: 0.6163\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6172\n",
      "Val Accuracy: 0.6602\n",
      "Precision: 0.6921, Recall: 0.6602, F1: 0.6455\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5584\n",
      "Val Accuracy: 0.7767\n",
      "Precision: 0.7899, Recall: 0.7767, F1: 0.7741\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5169\n",
      "Val Accuracy: 0.7864\n",
      "Precision: 0.8001, Recall: 0.7864, F1: 0.7839\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4721\n",
      "Val Accuracy: 0.7961\n",
      "Precision: 0.8163, Recall: 0.7961, F1: 0.7928\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6880\n",
      "Val Accuracy: 0.5680\n",
      "Precision: 0.5709, Recall: 0.5680, F1: 0.5634\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6524\n",
      "Val Accuracy: 0.6311\n",
      "Precision: 0.6311, Recall: 0.6311, F1: 0.6311\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6043\n",
      "Val Accuracy: 0.7282\n",
      "Precision: 0.7493, Recall: 0.7282, F1: 0.7223\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5279\n",
      "Val Accuracy: 0.7767\n",
      "Precision: 0.7793, Recall: 0.7767, F1: 0.7762\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4710\n",
      "Val Accuracy: 0.8107\n",
      "Precision: 0.8157, Recall: 0.8107, F1: 0.8099\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6798\n",
      "Val Accuracy: 0.5765\n",
      "Precision: 0.5893, Recall: 0.5765, F1: 0.5609\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6208\n",
      "Val Accuracy: 0.7041\n",
      "Precision: 0.7130, Recall: 0.7041, F1: 0.7010\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5438\n",
      "Val Accuracy: 0.7092\n",
      "Precision: 0.7119, Recall: 0.7092, F1: 0.7083\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4992\n",
      "Val Accuracy: 0.6633\n",
      "Precision: 0.6756, Recall: 0.6633, F1: 0.6572\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4567\n",
      "Val Accuracy: 0.7449\n",
      "Precision: 0.7579, Recall: 0.7449, F1: 0.7416\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6193\n",
      "Val Accuracy: 0.7204\n",
      "Precision: 0.7230, Recall: 0.7204, F1: 0.7196\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5317\n",
      "Val Accuracy: 0.6989\n",
      "Precision: 0.6998, Recall: 0.6989, F1: 0.6986\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5071\n",
      "Val Accuracy: 0.7581\n",
      "Precision: 0.7719, Recall: 0.7581, F1: 0.7549\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4743\n",
      "Val Accuracy: 0.7742\n",
      "Precision: 0.7762, Recall: 0.7742, F1: 0.7738\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4464\n",
      "Val Accuracy: 0.7688\n",
      "Precision: 0.7703, Recall: 0.7688, F1: 0.7685\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6774\n",
      "Val Accuracy: 0.6263\n",
      "Precision: 0.7169, Recall: 0.6263, F1: 0.5827\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5480\n",
      "Val Accuracy: 0.7374\n",
      "Precision: 0.7398, Recall: 0.7374, F1: 0.7367\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4224\n",
      "Val Accuracy: 0.6869\n",
      "Precision: 0.7233, Recall: 0.6869, F1: 0.6735\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4580\n",
      "Val Accuracy: 0.7828\n",
      "Precision: 0.7878, Recall: 0.7828, F1: 0.7819\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3671\n",
      "Val Accuracy: 0.7273\n",
      "Precision: 0.7619, Recall: 0.7273, F1: 0.7179\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6612\n",
      "Val Accuracy: 0.6398\n",
      "Precision: 0.6612, Recall: 0.6398, F1: 0.6275\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6042\n",
      "Val Accuracy: 0.6737\n",
      "Precision: 0.6833, Recall: 0.6737, F1: 0.6694\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5292\n",
      "Val Accuracy: 0.7246\n",
      "Precision: 0.7293, Recall: 0.7246, F1: 0.7231\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4601\n",
      "Val Accuracy: 0.7542\n",
      "Precision: 0.7872, Recall: 0.7542, F1: 0.7470\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4432\n",
      "Val Accuracy: 0.7754\n",
      "Precision: 0.7958, Recall: 0.7754, F1: 0.7715\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6907\n",
      "Val Accuracy: 0.6050\n",
      "Precision: 0.6051, Recall: 0.6050, F1: 0.6049\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6443\n",
      "Val Accuracy: 0.5750\n",
      "Precision: 0.6043, Recall: 0.5750, F1: 0.5429\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6205\n",
      "Val Accuracy: 0.6550\n",
      "Precision: 0.6766, Recall: 0.6550, F1: 0.6441\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5775\n",
      "Val Accuracy: 0.7100\n",
      "Precision: 0.7131, Recall: 0.7100, F1: 0.7090\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5279\n",
      "Val Accuracy: 0.6750\n",
      "Precision: 0.7104, Recall: 0.6750, F1: 0.6607\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6885\n",
      "Val Accuracy: 0.6436\n",
      "Precision: 0.6436, Recall: 0.6436, F1: 0.6436\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6575\n",
      "Val Accuracy: 0.6649\n",
      "Precision: 0.6654, Recall: 0.6649, F1: 0.6647\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6282\n",
      "Val Accuracy: 0.6809\n",
      "Precision: 0.6809, Recall: 0.6809, F1: 0.6808\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5751\n",
      "Val Accuracy: 0.7074\n",
      "Precision: 0.7086, Recall: 0.7074, F1: 0.7070\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4918\n",
      "Val Accuracy: 0.6968\n",
      "Precision: 0.6979, Recall: 0.6968, F1: 0.6964\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5549\n",
      "Val Accuracy: 0.8118\n",
      "Precision: 0.8180, Recall: 0.8118, F1: 0.8109\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3446\n",
      "Val Accuracy: 0.8763\n",
      "Precision: 0.8838, Recall: 0.8763, F1: 0.8757\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3119\n",
      "Val Accuracy: 0.8280\n",
      "Precision: 0.8558, Recall: 0.8280, F1: 0.8245\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3179\n",
      "Val Accuracy: 0.8925\n",
      "Precision: 0.8927, Recall: 0.8925, F1: 0.8925\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2853\n",
      "Val Accuracy: 0.8763\n",
      "Precision: 0.8799, Recall: 0.8763, F1: 0.8761\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6456\n",
      "Val Accuracy: 0.7423\n",
      "Precision: 0.7439, Recall: 0.7423, F1: 0.7418\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5524\n",
      "Val Accuracy: 0.7577\n",
      "Precision: 0.7624, Recall: 0.7577, F1: 0.7566\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5269\n",
      "Val Accuracy: 0.7732\n",
      "Precision: 0.7774, Recall: 0.7732, F1: 0.7723\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4810\n",
      "Val Accuracy: 0.7938\n",
      "Precision: 0.8020, Recall: 0.7938, F1: 0.7924\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4691\n",
      "Val Accuracy: 0.7990\n",
      "Precision: 0.8063, Recall: 0.7990, F1: 0.7978\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6853\n",
      "Val Accuracy: 0.6211\n",
      "Precision: 0.6855, Recall: 0.6211, F1: 0.5850\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6322\n",
      "Val Accuracy: 0.6789\n",
      "Precision: 0.6973, Recall: 0.6789, F1: 0.6713\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5814\n",
      "Val Accuracy: 0.7053\n",
      "Precision: 0.7056, Recall: 0.7053, F1: 0.7051\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5316\n",
      "Val Accuracy: 0.7421\n",
      "Precision: 0.7545, Recall: 0.7421, F1: 0.7389\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4702\n",
      "Val Accuracy: 0.7526\n",
      "Precision: 0.7540, Recall: 0.7526, F1: 0.7523\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5325\n",
      "Val Accuracy: 0.8102\n",
      "Precision: 0.8224, Recall: 0.8102, F1: 0.8084\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4889\n",
      "Val Accuracy: 0.8380\n",
      "Precision: 0.8403, Recall: 0.8380, F1: 0.8377\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4089\n",
      "Val Accuracy: 0.8194\n",
      "Precision: 0.8228, Recall: 0.8194, F1: 0.8190\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4050\n",
      "Val Accuracy: 0.8519\n",
      "Precision: 0.8523, Recall: 0.8519, F1: 0.8518\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3804\n",
      "Val Accuracy: 0.8333\n",
      "Precision: 0.8478, Recall: 0.8333, F1: 0.8316\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5874\n",
      "Val Accuracy: 0.7228\n",
      "Precision: 0.7285, Recall: 0.7228, F1: 0.7210\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4483\n",
      "Val Accuracy: 0.7376\n",
      "Precision: 0.7660, Recall: 0.7376, F1: 0.7304\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3937\n",
      "Val Accuracy: 0.7673\n",
      "Precision: 0.7993, Recall: 0.7673, F1: 0.7609\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3242\n",
      "Val Accuracy: 0.7822\n",
      "Precision: 0.7937, Recall: 0.7822, F1: 0.7800\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2909\n",
      "Val Accuracy: 0.8168\n",
      "Precision: 0.8222, Recall: 0.8168, F1: 0.8161\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6762\n",
      "Val Accuracy: 0.6416\n",
      "Precision: 0.6899, Recall: 0.6416, F1: 0.6172\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6314\n",
      "Val Accuracy: 0.6593\n",
      "Precision: 0.7190, Recall: 0.6593, F1: 0.6344\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5602\n",
      "Val Accuracy: 0.7478\n",
      "Precision: 0.7483, Recall: 0.7478, F1: 0.7477\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4972\n",
      "Val Accuracy: 0.7522\n",
      "Precision: 0.7663, Recall: 0.7522, F1: 0.7489\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5078\n",
      "Val Accuracy: 0.8097\n",
      "Precision: 0.8109, Recall: 0.8097, F1: 0.8096\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4961\n",
      "Val Accuracy: 0.7938\n",
      "Precision: 0.8471, Recall: 0.7938, F1: 0.7856\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3315\n",
      "Val Accuracy: 0.8041\n",
      "Precision: 0.8467, Recall: 0.8041, F1: 0.7979\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2881\n",
      "Val Accuracy: 0.8557\n",
      "Precision: 0.8595, Recall: 0.8557, F1: 0.8553\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2415\n",
      "Val Accuracy: 0.8557\n",
      "Precision: 0.8557, Recall: 0.8557, F1: 0.8557\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2199\n",
      "Val Accuracy: 0.8608\n",
      "Precision: 0.8823, Recall: 0.8608, F1: 0.8588\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5682\n",
      "Val Accuracy: 0.8114\n",
      "Precision: 0.8120, Recall: 0.8114, F1: 0.8113\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4369\n",
      "Val Accuracy: 0.8377\n",
      "Precision: 0.8520, Recall: 0.8377, F1: 0.8361\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3733\n",
      "Val Accuracy: 0.8509\n",
      "Precision: 0.8645, Recall: 0.8509, F1: 0.8495\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3517\n",
      "Val Accuracy: 0.8465\n",
      "Precision: 0.8587, Recall: 0.8465, F1: 0.8452\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3522\n",
      "Val Accuracy: 0.8947\n",
      "Precision: 0.8958, Recall: 0.8947, F1: 0.8947\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6860\n",
      "Val Accuracy: 0.5529\n",
      "Precision: 0.5945, Recall: 0.5529, F1: 0.4976\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6298\n",
      "Val Accuracy: 0.6202\n",
      "Precision: 0.6303, Recall: 0.6202, F1: 0.6127\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5804\n",
      "Val Accuracy: 0.6923\n",
      "Precision: 0.6935, Recall: 0.6923, F1: 0.6919\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5125\n",
      "Val Accuracy: 0.7115\n",
      "Precision: 0.7154, Recall: 0.7115, F1: 0.7102\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4816\n",
      "Val Accuracy: 0.7356\n",
      "Precision: 0.7500, Recall: 0.7356, F1: 0.7317\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6824\n",
      "Val Accuracy: 0.6762\n",
      "Precision: 0.6942, Recall: 0.6762, F1: 0.6685\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6304\n",
      "Val Accuracy: 0.6857\n",
      "Precision: 0.6863, Recall: 0.6857, F1: 0.6855\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6004\n",
      "Val Accuracy: 0.6667\n",
      "Precision: 0.6889, Recall: 0.6667, F1: 0.6566\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5713\n",
      "Val Accuracy: 0.7000\n",
      "Precision: 0.7005, Recall: 0.7000, F1: 0.6998\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5274\n",
      "Val Accuracy: 0.7429\n",
      "Precision: 0.7473, Recall: 0.7429, F1: 0.7417\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6837\n",
      "Val Accuracy: 0.7448\n",
      "Precision: 0.7461, Recall: 0.7448, F1: 0.7445\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6412\n",
      "Val Accuracy: 0.7188\n",
      "Precision: 0.7287, Recall: 0.7188, F1: 0.7157\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6181\n",
      "Val Accuracy: 0.6771\n",
      "Precision: 0.7361, Recall: 0.6771, F1: 0.6556\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5844\n",
      "Val Accuracy: 0.7552\n",
      "Precision: 0.7575, Recall: 0.7552, F1: 0.7547\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5277\n",
      "Val Accuracy: 0.7292\n",
      "Precision: 0.7620, Recall: 0.7292, F1: 0.7204\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6699\n",
      "Val Accuracy: 0.6893\n",
      "Precision: 0.6911, Recall: 0.6893, F1: 0.6886\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5603\n",
      "Val Accuracy: 0.7476\n",
      "Precision: 0.7778, Recall: 0.7476, F1: 0.7405\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5089\n",
      "Val Accuracy: 0.8107\n",
      "Precision: 0.8416, Recall: 0.8107, F1: 0.8063\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4410\n",
      "Val Accuracy: 0.7767\n",
      "Precision: 0.7955, Recall: 0.7767, F1: 0.7731\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4095\n",
      "Val Accuracy: 0.8495\n",
      "Precision: 0.8593, Recall: 0.8495, F1: 0.8485\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6738\n",
      "Val Accuracy: 0.5691\n",
      "Precision: 0.5710, Recall: 0.5691, F1: 0.5664\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6395\n",
      "Val Accuracy: 0.6117\n",
      "Precision: 0.6583, Recall: 0.6117, F1: 0.5809\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6191\n",
      "Val Accuracy: 0.6436\n",
      "Precision: 0.6972, Recall: 0.6436, F1: 0.6176\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5790\n",
      "Val Accuracy: 0.6277\n",
      "Precision: 0.6559, Recall: 0.6277, F1: 0.6100\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5423\n",
      "Val Accuracy: 0.7128\n",
      "Precision: 0.7448, Recall: 0.7128, F1: 0.7031\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4738\n",
      "Val Accuracy: 0.8483\n",
      "Precision: 0.8615, Recall: 0.8483, F1: 0.8469\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3479\n",
      "Val Accuracy: 0.9213\n",
      "Precision: 0.9216, Recall: 0.9213, F1: 0.9213\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3105\n",
      "Val Accuracy: 0.9213\n",
      "Precision: 0.9222, Recall: 0.9213, F1: 0.9213\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2994\n",
      "Val Accuracy: 0.9157\n",
      "Precision: 0.9170, Recall: 0.9157, F1: 0.9157\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2868\n",
      "Val Accuracy: 0.9270\n",
      "Precision: 0.9275, Recall: 0.9270, F1: 0.9269\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6643\n",
      "Val Accuracy: 0.6932\n",
      "Precision: 0.6936, Recall: 0.6932, F1: 0.6930\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5631\n",
      "Val Accuracy: 0.6591\n",
      "Precision: 0.7060, Recall: 0.6591, F1: 0.6385\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5001\n",
      "Val Accuracy: 0.7898\n",
      "Precision: 0.8199, Recall: 0.7898, F1: 0.7847\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4245\n",
      "Val Accuracy: 0.8068\n",
      "Precision: 0.8148, Recall: 0.8068, F1: 0.8056\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4014\n",
      "Val Accuracy: 0.8011\n",
      "Precision: 0.8021, Recall: 0.8011, F1: 0.8010\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6924\n",
      "Val Accuracy: 0.5652\n",
      "Precision: 0.5926, Recall: 0.5652, F1: 0.5306\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6741\n",
      "Val Accuracy: 0.6141\n",
      "Precision: 0.6141, Recall: 0.6141, F1: 0.6141\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6494\n",
      "Val Accuracy: 0.6467\n",
      "Precision: 0.6789, Recall: 0.6467, F1: 0.6301\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6118\n",
      "Val Accuracy: 0.7065\n",
      "Precision: 0.7101, Recall: 0.7065, F1: 0.7053\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5735\n",
      "Val Accuracy: 0.6685\n",
      "Precision: 0.7010, Recall: 0.6685, F1: 0.6545\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6047\n",
      "Val Accuracy: 0.6840\n",
      "Precision: 0.7163, Recall: 0.6840, F1: 0.6717\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4951\n",
      "Val Accuracy: 0.7594\n",
      "Precision: 0.7595, Recall: 0.7594, F1: 0.7594\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4778\n",
      "Val Accuracy: 0.7736\n",
      "Precision: 0.7837, Recall: 0.7736, F1: 0.7716\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4670\n",
      "Val Accuracy: 0.7736\n",
      "Precision: 0.7752, Recall: 0.7736, F1: 0.7733\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4352\n",
      "Val Accuracy: 0.7358\n",
      "Precision: 0.7564, Recall: 0.7358, F1: 0.7305\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6978\n",
      "Val Accuracy: 0.6436\n",
      "Precision: 0.6736, Recall: 0.6436, F1: 0.6275\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6416\n",
      "Val Accuracy: 0.5594\n",
      "Precision: 0.6665, Recall: 0.5594, F1: 0.4750\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6303\n",
      "Val Accuracy: 0.6436\n",
      "Precision: 0.6811, Recall: 0.6436, F1: 0.6241\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6335\n",
      "Val Accuracy: 0.6980\n",
      "Precision: 0.7038, Recall: 0.6980, F1: 0.6959\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5881\n",
      "Val Accuracy: 0.7129\n",
      "Precision: 0.7136, Recall: 0.7129, F1: 0.7126\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6561\n",
      "Val Accuracy: 0.5684\n",
      "Precision: 0.6253, Recall: 0.5684, F1: 0.5132\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6162\n",
      "Val Accuracy: 0.6684\n",
      "Precision: 0.6740, Recall: 0.6684, F1: 0.6657\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5464\n",
      "Val Accuracy: 0.7368\n",
      "Precision: 0.7378, Recall: 0.7368, F1: 0.7366\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5178\n",
      "Val Accuracy: 0.7158\n",
      "Precision: 0.7258, Recall: 0.7158, F1: 0.7126\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4669\n",
      "Val Accuracy: 0.7421\n",
      "Precision: 0.7501, Recall: 0.7421, F1: 0.7400\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6151\n",
      "Val Accuracy: 0.6927\n",
      "Precision: 0.7044, Recall: 0.6927, F1: 0.6882\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5246\n",
      "Val Accuracy: 0.7448\n",
      "Precision: 0.7571, Recall: 0.7448, F1: 0.7417\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4672\n",
      "Val Accuracy: 0.7396\n",
      "Precision: 0.7556, Recall: 0.7396, F1: 0.7354\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4445\n",
      "Val Accuracy: 0.7760\n",
      "Precision: 0.7763, Recall: 0.7760, F1: 0.7760\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4376\n",
      "Val Accuracy: 0.7865\n",
      "Precision: 0.7880, Recall: 0.7865, F1: 0.7862\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6283\n",
      "Val Accuracy: 0.6978\n",
      "Precision: 0.7238, Recall: 0.6978, F1: 0.6888\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5565\n",
      "Val Accuracy: 0.7582\n",
      "Precision: 0.7713, Recall: 0.7582, F1: 0.7553\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4723\n",
      "Val Accuracy: 0.7692\n",
      "Precision: 0.7829, Recall: 0.7692, F1: 0.7664\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4500\n",
      "Val Accuracy: 0.7967\n",
      "Precision: 0.8102, Recall: 0.7967, F1: 0.7945\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4227\n",
      "Val Accuracy: 0.8022\n",
      "Precision: 0.8059, Recall: 0.8022, F1: 0.8016\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6746\n",
      "Val Accuracy: 0.6814\n",
      "Precision: 0.7637, Recall: 0.6814, F1: 0.6544\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5901\n",
      "Val Accuracy: 0.6863\n",
      "Precision: 0.6954, Recall: 0.6863, F1: 0.6826\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5670\n",
      "Val Accuracy: 0.7206\n",
      "Precision: 0.7255, Recall: 0.7206, F1: 0.7191\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5526\n",
      "Val Accuracy: 0.7402\n",
      "Precision: 0.7408, Recall: 0.7402, F1: 0.7400\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5216\n",
      "Val Accuracy: 0.7647\n",
      "Precision: 0.7648, Recall: 0.7647, F1: 0.7647\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6521\n",
      "Val Accuracy: 0.6667\n",
      "Precision: 0.6861, Recall: 0.6667, F1: 0.6577\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6051\n",
      "Val Accuracy: 0.6667\n",
      "Precision: 0.6738, Recall: 0.6667, F1: 0.6632\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5643\n",
      "Val Accuracy: 0.6869\n",
      "Precision: 0.6872, Recall: 0.6869, F1: 0.6867\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5422\n",
      "Val Accuracy: 0.7172\n",
      "Precision: 0.7173, Recall: 0.7172, F1: 0.7171\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5109\n",
      "Val Accuracy: 0.7020\n",
      "Precision: 0.7183, Recall: 0.7020, F1: 0.6964\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6697\n",
      "Val Accuracy: 0.5850\n",
      "Precision: 0.6472, Recall: 0.5850, F1: 0.5360\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6145\n",
      "Val Accuracy: 0.6600\n",
      "Precision: 0.6783, Recall: 0.6600, F1: 0.6511\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5879\n",
      "Val Accuracy: 0.7100\n",
      "Precision: 0.7188, Recall: 0.7100, F1: 0.7071\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5223\n",
      "Val Accuracy: 0.7550\n",
      "Precision: 0.7550, Recall: 0.7550, F1: 0.7550\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5165\n",
      "Val Accuracy: 0.7300\n",
      "Precision: 0.7562, Recall: 0.7300, F1: 0.7229\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6329\n",
      "Val Accuracy: 0.6378\n",
      "Precision: 0.6381, Recall: 0.6378, F1: 0.6375\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5246\n",
      "Val Accuracy: 0.7908\n",
      "Precision: 0.7978, Recall: 0.7908, F1: 0.7896\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4890\n",
      "Val Accuracy: 0.8061\n",
      "Precision: 0.8293, Recall: 0.8061, F1: 0.8026\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4129\n",
      "Val Accuracy: 0.8724\n",
      "Precision: 0.8734, Recall: 0.8724, F1: 0.8724\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3569\n",
      "Val Accuracy: 0.8316\n",
      "Precision: 0.8359, Recall: 0.8316, F1: 0.8311\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6700\n",
      "Val Accuracy: 0.6190\n",
      "Precision: 0.6201, Recall: 0.6190, F1: 0.6182\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6138\n",
      "Val Accuracy: 0.6905\n",
      "Precision: 0.7019, Recall: 0.6905, F1: 0.6860\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5557\n",
      "Val Accuracy: 0.7083\n",
      "Precision: 0.7086, Recall: 0.7083, F1: 0.7082\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5231\n",
      "Val Accuracy: 0.6845\n",
      "Precision: 0.6968, Recall: 0.6845, F1: 0.6795\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5027\n",
      "Val Accuracy: 0.6667\n",
      "Precision: 0.7475, Recall: 0.6667, F1: 0.6370\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6445\n",
      "Val Accuracy: 0.7624\n",
      "Precision: 0.7650, Recall: 0.7624, F1: 0.7618\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5690\n",
      "Val Accuracy: 0.7673\n",
      "Precision: 0.7676, Recall: 0.7673, F1: 0.7673\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5412\n",
      "Val Accuracy: 0.7426\n",
      "Precision: 0.7628, Recall: 0.7426, F1: 0.7375\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5124\n",
      "Val Accuracy: 0.7772\n",
      "Precision: 0.7853, Recall: 0.7772, F1: 0.7756\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5208\n",
      "Val Accuracy: 0.7921\n",
      "Precision: 0.7922, Recall: 0.7921, F1: 0.7921\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6920\n",
      "Val Accuracy: 0.6117\n",
      "Precision: 0.6165, Recall: 0.6117, F1: 0.6077\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6736\n",
      "Val Accuracy: 0.6277\n",
      "Precision: 0.6526, Recall: 0.6277, F1: 0.6118\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6332\n",
      "Val Accuracy: 0.6968\n",
      "Precision: 0.7093, Recall: 0.6968, F1: 0.6922\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6173\n",
      "Val Accuracy: 0.7021\n",
      "Precision: 0.7117, Recall: 0.7021, F1: 0.6987\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5907\n",
      "Val Accuracy: 0.6862\n",
      "Precision: 0.7089, Recall: 0.6862, F1: 0.6774\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6827\n",
      "Val Accuracy: 0.6044\n",
      "Precision: 0.6611, Recall: 0.6044, F1: 0.5662\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6265\n",
      "Val Accuracy: 0.6538\n",
      "Precision: 0.7036, Recall: 0.6538, F1: 0.6313\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6151\n",
      "Val Accuracy: 0.7088\n",
      "Precision: 0.7501, Recall: 0.7088, F1: 0.6962\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5708\n",
      "Val Accuracy: 0.7308\n",
      "Precision: 0.7438, Recall: 0.7308, F1: 0.7271\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5627\n",
      "Val Accuracy: 0.7253\n",
      "Precision: 0.7307, Recall: 0.7253, F1: 0.7236\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6927\n",
      "Val Accuracy: 0.5377\n",
      "Precision: 0.5485, Recall: 0.5377, F1: 0.5105\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6519\n",
      "Val Accuracy: 0.6462\n",
      "Precision: 0.6511, Recall: 0.6462, F1: 0.6434\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5988\n",
      "Val Accuracy: 0.6934\n",
      "Precision: 0.6963, Recall: 0.6934, F1: 0.6922\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5538\n",
      "Val Accuracy: 0.6462\n",
      "Precision: 0.7537, Recall: 0.6462, F1: 0.6043\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5201\n",
      "Val Accuracy: 0.7311\n",
      "Precision: 0.7359, Recall: 0.7311, F1: 0.7298\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6165\n",
      "Val Accuracy: 0.7708\n",
      "Precision: 0.7767, Recall: 0.7708, F1: 0.7696\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4871\n",
      "Val Accuracy: 0.8490\n",
      "Precision: 0.8490, Recall: 0.8490, F1: 0.8490\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3958\n",
      "Val Accuracy: 0.8438\n",
      "Precision: 0.8563, Recall: 0.8438, F1: 0.8424\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3570\n",
      "Val Accuracy: 0.8802\n",
      "Precision: 0.8802, Recall: 0.8802, F1: 0.8802\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3034\n",
      "Val Accuracy: 0.8802\n",
      "Precision: 0.8806, Recall: 0.8802, F1: 0.8802\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6396\n",
      "Val Accuracy: 0.5366\n",
      "Precision: 0.7595, Recall: 0.5366, F1: 0.4098\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4749\n",
      "Val Accuracy: 0.8293\n",
      "Precision: 0.8392, Recall: 0.8293, F1: 0.8280\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3459\n",
      "Val Accuracy: 0.7622\n",
      "Precision: 0.8292, Recall: 0.7622, F1: 0.7494\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3698\n",
      "Val Accuracy: 0.7622\n",
      "Precision: 0.8059, Recall: 0.7622, F1: 0.7534\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3299\n",
      "Val Accuracy: 0.8780\n",
      "Precision: 0.8783, Recall: 0.8780, F1: 0.8780\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5269\n",
      "Val Accuracy: 0.8837\n",
      "Precision: 0.8839, Recall: 0.8837, F1: 0.8837\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3230\n",
      "Val Accuracy: 0.8953\n",
      "Precision: 0.8973, Recall: 0.8953, F1: 0.8952\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2749\n",
      "Val Accuracy: 0.8953\n",
      "Precision: 0.8962, Recall: 0.8953, F1: 0.8953\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2475\n",
      "Val Accuracy: 0.9070\n",
      "Precision: 0.9079, Recall: 0.9070, F1: 0.9069\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2239\n",
      "Val Accuracy: 0.8953\n",
      "Precision: 0.8953, Recall: 0.8953, F1: 0.8953\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6600\n",
      "Val Accuracy: 0.5826\n",
      "Precision: 0.6081, Recall: 0.5826, F1: 0.5563\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6272\n",
      "Val Accuracy: 0.6147\n",
      "Precision: 0.6370, Recall: 0.6147, F1: 0.5983\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5906\n",
      "Val Accuracy: 0.6101\n",
      "Precision: 0.6101, Recall: 0.6101, F1: 0.6101\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5729\n",
      "Val Accuracy: 0.6514\n",
      "Precision: 0.6522, Recall: 0.6514, F1: 0.6509\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5104\n",
      "Val Accuracy: 0.6009\n",
      "Precision: 0.6354, Recall: 0.6009, F1: 0.5738\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6544\n",
      "Val Accuracy: 0.7071\n",
      "Precision: 0.7074, Recall: 0.7071, F1: 0.7070\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5659\n",
      "Val Accuracy: 0.7323\n",
      "Precision: 0.7456, Recall: 0.7323, F1: 0.7287\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4867\n",
      "Val Accuracy: 0.7323\n",
      "Precision: 0.7576, Recall: 0.7323, F1: 0.7256\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4529\n",
      "Val Accuracy: 0.7071\n",
      "Precision: 0.7092, Recall: 0.7071, F1: 0.7063\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4271\n",
      "Val Accuracy: 0.7677\n",
      "Precision: 0.7694, Recall: 0.7677, F1: 0.7673\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4814\n",
      "Val Accuracy: 0.9076\n",
      "Precision: 0.9080, Recall: 0.9076, F1: 0.9076\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.2652\n",
      "Val Accuracy: 0.9348\n",
      "Precision: 0.9356, Recall: 0.9348, F1: 0.9348\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1914\n",
      "Val Accuracy: 0.9457\n",
      "Precision: 0.9465, Recall: 0.9457, F1: 0.9456\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1526\n",
      "Val Accuracy: 0.9348\n",
      "Precision: 0.9366, Recall: 0.9348, F1: 0.9347\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1537\n",
      "Val Accuracy: 0.9565\n",
      "Precision: 0.9567, Recall: 0.9565, F1: 0.9565\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4772\n",
      "Val Accuracy: 0.8943\n",
      "Precision: 0.8981, Recall: 0.8943, F1: 0.8941\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3225\n",
      "Val Accuracy: 0.8862\n",
      "Precision: 0.8967, Recall: 0.8862, F1: 0.8854\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2236\n",
      "Val Accuracy: 0.9065\n",
      "Precision: 0.9078, Recall: 0.9065, F1: 0.9064\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1872\n",
      "Val Accuracy: 0.9309\n",
      "Precision: 0.9312, Recall: 0.9309, F1: 0.9309\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1709\n",
      "Val Accuracy: 0.9187\n",
      "Precision: 0.9197, Recall: 0.9187, F1: 0.9187\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6173\n",
      "Val Accuracy: 0.8000\n",
      "Precision: 0.8049, Recall: 0.8000, F1: 0.7992\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4615\n",
      "Val Accuracy: 0.8364\n",
      "Precision: 0.8392, Recall: 0.8364, F1: 0.8360\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3921\n",
      "Val Accuracy: 0.8273\n",
      "Precision: 0.8385, Recall: 0.8273, F1: 0.8258\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3340\n",
      "Val Accuracy: 0.8591\n",
      "Precision: 0.8615, Recall: 0.8591, F1: 0.8589\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3156\n",
      "Val Accuracy: 0.8364\n",
      "Precision: 0.8479, Recall: 0.8364, F1: 0.8350\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6937\n",
      "Val Accuracy: 0.5165\n",
      "Precision: 0.6542, Recall: 0.5165, F1: 0.3775\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6500\n",
      "Val Accuracy: 0.6099\n",
      "Precision: 0.6149, Recall: 0.6099, F1: 0.6056\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6109\n",
      "Val Accuracy: 0.6978\n",
      "Precision: 0.6980, Recall: 0.6978, F1: 0.6977\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5778\n",
      "Val Accuracy: 0.7033\n",
      "Precision: 0.7033, Recall: 0.7033, F1: 0.7033\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5382\n",
      "Val Accuracy: 0.7088\n",
      "Precision: 0.7183, Recall: 0.7088, F1: 0.7056\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6694\n",
      "Val Accuracy: 0.6373\n",
      "Precision: 0.7594, Recall: 0.6373, F1: 0.5888\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5577\n",
      "Val Accuracy: 0.7549\n",
      "Precision: 0.7558, Recall: 0.7549, F1: 0.7547\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4692\n",
      "Val Accuracy: 0.8088\n",
      "Precision: 0.8103, Recall: 0.8088, F1: 0.8086\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4204\n",
      "Val Accuracy: 0.8480\n",
      "Precision: 0.8521, Recall: 0.8480, F1: 0.8476\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3770\n",
      "Val Accuracy: 0.8578\n",
      "Precision: 0.8587, Recall: 0.8578, F1: 0.8578\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6550\n",
      "Val Accuracy: 0.6542\n",
      "Precision: 0.6794, Recall: 0.6542, F1: 0.6416\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5548\n",
      "Val Accuracy: 0.7625\n",
      "Precision: 0.7813, Recall: 0.7625, F1: 0.7585\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5212\n",
      "Val Accuracy: 0.7750\n",
      "Precision: 0.7865, Recall: 0.7750, F1: 0.7727\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4841\n",
      "Val Accuracy: 0.7750\n",
      "Precision: 0.7908, Recall: 0.7750, F1: 0.7719\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4350\n",
      "Val Accuracy: 0.7667\n",
      "Precision: 0.7930, Recall: 0.7667, F1: 0.7613\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6608\n",
      "Val Accuracy: 0.7400\n",
      "Precision: 0.7674, Recall: 0.7400, F1: 0.7332\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5133\n",
      "Val Accuracy: 0.8550\n",
      "Precision: 0.8748, Recall: 0.8550, F1: 0.8531\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3927\n",
      "Val Accuracy: 0.8550\n",
      "Precision: 0.8553, Recall: 0.8550, F1: 0.8550\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4161\n",
      "Val Accuracy: 0.8600\n",
      "Precision: 0.8601, Recall: 0.8600, F1: 0.8600\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3648\n",
      "Val Accuracy: 0.8750\n",
      "Precision: 0.8923, Recall: 0.8750, F1: 0.8736\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6050\n",
      "Val Accuracy: 0.6989\n",
      "Precision: 0.7441, Recall: 0.6989, F1: 0.6843\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4342\n",
      "Val Accuracy: 0.7957\n",
      "Precision: 0.7962, Recall: 0.7957, F1: 0.7956\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3871\n",
      "Val Accuracy: 0.7688\n",
      "Precision: 0.7760, Recall: 0.7688, F1: 0.7673\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3419\n",
      "Val Accuracy: 0.8011\n",
      "Precision: 0.8014, Recall: 0.8011, F1: 0.8010\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3149\n",
      "Val Accuracy: 0.8011\n",
      "Precision: 0.8019, Recall: 0.8011, F1: 0.8009\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4706\n",
      "Val Accuracy: 0.8636\n",
      "Precision: 0.8656, Recall: 0.8636, F1: 0.8635\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3064\n",
      "Val Accuracy: 0.9273\n",
      "Precision: 0.9274, Recall: 0.9273, F1: 0.9273\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2139\n",
      "Val Accuracy: 0.9091\n",
      "Precision: 0.9125, Recall: 0.9091, F1: 0.9089\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1713\n",
      "Val Accuracy: 0.9455\n",
      "Precision: 0.9456, Recall: 0.9455, F1: 0.9455\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1154\n",
      "Val Accuracy: 0.9545\n",
      "Precision: 0.9551, Recall: 0.9545, F1: 0.9545\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5841\n",
      "Val Accuracy: 0.7718\n",
      "Precision: 0.7777, Recall: 0.7718, F1: 0.7706\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4799\n",
      "Val Accuracy: 0.8252\n",
      "Precision: 0.8252, Recall: 0.8252, F1: 0.8252\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4445\n",
      "Val Accuracy: 0.8447\n",
      "Precision: 0.8447, Recall: 0.8447, F1: 0.8447\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3849\n",
      "Val Accuracy: 0.8592\n",
      "Precision: 0.8748, Recall: 0.8592, F1: 0.8577\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3769\n",
      "Val Accuracy: 0.8592\n",
      "Precision: 0.8748, Recall: 0.8592, F1: 0.8577\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5160\n",
      "Val Accuracy: 0.8034\n",
      "Precision: 0.8043, Recall: 0.8034, F1: 0.8032\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3525\n",
      "Val Accuracy: 0.8371\n",
      "Precision: 0.8381, Recall: 0.8371, F1: 0.8369\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3306\n",
      "Val Accuracy: 0.8427\n",
      "Precision: 0.8455, Recall: 0.8427, F1: 0.8424\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2974\n",
      "Val Accuracy: 0.8371\n",
      "Precision: 0.8381, Recall: 0.8371, F1: 0.8369\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2460\n",
      "Val Accuracy: 0.8652\n",
      "Precision: 0.8659, Recall: 0.8652, F1: 0.8651\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6300\n",
      "Val Accuracy: 0.6619\n",
      "Precision: 0.7119, Recall: 0.6619, F1: 0.6407\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5424\n",
      "Val Accuracy: 0.7143\n",
      "Precision: 0.7162, Recall: 0.7143, F1: 0.7136\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4633\n",
      "Val Accuracy: 0.7524\n",
      "Precision: 0.7782, Recall: 0.7524, F1: 0.7465\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4432\n",
      "Val Accuracy: 0.7571\n",
      "Precision: 0.7600, Recall: 0.7571, F1: 0.7565\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4117\n",
      "Val Accuracy: 0.8048\n",
      "Precision: 0.8055, Recall: 0.8048, F1: 0.8047\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6491\n",
      "Val Accuracy: 0.7262\n",
      "Precision: 0.7263, Recall: 0.7262, F1: 0.7262\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5971\n",
      "Val Accuracy: 0.7619\n",
      "Precision: 0.7643, Recall: 0.7619, F1: 0.7614\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5620\n",
      "Val Accuracy: 0.6786\n",
      "Precision: 0.7766, Recall: 0.6786, F1: 0.6473\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5459\n",
      "Val Accuracy: 0.7798\n",
      "Precision: 0.7948, Recall: 0.7798, F1: 0.7769\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4902\n",
      "Val Accuracy: 0.7917\n",
      "Precision: 0.7927, Recall: 0.7917, F1: 0.7915\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6740\n",
      "Val Accuracy: 0.6515\n",
      "Precision: 0.6680, Recall: 0.6515, F1: 0.6428\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6296\n",
      "Val Accuracy: 0.7121\n",
      "Precision: 0.7123, Recall: 0.7121, F1: 0.7121\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5906\n",
      "Val Accuracy: 0.7273\n",
      "Precision: 0.7350, Recall: 0.7273, F1: 0.7250\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5631\n",
      "Val Accuracy: 0.7374\n",
      "Precision: 0.7735, Recall: 0.7374, F1: 0.7284\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5217\n",
      "Val Accuracy: 0.7374\n",
      "Precision: 0.7437, Recall: 0.7374, F1: 0.7356\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6645\n",
      "Val Accuracy: 0.6450\n",
      "Precision: 0.6710, Recall: 0.6450, F1: 0.6310\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6122\n",
      "Val Accuracy: 0.7000\n",
      "Precision: 0.7003, Recall: 0.7000, F1: 0.6999\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5650\n",
      "Val Accuracy: 0.7050\n",
      "Precision: 0.7097, Recall: 0.7050, F1: 0.7033\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4970\n",
      "Val Accuracy: 0.7500\n",
      "Precision: 0.7584, Recall: 0.7500, F1: 0.7480\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4719\n",
      "Val Accuracy: 0.7500\n",
      "Precision: 0.7627, Recall: 0.7500, F1: 0.7469\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5688\n",
      "Val Accuracy: 0.6827\n",
      "Precision: 0.7076, Recall: 0.6827, F1: 0.6729\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5060\n",
      "Val Accuracy: 0.7212\n",
      "Precision: 0.7810, Recall: 0.7212, F1: 0.7055\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4361\n",
      "Val Accuracy: 0.7452\n",
      "Precision: 0.7691, Recall: 0.7452, F1: 0.7394\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3834\n",
      "Val Accuracy: 0.7596\n",
      "Precision: 0.7659, Recall: 0.7596, F1: 0.7582\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3873\n",
      "Val Accuracy: 0.7837\n",
      "Precision: 0.8010, Recall: 0.7837, F1: 0.7805\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5424\n",
      "Val Accuracy: 0.8298\n",
      "Precision: 0.8528, Recall: 0.8298, F1: 0.8270\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3189\n",
      "Val Accuracy: 0.8564\n",
      "Precision: 0.8791, Recall: 0.8564, F1: 0.8542\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2628\n",
      "Val Accuracy: 0.9149\n",
      "Precision: 0.9179, Recall: 0.9149, F1: 0.9147\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2399\n",
      "Val Accuracy: 0.9096\n",
      "Precision: 0.9107, Recall: 0.9096, F1: 0.9095\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1691\n",
      "Val Accuracy: 0.9309\n",
      "Precision: 0.9368, Recall: 0.9309, F1: 0.9306\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5542\n",
      "Val Accuracy: 0.7500\n",
      "Precision: 0.7720, Recall: 0.7500, F1: 0.7448\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3907\n",
      "Val Accuracy: 0.8137\n",
      "Precision: 0.8263, Recall: 0.8137, F1: 0.8119\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3408\n",
      "Val Accuracy: 0.8480\n",
      "Precision: 0.8483, Recall: 0.8480, F1: 0.8480\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2947\n",
      "Val Accuracy: 0.8578\n",
      "Precision: 0.8658, Recall: 0.8578, F1: 0.8571\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2587\n",
      "Val Accuracy: 0.8382\n",
      "Precision: 0.8391, Recall: 0.8382, F1: 0.8381\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6513\n",
      "Val Accuracy: 0.7269\n",
      "Precision: 0.7484, Recall: 0.7269, F1: 0.7209\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4921\n",
      "Val Accuracy: 0.7941\n",
      "Precision: 0.7958, Recall: 0.7941, F1: 0.7938\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4237\n",
      "Val Accuracy: 0.7983\n",
      "Precision: 0.8038, Recall: 0.7983, F1: 0.7974\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3644\n",
      "Val Accuracy: 0.8319\n",
      "Precision: 0.8328, Recall: 0.8319, F1: 0.8318\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3397\n",
      "Val Accuracy: 0.8403\n",
      "Precision: 0.8407, Recall: 0.8403, F1: 0.8403\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5212\n",
      "Val Accuracy: 0.8241\n",
      "Precision: 0.8241, Recall: 0.8241, F1: 0.8241\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3777\n",
      "Val Accuracy: 0.8657\n",
      "Precision: 0.8660, Recall: 0.8657, F1: 0.8657\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3076\n",
      "Val Accuracy: 0.9120\n",
      "Precision: 0.9138, Recall: 0.9120, F1: 0.9119\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2468\n",
      "Val Accuracy: 0.8796\n",
      "Precision: 0.8808, Recall: 0.8796, F1: 0.8795\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1996\n",
      "Val Accuracy: 0.8981\n",
      "Precision: 0.9095, Recall: 0.8981, F1: 0.8974\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6554\n",
      "Val Accuracy: 0.5647\n",
      "Precision: 0.5656, Recall: 0.5647, F1: 0.5632\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6101\n",
      "Val Accuracy: 0.5706\n",
      "Precision: 0.6283, Recall: 0.5706, F1: 0.5162\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5756\n",
      "Val Accuracy: 0.5647\n",
      "Precision: 0.6143, Recall: 0.5647, F1: 0.5117\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5647\n",
      "Val Accuracy: 0.6118\n",
      "Precision: 0.6709, Recall: 0.6118, F1: 0.5750\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5373\n",
      "Val Accuracy: 0.6529\n",
      "Precision: 0.6593, Recall: 0.6529, F1: 0.6494\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4816\n",
      "Val Accuracy: 0.8088\n",
      "Precision: 0.8139, Recall: 0.8088, F1: 0.8080\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3315\n",
      "Val Accuracy: 0.8529\n",
      "Precision: 0.8597, Recall: 0.8529, F1: 0.8522\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3451\n",
      "Val Accuracy: 0.8382\n",
      "Precision: 0.8398, Recall: 0.8382, F1: 0.8380\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2881\n",
      "Val Accuracy: 0.8578\n",
      "Precision: 0.8579, Recall: 0.8578, F1: 0.8578\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2788\n",
      "Val Accuracy: 0.8676\n",
      "Precision: 0.8694, Recall: 0.8676, F1: 0.8675\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6637\n",
      "Val Accuracy: 0.6485\n",
      "Precision: 0.6552, Recall: 0.6485, F1: 0.6447\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5973\n",
      "Val Accuracy: 0.6881\n",
      "Precision: 0.7004, Recall: 0.6881, F1: 0.6833\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5595\n",
      "Val Accuracy: 0.7030\n",
      "Precision: 0.7037, Recall: 0.7030, F1: 0.7027\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5059\n",
      "Val Accuracy: 0.7030\n",
      "Precision: 0.7030, Recall: 0.7030, F1: 0.7029\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5123\n",
      "Val Accuracy: 0.7079\n",
      "Precision: 0.7114, Recall: 0.7079, F1: 0.7067\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6405\n",
      "Val Accuracy: 0.5815\n",
      "Precision: 0.7722, Recall: 0.5815, F1: 0.4927\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4276\n",
      "Val Accuracy: 0.8967\n",
      "Precision: 0.8979, Recall: 0.8967, F1: 0.8967\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2858\n",
      "Val Accuracy: 0.8913\n",
      "Precision: 0.8915, Recall: 0.8913, F1: 0.8913\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2534\n",
      "Val Accuracy: 0.9022\n",
      "Precision: 0.9070, Recall: 0.9022, F1: 0.9019\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2309\n",
      "Val Accuracy: 0.8967\n",
      "Precision: 0.8968, Recall: 0.8967, F1: 0.8967\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5571\n",
      "Val Accuracy: 0.8306\n",
      "Precision: 0.8372, Recall: 0.8306, F1: 0.8297\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4045\n",
      "Val Accuracy: 0.8719\n",
      "Precision: 0.8731, Recall: 0.8719, F1: 0.8718\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3403\n",
      "Val Accuracy: 0.8595\n",
      "Precision: 0.8718, Recall: 0.8595, F1: 0.8583\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3222\n",
      "Val Accuracy: 0.8719\n",
      "Precision: 0.8794, Recall: 0.8719, F1: 0.8713\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2677\n",
      "Val Accuracy: 0.8719\n",
      "Precision: 0.8719, Recall: 0.8719, F1: 0.8719\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6845\n",
      "Val Accuracy: 0.5870\n",
      "Precision: 0.5897, Recall: 0.5870, F1: 0.5838\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6220\n",
      "Val Accuracy: 0.6576\n",
      "Precision: 0.6681, Recall: 0.6576, F1: 0.6522\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5759\n",
      "Val Accuracy: 0.7174\n",
      "Precision: 0.7190, Recall: 0.7174, F1: 0.7169\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5287\n",
      "Val Accuracy: 0.6848\n",
      "Precision: 0.7008, Recall: 0.6848, F1: 0.6784\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4879\n",
      "Val Accuracy: 0.7446\n",
      "Precision: 0.7460, Recall: 0.7446, F1: 0.7442\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6930\n",
      "Val Accuracy: 0.5583\n",
      "Precision: 0.7299, Recall: 0.5583, F1: 0.4569\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6564\n",
      "Val Accuracy: 0.6505\n",
      "Precision: 0.7075, Recall: 0.6505, F1: 0.6247\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6028\n",
      "Val Accuracy: 0.6893\n",
      "Precision: 0.6900, Recall: 0.6893, F1: 0.6891\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5426\n",
      "Val Accuracy: 0.6699\n",
      "Precision: 0.6766, Recall: 0.6699, F1: 0.6668\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5263\n",
      "Val Accuracy: 0.6942\n",
      "Precision: 0.6946, Recall: 0.6942, F1: 0.6940\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6912\n",
      "Val Accuracy: 0.5000\n",
      "Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6713\n",
      "Val Accuracy: 0.5795\n",
      "Precision: 0.6818, Recall: 0.5795, F1: 0.5107\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6282\n",
      "Val Accuracy: 0.6648\n",
      "Precision: 0.6769, Recall: 0.6648, F1: 0.6589\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5536\n",
      "Val Accuracy: 0.7216\n",
      "Precision: 0.7282, Recall: 0.7216, F1: 0.7196\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5189\n",
      "Val Accuracy: 0.7045\n",
      "Precision: 0.7047, Recall: 0.7045, F1: 0.7045\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6945\n",
      "Val Accuracy: 0.5144\n",
      "Precision: 0.7537, Recall: 0.5144, F1: 0.3646\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6231\n",
      "Val Accuracy: 0.7260\n",
      "Precision: 0.7285, Recall: 0.7260, F1: 0.7252\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5476\n",
      "Val Accuracy: 0.7163\n",
      "Precision: 0.7180, Recall: 0.7163, F1: 0.7158\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5107\n",
      "Val Accuracy: 0.7356\n",
      "Precision: 0.7456, Recall: 0.7356, F1: 0.7329\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4730\n",
      "Val Accuracy: 0.7452\n",
      "Precision: 0.7480, Recall: 0.7452, F1: 0.7445\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6687\n",
      "Val Accuracy: 0.6927\n",
      "Precision: 0.6927, Recall: 0.6927, F1: 0.6927\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6077\n",
      "Val Accuracy: 0.7292\n",
      "Precision: 0.7308, Recall: 0.7292, F1: 0.7287\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5232\n",
      "Val Accuracy: 0.8073\n",
      "Precision: 0.8089, Recall: 0.8073, F1: 0.8070\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4505\n",
      "Val Accuracy: 0.8229\n",
      "Precision: 0.8235, Recall: 0.8229, F1: 0.8228\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3941\n",
      "Val Accuracy: 0.8021\n",
      "Precision: 0.8158, Recall: 0.8021, F1: 0.7999\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5256\n",
      "Val Accuracy: 0.8571\n",
      "Precision: 0.8615, Recall: 0.8571, F1: 0.8567\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3277\n",
      "Val Accuracy: 0.8791\n",
      "Precision: 0.8808, Recall: 0.8791, F1: 0.8790\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2593\n",
      "Val Accuracy: 0.8901\n",
      "Precision: 0.8918, Recall: 0.8901, F1: 0.8900\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2334\n",
      "Val Accuracy: 0.9176\n",
      "Precision: 0.9201, Recall: 0.9176, F1: 0.9175\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1961\n",
      "Val Accuracy: 0.9396\n",
      "Precision: 0.9396, Recall: 0.9396, F1: 0.9396\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5905\n",
      "Val Accuracy: 0.8113\n",
      "Precision: 0.8206, Recall: 0.8113, F1: 0.8100\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4956\n",
      "Val Accuracy: 0.8443\n",
      "Precision: 0.8446, Recall: 0.8443, F1: 0.8443\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3859\n",
      "Val Accuracy: 0.8066\n",
      "Precision: 0.8247, Recall: 0.8066, F1: 0.8039\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3570\n",
      "Val Accuracy: 0.8491\n",
      "Precision: 0.8536, Recall: 0.8491, F1: 0.8486\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3204\n",
      "Val Accuracy: 0.9104\n",
      "Precision: 0.9148, Recall: 0.9104, F1: 0.9101\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6868\n",
      "Val Accuracy: 0.5761\n",
      "Precision: 0.5839, Recall: 0.5761, F1: 0.5660\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6311\n",
      "Val Accuracy: 0.6413\n",
      "Precision: 0.6424, Recall: 0.6413, F1: 0.6406\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5686\n",
      "Val Accuracy: 0.6957\n",
      "Precision: 0.7226, Recall: 0.6957, F1: 0.6862\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5440\n",
      "Val Accuracy: 0.6848\n",
      "Precision: 0.6905, Recall: 0.6848, F1: 0.6824\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5179\n",
      "Val Accuracy: 0.6793\n",
      "Precision: 0.6842, Recall: 0.6793, F1: 0.6772\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6169\n",
      "Val Accuracy: 0.7857\n",
      "Precision: 0.7937, Recall: 0.7857, F1: 0.7842\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5108\n",
      "Val Accuracy: 0.8242\n",
      "Precision: 0.8281, Recall: 0.8242, F1: 0.8236\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4537\n",
      "Val Accuracy: 0.7802\n",
      "Precision: 0.8012, Recall: 0.7802, F1: 0.7763\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4358\n",
      "Val Accuracy: 0.8462\n",
      "Precision: 0.8463, Recall: 0.8462, F1: 0.8461\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3928\n",
      "Val Accuracy: 0.8352\n",
      "Precision: 0.8366, Recall: 0.8352, F1: 0.8350\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6044\n",
      "Val Accuracy: 0.7900\n",
      "Precision: 0.7942, Recall: 0.7900, F1: 0.7892\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5083\n",
      "Val Accuracy: 0.7850\n",
      "Precision: 0.7857, Recall: 0.7850, F1: 0.7849\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4736\n",
      "Val Accuracy: 0.7600\n",
      "Precision: 0.7626, Recall: 0.7600, F1: 0.7594\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4441\n",
      "Val Accuracy: 0.7950\n",
      "Precision: 0.7950, Recall: 0.7950, F1: 0.7950\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4172\n",
      "Val Accuracy: 0.8050\n",
      "Precision: 0.8164, Recall: 0.8050, F1: 0.8032\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6807\n",
      "Val Accuracy: 0.6095\n",
      "Precision: 0.6385, Recall: 0.6095, F1: 0.5880\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5932\n",
      "Val Accuracy: 0.6810\n",
      "Precision: 0.6810, Recall: 0.6810, F1: 0.6809\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5077\n",
      "Val Accuracy: 0.6619\n",
      "Precision: 0.7444, Recall: 0.6619, F1: 0.6307\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4653\n",
      "Val Accuracy: 0.7667\n",
      "Precision: 0.7801, Recall: 0.7667, F1: 0.7638\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4073\n",
      "Val Accuracy: 0.7571\n",
      "Precision: 0.7853, Recall: 0.7571, F1: 0.7510\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6410\n",
      "Val Accuracy: 0.6944\n",
      "Precision: 0.7137, Recall: 0.6944, F1: 0.6874\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5814\n",
      "Val Accuracy: 0.7111\n",
      "Precision: 0.7163, Recall: 0.7111, F1: 0.7094\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5336\n",
      "Val Accuracy: 0.7000\n",
      "Precision: 0.7009, Recall: 0.7000, F1: 0.6997\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4979\n",
      "Val Accuracy: 0.6889\n",
      "Precision: 0.7162, Recall: 0.6889, F1: 0.6787\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5033\n",
      "Val Accuracy: 0.6944\n",
      "Precision: 0.6964, Recall: 0.6944, F1: 0.6937\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6678\n",
      "Val Accuracy: 0.6963\n",
      "Precision: 0.7483, Recall: 0.6963, F1: 0.6795\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5903\n",
      "Val Accuracy: 0.7477\n",
      "Precision: 0.7566, Recall: 0.7477, F1: 0.7454\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5559\n",
      "Val Accuracy: 0.7383\n",
      "Precision: 0.7651, Recall: 0.7383, F1: 0.7315\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4944\n",
      "Val Accuracy: 0.7991\n",
      "Precision: 0.8004, Recall: 0.7991, F1: 0.7989\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4681\n",
      "Val Accuracy: 0.7710\n",
      "Precision: 0.8078, Recall: 0.7710, F1: 0.7640\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6272\n",
      "Val Accuracy: 0.7088\n",
      "Precision: 0.7163, Recall: 0.7088, F1: 0.7062\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5356\n",
      "Val Accuracy: 0.6978\n",
      "Precision: 0.6990, Recall: 0.6978, F1: 0.6974\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4958\n",
      "Val Accuracy: 0.7308\n",
      "Precision: 0.7330, Recall: 0.7308, F1: 0.7301\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4504\n",
      "Val Accuracy: 0.7308\n",
      "Precision: 0.7438, Recall: 0.7308, F1: 0.7271\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4137\n",
      "Val Accuracy: 0.8077\n",
      "Precision: 0.8077, Recall: 0.8077, F1: 0.8077\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6702\n",
      "Val Accuracy: 0.6947\n",
      "Precision: 0.7318, Recall: 0.6947, F1: 0.6820\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5668\n",
      "Val Accuracy: 0.7263\n",
      "Precision: 0.7347, Recall: 0.7263, F1: 0.7238\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4793\n",
      "Val Accuracy: 0.7947\n",
      "Precision: 0.7950, Recall: 0.7947, F1: 0.7947\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4138\n",
      "Val Accuracy: 0.8263\n",
      "Precision: 0.8371, Recall: 0.8263, F1: 0.8249\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3742\n",
      "Val Accuracy: 0.8316\n",
      "Precision: 0.8413, Recall: 0.8316, F1: 0.8304\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5966\n",
      "Val Accuracy: 0.7523\n",
      "Precision: 0.7541, Recall: 0.7523, F1: 0.7519\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4699\n",
      "Val Accuracy: 0.7897\n",
      "Precision: 0.7899, Recall: 0.7897, F1: 0.7897\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4183\n",
      "Val Accuracy: 0.8131\n",
      "Precision: 0.8185, Recall: 0.8131, F1: 0.8123\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3663\n",
      "Val Accuracy: 0.8037\n",
      "Precision: 0.8037, Recall: 0.8037, F1: 0.8037\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3534\n",
      "Val Accuracy: 0.8224\n",
      "Precision: 0.8229, Recall: 0.8224, F1: 0.8224\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6267\n",
      "Val Accuracy: 0.6283\n",
      "Precision: 0.7012, Recall: 0.6283, F1: 0.5913\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6381\n",
      "Val Accuracy: 0.6637\n",
      "Precision: 0.6930, Recall: 0.6637, F1: 0.6505\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5888\n",
      "Val Accuracy: 0.6593\n",
      "Precision: 0.6784, Recall: 0.6593, F1: 0.6499\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5810\n",
      "Val Accuracy: 0.6239\n",
      "Precision: 0.6275, Recall: 0.6239, F1: 0.6212\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5870\n",
      "Val Accuracy: 0.6726\n",
      "Precision: 0.7146, Recall: 0.6726, F1: 0.6557\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6156\n",
      "Val Accuracy: 0.7000\n",
      "Precision: 0.7501, Recall: 0.7000, F1: 0.6842\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4496\n",
      "Val Accuracy: 0.8476\n",
      "Precision: 0.8476, Recall: 0.8476, F1: 0.8476\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3737\n",
      "Val Accuracy: 0.8857\n",
      "Precision: 0.8863, Recall: 0.8857, F1: 0.8857\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2947\n",
      "Val Accuracy: 0.8905\n",
      "Precision: 0.8986, Recall: 0.8905, F1: 0.8899\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2796\n",
      "Val Accuracy: 0.9143\n",
      "Precision: 0.9149, Recall: 0.9143, F1: 0.9143\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4720\n",
      "Val Accuracy: 0.8738\n",
      "Precision: 0.8789, Recall: 0.8738, F1: 0.8734\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3028\n",
      "Val Accuracy: 0.8592\n",
      "Precision: 0.8601, Recall: 0.8592, F1: 0.8591\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2624\n",
      "Val Accuracy: 0.8641\n",
      "Precision: 0.8889, Recall: 0.8641, F1: 0.8619\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2551\n",
      "Val Accuracy: 0.8495\n",
      "Precision: 0.8753, Recall: 0.8495, F1: 0.8469\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2713\n",
      "Val Accuracy: 0.8786\n",
      "Precision: 0.8868, Recall: 0.8786, F1: 0.8780\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.5385\n",
      "Val Accuracy: 0.7739\n",
      "Precision: 0.7825, Recall: 0.7739, F1: 0.7722\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4345\n",
      "Val Accuracy: 0.7391\n",
      "Precision: 0.7896, Recall: 0.7391, F1: 0.7273\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4151\n",
      "Val Accuracy: 0.8000\n",
      "Precision: 0.8023, Recall: 0.8000, F1: 0.7996\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3649\n",
      "Val Accuracy: 0.7870\n",
      "Precision: 0.8037, Recall: 0.7870, F1: 0.7840\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4209\n",
      "Val Accuracy: 0.8000\n",
      "Precision: 0.8162, Recall: 0.8000, F1: 0.7974\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6553\n",
      "Val Accuracy: 0.6809\n",
      "Precision: 0.6985, Recall: 0.6809, F1: 0.6736\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5971\n",
      "Val Accuracy: 0.7819\n",
      "Precision: 0.7874, Recall: 0.7819, F1: 0.7809\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5059\n",
      "Val Accuracy: 0.8085\n",
      "Precision: 0.8098, Recall: 0.8085, F1: 0.8083\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4529\n",
      "Val Accuracy: 0.8404\n",
      "Precision: 0.8406, Recall: 0.8404, F1: 0.8404\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4077\n",
      "Val Accuracy: 0.8138\n",
      "Precision: 0.8303, Recall: 0.8138, F1: 0.8115\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6893\n",
      "Val Accuracy: 0.5130\n",
      "Precision: 0.7533, Recall: 0.5130, F1: 0.3617\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6549\n",
      "Val Accuracy: 0.6957\n",
      "Precision: 0.7144, Recall: 0.6957, F1: 0.6889\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5972\n",
      "Val Accuracy: 0.6783\n",
      "Precision: 0.7122, Recall: 0.6783, F1: 0.6649\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4716\n",
      "Val Accuracy: 0.6696\n",
      "Precision: 0.7175, Recall: 0.6696, F1: 0.6503\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4605\n",
      "Val Accuracy: 0.7783\n",
      "Precision: 0.8001, Recall: 0.7783, F1: 0.7742\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6697\n",
      "Val Accuracy: 0.6441\n",
      "Precision: 0.6696, Recall: 0.6441, F1: 0.6303\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5896\n",
      "Val Accuracy: 0.6351\n",
      "Precision: 0.6392, Recall: 0.6351, F1: 0.6324\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5363\n",
      "Val Accuracy: 0.6441\n",
      "Precision: 0.6476, Recall: 0.6441, F1: 0.6420\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4934\n",
      "Val Accuracy: 0.7072\n",
      "Precision: 0.7135, Recall: 0.7072, F1: 0.7050\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4687\n",
      "Val Accuracy: 0.6937\n",
      "Precision: 0.7002, Recall: 0.6937, F1: 0.6912\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key in task_dict.keys():\n",
    "  \n",
    "    tasks=task_dict[key];\n",
    "    print(len(tasks))\n",
    "    num_classes=num_classes_dict[key]\n",
    "    models=[]\n",
    "    # exclude_keys=list(conflict_model.state_dict().keys())\n",
    "    # exclude_keys.remove('encoder.encoced_blocks.2.0.weight')\n",
    "    exclude_keys=[]\n",
    "    for task_id in range(num_classes):\n",
    "        temp_model=copy.deepcopy(conflict_model);\n",
    "        # model_state_dict=conflict_model.state_dict()\n",
    "        history={}\n",
    "        model_state_dict=train_model_extra_metric(temp_model,tasks[task_id]['train'],tasks[task_id]['val'],num_epochs=5,device=device)\n",
    "        model_state_dict=model_state_dict.state_dict()\n",
    "        # print(model_state_dict)\n",
    "        # test_model(temp_model,tasks[task_id]['val'])\n",
    "        # break;\n",
    "        model_layer=remove_keys(model_state_dict,exclude_keys)\n",
    "        # torch.save(temp_model.state_dict(), f'model_{task_id}.pt')\n",
    "        # print(model_layer)\n",
    "        models.append(model_layer);\n",
    "        \n",
    "        del history\n",
    "        del model_state_dict\n",
    "        del temp_model;\n",
    "        del model_layer\n",
    "        # break;\n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "\n",
    "    \n",
    "    conflict2d=[[0 for y in range(num_classes)]for x in range(num_classes)]\n",
    "    for tp in types:\n",
    "        if tp in flags:\n",
    "            exclude_keys=list(conflict_model.state_dict().keys())\n",
    "            exclude_keys.remove('encoder.encoced_blocks.2.0.weight')\n",
    "        else:\n",
    "            exclude_keys=[];\n",
    "        # num_classes=1;\n",
    "        for metric in metrics.keys():\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                for j in range(num_classes):\n",
    "                    \n",
    "                    conflict2d[i][j]=metrics[metric]([models[i],models[j]],conflict_model,exclude_keys,CNN,2)[0]\n",
    "\n",
    "                    # conflict2d[i][j]=metrics[metric]([models[i],models[j]],conflict_model,exclude_keys,CNN,2)[0]\n",
    "            \n",
    "                \n",
    "            np_conflict=np.array(conflict2d)\n",
    "            np.savetxt(f\"{key}_{tp}_{metric}.csv\", np_conflict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5e5bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T15:10:37.508837Z",
     "iopub.status.busy": "2025-04-03T15:10:37.508416Z",
     "iopub.status.idle": "2025-04-03T15:10:37.512860Z",
     "shell.execute_reply": "2025-04-03T15:10:37.511694Z"
    },
    "papermill": {
     "duration": 0.057598,
     "end_time": "2025-04-03T15:10:37.514490",
     "exception": false,
     "start_time": "2025-04-03T15:10:37.456892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for key in task_dict.keys():\n",
    "  \n",
    "#     tasks=task_dict[key];\n",
    "#     # print(len(tasks))\n",
    "#     num_classes=num_classes_dict[key]\n",
    "#     models=[]\n",
    "#     exclude_keys=[]\n",
    "#     for task_id in range(num_classes):\n",
    "#         temp_model=copy.deepcopy(conflict_model);\n",
    "#         train_model_extra_metric(temp_model,tasks[task_id]['train'],tasks[task_id]['val'],num_epochs=5,device=device)\n",
    "#         # model_layer=remove_keys(temp_model.state_dict(),exclude_keys)\n",
    "#         models.append(temp_model);\n",
    "#         # del temp_model;\n",
    "    \n",
    "#     conflict2d=[[0 for y in range(num_classes)]for x in range(num_classes)]\n",
    "#     for tp in types:\n",
    "#         if tp in flags:\n",
    "#             exclude_keys=list(conflict_model.state_dict().keys())\n",
    "#             exclude_keys.remove('encoder.encoced_blocks.2.0.weight')\n",
    "#         else:\n",
    "#             exclude_keys=[];\n",
    "#         for metric in metrics.keys():\n",
    "            \n",
    "#             for i in range(num_classes):\n",
    "#                 for j in range(num_classes):\n",
    "#                     conflict2d[i][j]=metrics[metric]([models[i],models[j]],conflict_model,exclude_keys,CNN,2)[0]\n",
    "            \n",
    "                \n",
    "#             np_conflict=np.array(conflict2d)\n",
    "#             np.savetxt(f\"{key}_{tp}_{metric}.csv\", np_conflict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff450a62",
   "metadata": {
    "papermill": {
     "duration": 0.050659,
     "end_time": "2025-04-03T15:10:37.616442",
     "exception": false,
     "start_time": "2025-04-03T15:10:37.565783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 76785,
     "sourceId": 2271054,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2579480,
     "sourceId": 4532039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3999173,
     "sourceId": 6961629,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5314.113898,
   "end_time": "2025-04-03T15:10:40.356110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-03T13:42:06.242212",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
